{"docstore/data": {"942b2e0c-e53a-4648-8a9f-c6040b2c6ff8": {"__data__": {"id_": "942b2e0c-e53a-4648-8a9f-c6040b2c6ff8", "embedding": null, "metadata": {"page number": "4 - 12", "chapter": "Introduction"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b43c985c-9291-4546-8396-43fef2089134", "node_type": "4", "metadata": {"page number": "4 - 12", "chapter": "Introduction"}, "hash": "0424063a2f072c1f97c08b008fac084ac45f82647722cff1dbed7df4c4c83223", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cdf8fa84-273e-48fb-ac38-f565003d24c5", "node_type": "1", "metadata": {}, "hash": "d8595aa670b61afa7b952b94f62b750dbaf62a86ea94c77c00cf9023ccb06646", "class_name": "RelatedNodeInfo"}}, "text": "All rights reserved.\nThis book may not be reproduced, in whole or in part, including illustrations, in any form (beyond that copying permitted by Sections 107 and 108 of the U.S. Copyright Law and except by reviewers for the public press), without written permission from the publishers.\nYale University Press books may be purchased in quantity for educational, business, or promotional use. For information, please e-mail sales.press@yale.edu (U.S. o\ufb03ce) or sales@yaleup.co.uk (U.K. o\ufb03ce).\nA catalogue record for this book is available from the British Library.\nThis paper meets the requirements of ANSI/NISO Z39.48-1992 (Permanence of Paper).\nI love you. You\u2019ve tagged my head and heart.\nThis page intentionally left blank\nSynthetic Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 511 Introducing the Comparative Case Study . . . . . . . . . . . . . . . . . . . . . 511 Prison Construction and Black Male Incarceration . . . . . . . . . . . 525\nJust as it takes a village to raise a child, it takes many people to help me write a book like this. The people to whom I am indebted range from the scholars whose work has inspired me\u2014Alberto Abadie, Josh Angrist, Susan Athey, David Card, Esther Du\ufb02o, Guido Imbens, Alan Krueger, Robert LaLonde, Steven Levitt, Alex Tabarrok, John Snow, and many more\u2014to friends, mentors, and colleagues.\nI am most indebted \ufb01rst of all to my former advisor, mentor, coau- thor, and friend Christopher Cornwell. I probably owe Chris my entire career. He invested in me and taught me econometrics as well as empirical designs more generally when I was a grad student at the University of Georgia. I was brimming with a million ideas and he some- how managed to keep me focused. Always patient, always holding me to high standards, always believing I could achieve them, always trying to help me correct fallacious reasoning and poor knowledge of econo- metrics. I would also like to thank Alvin Roth, who has encouraged me over the last decade in my research. That encouragement has buoyed me throughout my career repeatedly. Finally, I\u2019d like to thank Judea Pearl for inviting me to UCLA for a day of discussions around an earlier draft of the Mixtape and helping improve it.\nBut a book like this is also due to countless conversations with friends over the years, as well as reading carefully their own work and learning from them. People like Mark Hoekstra, Rebecca Thornton, Paul Goldsmith-Pinkham, Mark Anderson, Greg DeAngelo, Manisha Shah, Christine Durrance, Melanie Guldi, Caitlyn Myers, Bernie Black, Keith Finlay, Jason Lindo, Andrew Goodman-Bacon, Pedro Sant\u2019anna, Andrew Baker, Rachael Meager, Nick Papageorge, Grant McDermott, Salvador Lozano, Daniel Millimet, David Jaeger, Berk Ozler, Erin Hen- gel, Alex Bartik, Megan Stevenson, Nick Huntington-Klein, Peter Hull,\nI would also like to thank my two students Hugo Rodrigues and Terry Tsai. Hugo and Terry worked tirelessly to adapt all of my blue collar Stata code into R programs. Without them, I would have been lost. I would also like to thank another student, Brice Green, for early trials of the code to con\ufb01rm it worked by non-authors. Blagoj Gegov helped create many of the \ufb01gures in Tikz. I would like to thank Ben Chidmi for adapting a simulation from R into Stata, and Yuki Yanai for allowing me to use his R code for a simulation. Thank you to Zeljko Hrcek for helping make amendments to the formatting of the LaTeX when I was running against deadline. And thank you to my friend Seth Hahne for creating several beautiful illustrations in the book.", "start_char_idx": 0, "end_char_idx": 3609, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cdf8fa84-273e-48fb-ac38-f565003d24c5": {"__data__": {"id_": "cdf8fa84-273e-48fb-ac38-f565003d24c5", "embedding": null, "metadata": {"page number": "4 - 12", "chapter": "Introduction"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b43c985c-9291-4546-8396-43fef2089134", "node_type": "4", "metadata": {"page number": "4 - 12", "chapter": "Introduction"}, "hash": "0424063a2f072c1f97c08b008fac084ac45f82647722cff1dbed7df4c4c83223", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "942b2e0c-e53a-4648-8a9f-c6040b2c6ff8", "node_type": "1", "metadata": {"page number": "4 - 12", "chapter": "Introduction"}, "hash": "438b277466d240c7db0ead53e42ab8aa97441e47a10e9747f8467944bc1b867d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "13a903af-57c2-4317-be27-f8f3b31bc6ea", "node_type": "1", "metadata": {}, "hash": "841924e362b0bd94d0a268dbf568025508d0e6056b0afa1ea66d103d4ad17dbd", "class_name": "RelatedNodeInfo"}}, "text": "Hugo and Terry worked tirelessly to adapt all of my blue collar Stata code into R programs. Without them, I would have been lost. I would also like to thank another student, Brice Green, for early trials of the code to con\ufb01rm it worked by non-authors. Blagoj Gegov helped create many of the \ufb01gures in Tikz. I would like to thank Ben Chidmi for adapting a simulation from R into Stata, and Yuki Yanai for allowing me to use his R code for a simulation. Thank you to Zeljko Hrcek for helping make amendments to the formatting of the LaTeX when I was running against deadline. And thank you to my friend Seth Hahne for creating several beautiful illustrations in the book. I would also like to thank Seth Ditchik for believing in this project, my agent Lindsay Edgecombe for her encouragement and work on my behalf, and Yale University Press. And to my other editor, Charlie Clark, who must have personally read this book \ufb01fty times and worked so hard to improve it. Thank you, Charlie. And to the musicians who have sung the soundtrack to my life, thanks to Chance, Drake, Dr. Dre, Eminem, Lauryn Hill, House of Pain, Jay-Z, Mos Def, Notorious B.I.G., Pharcyde, Tupac Shakur, Tribe, Kanye West, Young MC, and many others.\nFinally, I\u2019d like to thank my close friends, Baylor colleagues, stu- dents, and family for tolerating my eccentric enthusiasm for causal inference and economics for years. I have bene\ufb01ted tremendously from many opportunities and resources, and for that and other things I am very grateful.\nThis book, and the class it was based on, is a distillation of count- less journal articles, books, as well as classes I have taken in person and studied from afar. It is also a product of numerous conversations I\u2019ve had with colleagues, students and teachers for many years. I have attempted to give credit where credit is due. All errors in this book were caused entirely by me, not the people listed above.\nThis page intentionally left blank", "start_char_idx": 2940, "end_char_idx": 4894, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "13a903af-57c2-4317-be27-f8f3b31bc6ea": {"__data__": {"id_": "13a903af-57c2-4317-be27-f8f3b31bc6ea", "embedding": null, "metadata": {"page number": "13 - 15", "chapter": "Introduction"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0a516dbb-2de4-488a-b681-0e73fd98c9a7", "node_type": "4", "metadata": {"page number": "13 - 15", "chapter": "Introduction"}, "hash": "c5f6f592b77f84d55fbaffeae1b6465dc62d49ebf67c504c051c732afe616c34", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cdf8fa84-273e-48fb-ac38-f565003d24c5", "node_type": "1", "metadata": {"page number": "4 - 12", "chapter": "Introduction"}, "hash": "491873f8c2d6096943ceed015e3f9fe2eccc400abaedc083237d249cd8d7e348", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "19023b39-bf25-4b31-88dd-ab8251717e2c", "node_type": "1", "metadata": {}, "hash": "ff25123a2e19c6ad6ef44c2d67ec0be6f6ab8b18a5ad99676920328aacff78cf", "class_name": "RelatedNodeInfo"}}, "text": "Introduction:\nMy path to economics was not linear. I didn\u2019t major in economics, for instance. I didn\u2019t even take an economics course in college. I majored in English, for Pete\u2019s sake. My ambition was to become a poet. But then I became intrigued with the idea that humans can form plausible beliefs about causal effects even without a randomized experiment. Twenty-\ufb01ve years ago, I wouldn\u2019t have had a clue what that sentence even meant, let alone how to do such an experiment. So how did I get here? Maybe you would like to know how I got to the point where I felt I needed to write this book. The TL;DR version is that I followed a windy path from English to causal inference.1 First, I fell in love with economics. Then I fell in love with empirical research. Then I noticed that a growing interest in causal inference had been happening in me the entire time. But let me tell the longer version.\nI majored in English at the University of Tennessee at Knoxville and graduated with a serious ambition to become a professional poet. But, while I had been successful writing poetry in college, I quickly realized that \ufb01nding the road to success beyond that point was probably not realistic. I was newly married, with a baby on the way, and working as a qualitative research analyst doing market research. Slowly, I had stopped writing poetry altogether.2\nMy job as a qualitative research analyst was eye opening, in part because it was my \ufb01rst exposure to empiricism. My job was to do \u201cgrounded theory\u201d\u2014a kind of inductive approach to generating expla- nations of human behavior based on observations. I did this by run- ning focus groups and conducting in-depth interviews, as well as\n1 \u201cToo long; didn\u2019t read.\u201d\n2 Rilke said you should quit writing poetry when you can imagine yourself living\nwithout it [Rilke, 2012]. I could imagine living without poetry, so I took his advice and\nquit. Interestingly, when I later found economics, I went back to Rilke and asked myself\nif I could live without it. This time, I decided I couldn\u2019t, or wouldn\u2019t\u2014I wasn\u2019t sure which.\nSo I stuck with it and got a PhD.\nthrough other ethnographic methods. I approached each project as an opportunity to understand why people did the things they did (even if what they did was buy detergent or pick a cable provider). While the job inspired me to develop my own theories about human behavior, it didn\u2019t provide me a way of falsifying those theories.\nI lacked a background in the social sciences, so I would spend my evenings downloading and reading articles from the Internet. I don\u2019t remember how I ended up there, but one night I was on the University of Chicago Law and Economics working paper series website when a speech by Gary Becker caught my eye. It was his Nobel Prize accep- tance speech on how economics applies to all of human behavior [Becker, 1993], and reading it changed my life. I thought economics was about stock markets and banks until I read that speech. I didn\u2019t know economics was an engine that one could use to analyze all of human behavior. This was overwhelmingly exciting, and a seed had been planted.\nBut it wasn\u2019t until I read an article on crime by Lott and Mustard [1997] that I became truly enamored of economics. I had no idea that there was an empirical component where economists sought to esti- mate causal effects with quantitative data. A coauthor of that paper was David Mustard, then an associate professor of economics at the University of Georgia, and one of Gary Becker\u2019s former students. I decided that I wanted to study with Mustard, and so I applied to the Uni- versity of Georgia\u2019s doctoral program in economics. I moved to Athens, Georgia, with my wife, Paige, and our infant son, Miles, and started classes in the fall of 2002.\nAfter passing my \ufb01rst-year comprehensive exams, I took Mus- tard\u2019s labor economics \ufb01eld class and learned about a variety of top- ics that would shape my interests for years. These topics included the returns to education, inequality, racial discrimination, crime, and many other fascinating topics in labor. We read many, many empir- ical papers in that class, and afterwards I knew that I would need a strong background in econometrics to do the kind of research I cared about. In fact, I decided to make econometrics my main \ufb01eld of study. This led me to work with Christopher Cornwell, an econome- trician and labor economist at Georgia. I learned a lot from Chris, both\nabout econometrics and about research itself.", "start_char_idx": 0, "end_char_idx": 4479, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "19023b39-bf25-4b31-88dd-ab8251717e2c": {"__data__": {"id_": "19023b39-bf25-4b31-88dd-ab8251717e2c", "embedding": null, "metadata": {"page number": "13 - 15", "chapter": "Introduction"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0a516dbb-2de4-488a-b681-0e73fd98c9a7", "node_type": "4", "metadata": {"page number": "13 - 15", "chapter": "Introduction"}, "hash": "c5f6f592b77f84d55fbaffeae1b6465dc62d49ebf67c504c051c732afe616c34", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "13a903af-57c2-4317-be27-f8f3b31bc6ea", "node_type": "1", "metadata": {"page number": "13 - 15", "chapter": "Introduction"}, "hash": "c066899586a634f1b0593ded09135f98559809cde48c35c07b7c8fa7f0b6ceef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "45c746f2-1658-4d10-8798-23f5c4d8e162", "node_type": "1", "metadata": {}, "hash": "edcd090513b04b5a271163ef8065b8fafff4dcfd6897a87bbf303e52248264f0", "class_name": "RelatedNodeInfo"}}, "text": "I moved to Athens, Georgia, with my wife, Paige, and our infant son, Miles, and started classes in the fall of 2002.\nAfter passing my \ufb01rst-year comprehensive exams, I took Mus- tard\u2019s labor economics \ufb01eld class and learned about a variety of top- ics that would shape my interests for years. These topics included the returns to education, inequality, racial discrimination, crime, and many other fascinating topics in labor. We read many, many empir- ical papers in that class, and afterwards I knew that I would need a strong background in econometrics to do the kind of research I cared about. In fact, I decided to make econometrics my main \ufb01eld of study. This led me to work with Christopher Cornwell, an econome- trician and labor economist at Georgia. I learned a lot from Chris, both\nabout econometrics and about research itself. He became a mentor, coauthor, and close friend.\nEconometrics was di\ufb03cult. I won\u2019t even pretend I was good at it. I took all the econometrics courses offered at the University of Geor- gia, some more than once. They included classes covering topics like probability and statistics, cross-sections, panel data, time series, and qualitative dependent variables. But while I passed my \ufb01eld exam in econometrics, I struggled to understand econometrics at a deep level. As the saying goes, I could not see the forest for the trees. Something just wasn\u2019t clicking.\nI noticed something, though, while I was writing the third chapter of my dissertation that I hadn\u2019t noticed before. My third chapter was an investigation of the effect of abortion legalization on the cohort\u2019s future sexual behavior [Cunningham and Cornwell, 2013]. It was a revis- iting of Donohue and Levitt [2001]. One of the books I read in prepara- tion for my study was Levine [2004], which in addition to reviewing the theory of and empirical studies on abortion had a little table explaining the difference-in-differences identi\ufb01cation strategy. The University of Georgia had a traditional econometrics pedagogy, and most of my \ufb01eld courses were theoretical (e.g., public economics, industrial organiza- tion), so I never really had heard the phrase \u201cidenti\ufb01cation strategy,\u201d let alone \u201ccausal inference.\u201d Levine\u2019s simple difference-in-differences table for some reason opened my eyes. I saw how econometric mod- eling could be used to isolate the causal effects of some treatment, and that led to a change in how I approach empirical problems.", "start_char_idx": 3642, "end_char_idx": 6089, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45c746f2-1658-4d10-8798-23f5c4d8e162": {"__data__": {"id_": "45c746f2-1658-4d10-8798-23f5c4d8e162", "embedding": null, "metadata": {"page number": "15 - 16"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "52827240-61a7-40e1-8078-45bacdcb7d9d", "node_type": "4", "metadata": {"page number": "15 - 16"}, "hash": "317a23f729db58be1c4245393ab9ceefb1a2e3d9d6636a4e8e2f75a0f92e36cb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "19023b39-bf25-4b31-88dd-ab8251717e2c", "node_type": "1", "metadata": {"page number": "13 - 15", "chapter": "Introduction"}, "hash": "f85ddf250b7a24ad429289454576a814f0ef544002fdc214f37f513db93a5fef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9156c820-862b-4eda-84c7-7b7632de30b4", "node_type": "1", "metadata": {}, "hash": "9cd4fed86bec237088b65fa7db8ef71ee157762cfe7093dc655ab4eb4b9c35fa", "class_name": "RelatedNodeInfo"}}, "text": "What Is Causal Inference?:\nMy \ufb01rst job out of graduate school was as an assistant professor at Baylor University in Waco, Texas, where I still work and live today. I was restless the second I got there. I could feel that econometrics was indispensable, and yet I was missing something. But what? It was a theory of causality. I had been orbiting that theory ever since seeing that difference-in-differences table in Levine [2004]. But I needed more. So, desperate, I did what I always do when I want to learn something new\u2014I developed a course on causality to force myself to learn all the things I didn\u2019t know.\nabout econometrics and about research itself. He became a mentor, coauthor, and close friend.\nEconometrics was di\ufb03cult. I won\u2019t even pretend I was good at it. I took all the econometrics courses offered at the University of Geor- gia, some more than once. They included classes covering topics like probability and statistics, cross-sections, panel data, time series, and qualitative dependent variables. But while I passed my \ufb01eld exam in econometrics, I struggled to understand econometrics at a deep level. As the saying goes, I could not see the forest for the trees. Something just wasn\u2019t clicking.\nI noticed something, though, while I was writing the third chapter of my dissertation that I hadn\u2019t noticed before. My third chapter was an investigation of the effect of abortion legalization on the cohort\u2019s future sexual behavior [Cunningham and Cornwell, 2013]. It was a revis- iting of Donohue and Levitt [2001]. One of the books I read in prepara- tion for my study was Levine [2004], which in addition to reviewing the theory of and empirical studies on abortion had a little table explaining the difference-in-differences identi\ufb01cation strategy. The University of Georgia had a traditional econometrics pedagogy, and most of my \ufb01eld courses were theoretical (e.g., public economics, industrial organiza- tion), so I never really had heard the phrase \u201cidenti\ufb01cation strategy,\u201d let alone \u201ccausal inference.\u201d Levine\u2019s simple difference-in-differences table for some reason opened my eyes. I saw how econometric mod- eling could be used to isolate the causal effects of some treatment, and that led to a change in how I approach empirical problems.", "start_char_idx": 0, "end_char_idx": 2267, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9156c820-862b-4eda-84c7-7b7632de30b4": {"__data__": {"id_": "9156c820-862b-4eda-84c7-7b7632de30b4", "embedding": null, "metadata": {"page number": "16 - 19", "chapter": "Probability and Regression Review"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "717c74f0-f551-4617-bf30-e276298d1582", "node_type": "4", "metadata": {"page number": "16 - 19", "chapter": "Probability and Regression Review"}, "hash": "9692856eea35efc45ba0b7c607a7eb28402135801a1b369c7c60ded491d9365e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "45c746f2-1658-4d10-8798-23f5c4d8e162", "node_type": "1", "metadata": {"page number": "15 - 16"}, "hash": "0bc8f5f6817858cf2e29cd8bb8bb24ffc3c4d6bed0f8654a7cc8bbb324a44e3a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4f3291a2-2015-44bc-80c8-79c563ea570b", "node_type": "1", "metadata": {}, "hash": "246a3776cc6a422b97b698340467e23f7ac7b45d40a090b3b8f825ab49839ffa", "class_name": "RelatedNodeInfo"}}, "text": "What Is Causal Inference?:\nMy \ufb01rst job out of graduate school was as an assistant professor at Baylor University in Waco, Texas, where I still work and live today. I was restless the second I got there. I could feel that econometrics was indispensable, and yet I was missing something. But what? It was a theory of causality. I had been orbiting that theory ever since seeing that difference-in-differences table in Levine [2004]. But I needed more. So, desperate, I did what I always do when I want to learn something new\u2014I developed a course on causality to force myself to learn all the things I didn\u2019t know.\nI named the course Causal Inference and Research Design and taught it for the \ufb01rst time to Baylor master\u2019s students in 2010. At the time, I couldn\u2019t really \ufb01nd an example of the sort of class I was looking for, so I cobbled together a patchwork of ideas from several disci- plines and authors, like labor economics, public economics, sociology, political science, epidemiology, and statistics. You name it. My class wasn\u2019t a pure econometrics course; rather, it was an applied empirical class that taught a variety of contemporary research designs, such as difference-in-differences, and it was \ufb01lled with empirical replications and readings, all of which were built on the robust theory of causality found in Donald Rubin\u2019s work as well as the work of Judea Pearl. This book and that class are in fact very similar to one another.3\nSo how would I de\ufb01ne causal inference? Causal inference is the leveraging of theory and deep knowledge of institutional details to esti- mate the impact of events and choices on a given outcome of interest. It is not a new \ufb01eld; humans have been obsessing over causality since antiquity. But what is new is the progress we believe we\u2019ve made in esti- mating causal effects both inside and outside the laboratory. Some date the beginning of this new, modern causal inference to Fisher [1935], Haavelmo [1943], or Rubin [1974]. Some connect it to the work of early pioneers like John Snow. We should give a lot of credit to numer- ous highly creative labor economists from the late 1970s to late 1990s whose ambitious research agendas created a revolution in economics that continues to this day. You could even make an argument that we owe it to the Cowles Commission, Philip and Sewall Wright, and the computer scientist Judea Pearl.\nBut however you date its emergence, causal inference has now matured into a distinct \ufb01eld, and not surprisingly, you\u2019re starting to see more and more treatments of it as such. It\u2019s sometimes reviewed in a lengthy chapter on \u201cprogram evaluation\u201d in econometrics textbooks\n3 I decided to write this book for one simple reason: I didn\u2019t feel that the market had\nprovided the book that I needed for my students. So I wrote this book for my students\nand me so that we\u2019d all be on the same page. This book is my best effort to explain\ncausal inference to myself. I felt that if I could explain causal inference to myself, then I\nwould be able to explain it to others too. Not thinking the book would have much value outside of my class, I posted it to my website and told people about it on Twitter. I was\nsurprised to learn that so many people found the book helpful.\n[Wooldridge, 2010], or even given entire book-length treatments. To name just a few textbooks in the growing area, there\u2019s Angrist and Pischke [2009], Morgan and Winship [2014], Imbens and Rubin [2015], and probably a half dozen others, not to mention numerous, lengthy treatments of speci\ufb01c strategies, such as those found in Angrist and Krueger [2001] and Imbens and Lemieux [2008]. The market is quietly adding books and articles about identifying causal effects with data all the time.\nSo why does Causal Inference: The Mixtape exist? Well, to put it bluntly, a readable introductory book with programming examples, data, and detailed exposition didn\u2019t exist until this one. My book is an effort to \ufb01ll that hole, because I believe what researchers really need is a guide that takes them from knowing almost nothing about causal inference to a place of competency. Competency in the sense that they are conversant and literate about what designs can and cannot do.", "start_char_idx": 0, "end_char_idx": 4210, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4f3291a2-2015-44bc-80c8-79c563ea570b": {"__data__": {"id_": "4f3291a2-2015-44bc-80c8-79c563ea570b", "embedding": null, "metadata": {"page number": "16 - 19", "chapter": "Probability and Regression Review"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "717c74f0-f551-4617-bf30-e276298d1582", "node_type": "4", "metadata": {"page number": "16 - 19", "chapter": "Probability and Regression Review"}, "hash": "9692856eea35efc45ba0b7c607a7eb28402135801a1b369c7c60ded491d9365e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9156c820-862b-4eda-84c7-7b7632de30b4", "node_type": "1", "metadata": {"page number": "16 - 19", "chapter": "Probability and Regression Review"}, "hash": "8f1da4e11a0dec1201fe7b4530e57dc45dbd2215c26422597e69533ac827af2b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2f729636-0b2a-4a18-a03b-0fca3c13baac", "node_type": "1", "metadata": {}, "hash": "bce8af9fd1161100c48d2aad41de899de08d4c4f521bebfbda8f20354f422925", "class_name": "RelatedNodeInfo"}}, "text": "The market is quietly adding books and articles about identifying causal effects with data all the time.\nSo why does Causal Inference: The Mixtape exist? Well, to put it bluntly, a readable introductory book with programming examples, data, and detailed exposition didn\u2019t exist until this one. My book is an effort to \ufb01ll that hole, because I believe what researchers really need is a guide that takes them from knowing almost nothing about causal inference to a place of competency. Competency in the sense that they are conversant and literate about what designs can and cannot do. Competency in the sense that they can take data, write code and, using theoretical and contextual knowledge, implement a reasonable design in one of their own projects. If this book helps someone do that, then this book will have had value, and that is all I can and should hope for. But what books out there do I like? Which ones have inspired this book? And why don\u2019t I just keep using them? For my classes, I mainly relied on Morgan and Winship [2014], Angrist and Pischke [2009], as well as a library of theoretical and empirical articles. These books are in my opinion de\ufb01nitive classics. But they didn\u2019t satisfy my needs, and as a result, I was constantly jumping between material. Other books were awesome but not quite right for me either. Imbens and Rubin [2015] cover the potential outcomes model, experimental design, and matching and instrumental variables, but not directed acyclic graphical models (DAGs), regression discontinuity, panel data, or synthetic con- trol. Morgan and Winship [2014] cover DAGs, the potential outcomes model, and instrumental variables, but have too light a touch on regres- sion discontinuity and panel data for my tastes. They also don\u2019t cover synthetic control, which has been called the most important innovation in causal inference of the last 15 years by Athey and Imbens [2017b]. Angrist and Pischke [2009] is very close to what I need but does not include anything on synthetic control or on the graphical models that I \ufb01nd so critically useful. But maybe most importantly, Imbens and Rubin\n[2015], Angrist and Pischke [2009], and Morgan and Winship [2014] do not provide any practical programming guidance, and I believe it is in replication and coding that we gain knowledge in these areas.4\nThis book was written with a few different people in mind. It was written \ufb01rst and foremost for practitioners, which is why it includes easy-to-download data sets and programs. It\u2019s why I have made sev- eral efforts to review papers as well as replicate the models as much as possible. I want readers to understand this \ufb01eld, but as important, I want them to feel empowered so that they can use these tools to answer their own research questions.\nAnother person I have in mind is the experienced social scientist who wants to retool. Maybe these are people with more of a theoretical bent or background, or maybe they\u2019re people who simply have some holes in their human capital. This book, I hope, can help guide them through the modern theories of causality so common in the social sci- ences, as well as provide a calculus in directed acyclic graphical mod- els that can help connect their knowledge of theory with estimation. The DAGs in particular are valuable for this group, I think.\nA third group that I\u2019m focusing on is the nonacademic person in industry, media, think tanks, and the like. Increasingly, knowledge about causal inference is expected throughout the professional world. It is no longer simply something that academics sit around and debate. It is crucial knowledge for making business decisions as well as for interpreting policy.\nFinally, this book is written for people very early in their careers, be they undergraduates, graduate students, or newly minted PhDs. My hope is that this book can give them a jump start so that they don\u2019t have to meander, like many of us did, through a somewhat labyrinthine path to these methods.", "start_char_idx": 3627, "end_char_idx": 7606, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2f729636-0b2a-4a18-a03b-0fca3c13baac": {"__data__": {"id_": "2f729636-0b2a-4a18-a03b-0fca3c13baac", "embedding": null, "metadata": {"page number": "19 - 22"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7c3a6c55-9360-4422-9ac1-87a0145579d6", "node_type": "4", "metadata": {"page number": "19 - 22"}, "hash": "476b0580874f078baa90d9a3c4fd3e2c38c256dcf9c06a85acde9e62231ac770", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4f3291a2-2015-44bc-80c8-79c563ea570b", "node_type": "1", "metadata": {"page number": "16 - 19", "chapter": "Probability and Regression Review"}, "hash": "bbeb2abb302e9c64a6d6f49ce54cf0c8f6c6a70ef3c74929b5f73b7f665a2f67", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a9ce38c9-acc2-4f68-af34-ade27cdb27f8", "node_type": "1", "metadata": {}, "hash": "6f75b8e44fbdea02e6f9684270a58bf192c11b767c56c359e01a629cfd404f6c", "class_name": "RelatedNodeInfo"}}, "text": "Do Not Confuse Correlation with Causality:\nIt is very common these days to hear someone say \u201ccorrelation does not mean causality.\u201d Part of the purpose of this book is to help\n4 Although Angrist and Pischke [2009] provides an online data warehouse from\ndozens of papers, replications for these ideas to become concrete and familiar.\nI \ufb01nd that students need more pedagogical walk-throughs and\n[2015], Angrist and Pischke [2009], and Morgan and Winship [2014] do not provide any practical programming guidance, and I believe it is in replication and coding that we gain knowledge in these areas.4\nThis book was written with a few different people in mind. It was written \ufb01rst and foremost for practitioners, which is why it includes easy-to-download data sets and programs. It\u2019s why I have made sev- eral efforts to review papers as well as replicate the models as much as possible. I want readers to understand this \ufb01eld, but as important, I want them to feel empowered so that they can use these tools to answer their own research questions.\nAnother person I have in mind is the experienced social scientist who wants to retool. Maybe these are people with more of a theoretical bent or background, or maybe they\u2019re people who simply have some holes in their human capital. This book, I hope, can help guide them through the modern theories of causality so common in the social sci- ences, as well as provide a calculus in directed acyclic graphical mod- els that can help connect their knowledge of theory with estimation. The DAGs in particular are valuable for this group, I think.\nA third group that I\u2019m focusing on is the nonacademic person in industry, media, think tanks, and the like. Increasingly, knowledge about causal inference is expected throughout the professional world. It is no longer simply something that academics sit around and debate. It is crucial knowledge for making business decisions as well as for interpreting policy.\nFinally, this book is written for people very early in their careers, be they undergraduates, graduate students, or newly minted PhDs. My hope is that this book can give them a jump start so that they don\u2019t have to meander, like many of us did, through a somewhat labyrinthine path to these methods.\nIt is very common these days to hear someone say \u201ccorrelation does not mean causality.\u201d Part of the purpose of this book is to help\n4 Although Angrist and Pischke [2009] provides an online data warehouse from\ndozens of papers, replications for these ideas to become concrete and familiar.\nI \ufb01nd that students need more pedagogical walk-throughs and\nreaders be able to understand exactly why correlations, particularly in observational data, are unlikely to be re\ufb02ective of a causal relationship. When the rooster crows, the sun soon after rises, but we know the rooster didn\u2019t cause the sun to rise. Had the rooster been eaten by the farmer\u2019s cat, the sun still would have risen. Yet so often people make this kind of mistake when naively interpreting simple correlations.\nBut weirdly enough, sometimes there are causal relationships between two things and yet no observable correlation. Now that is def- initely strange. How can one thing cause another thing without any discernible correlation between the two things? Consider this exam- ple, which is illustrated in Figure 1. A sailor is sailing her boat across the lake on a windy day. As the wind blows, she counters by turning the rudder in such a way so as to exactly offset the force of the wind. Back and forth she moves the rudder, yet the boat follows a straight line across the lake. A kindhearted yet naive person with no knowledge of wind or boats might look at this woman and say, \u201cSomeone get this sailor a new rudder! Hers is broken!\u201d He thinks this because he can- not see any relationship between the movement of the rudder and the direction of the boat.\nFigure 1. No correlation doesn\u2019t mean no causality. Artwork by Seth Hahne \u00a9 2020.\nBut does the fact that he cannot see the relationship mean there isn\u2019t one? Just because there is no observable relationship does not mean there is no causal one. Imagine that instead of perfectly coun- tering the wind by turning the rudder, she had instead \ufb02ipped a coin\u2014 heads she turns the rudder left, tails she turns the rudder right. What do you think this man would have seen if she was sailing her boat accord- ing to coin \ufb02ips?", "start_char_idx": 0, "end_char_idx": 4390, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a9ce38c9-acc2-4f68-af34-ade27cdb27f8": {"__data__": {"id_": "a9ce38c9-acc2-4f68-af34-ade27cdb27f8", "embedding": null, "metadata": {"page number": "19 - 22"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7c3a6c55-9360-4422-9ac1-87a0145579d6", "node_type": "4", "metadata": {"page number": "19 - 22"}, "hash": "476b0580874f078baa90d9a3c4fd3e2c38c256dcf9c06a85acde9e62231ac770", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2f729636-0b2a-4a18-a03b-0fca3c13baac", "node_type": "1", "metadata": {"page number": "19 - 22"}, "hash": "66be530eb69a5f82c4a8b815609461b5251373845686960f26cc6e74859fd366", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a909d11-e83f-465f-8b8d-3bfd44717207", "node_type": "1", "metadata": {}, "hash": "728feca9bb863da04c7f75126669b7db7ea6aeca73b4db0a6b12bc6598e94d8d", "class_name": "RelatedNodeInfo"}}, "text": "A kindhearted yet naive person with no knowledge of wind or boats might look at this woman and say, \u201cSomeone get this sailor a new rudder! Hers is broken!\u201d He thinks this because he can- not see any relationship between the movement of the rudder and the direction of the boat.\nFigure 1. No correlation doesn\u2019t mean no causality. Artwork by Seth Hahne \u00a9 2020.\nBut does the fact that he cannot see the relationship mean there isn\u2019t one? Just because there is no observable relationship does not mean there is no causal one. Imagine that instead of perfectly coun- tering the wind by turning the rudder, she had instead \ufb02ipped a coin\u2014 heads she turns the rudder left, tails she turns the rudder right. What do you think this man would have seen if she was sailing her boat accord- ing to coin \ufb02ips? If she randomly moved the rudder on a windy day, then he would see a sailor zigzagging across the lake. Why would he see the relationship if the movement were randomized but not be able to see it otherwise? Because the sailor is endogenously moving the rud- der in response to the unobserved wind. And as such, the relationship between the rudder and the boat\u2019s direction is canceled\u2014even though there is a causal relationship between the two.\nThis sounds like a silly example, but in fact there are more serious versions of it. Consider a central bank reading tea leaves to discern when a recessionary wave is forming. Seeing evidence that a reces- sion is emerging, the bank enters into open-market operations, buy- ing bonds and pumping liquidity into the economy. Insofar as these actions are done optimally, these open-market operations will show no relationship whatsoever with actual output. In fact, in the ideal, banks may engage in aggressive trading in order to stop a recession, and we would be unable to see any evidence that it was working even though it was!\nHuman beings engaging in optimal behavior are the main rea- son correlations almost never reveal causal relationships, because rarely are human beings acting randomly. And as we will see, it is the presence of randomness that is crucial for identifying causal effect.", "start_char_idx": 3594, "end_char_idx": 5732, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a909d11-e83f-465f-8b8d-3bfd44717207": {"__data__": {"id_": "3a909d11-e83f-465f-8b8d-3bfd44717207", "embedding": null, "metadata": {"page number": "22 - 25"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "314f3329-a54b-4e56-8822-20973e9f563f", "node_type": "4", "metadata": {"page number": "22 - 25"}, "hash": "3cd2d10bfdfa70e3a080cfc8f73327dcc2bec9a6a79acc78ebee1965a62d6c88", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a9ce38c9-acc2-4f68-af34-ade27cdb27f8", "node_type": "1", "metadata": {"page number": "19 - 22"}, "hash": "719770660a421140c9bcec9bf213a3e7114fc9a76e49487135c8daef39133eac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1371dc4d-518d-4ff8-a820-1ceea4b8e222", "node_type": "1", "metadata": {}, "hash": "2c0bac6b4e6bb9ddc11ceccb6c34376883aab4cec102f4b34be1baec57127fc4", "class_name": "RelatedNodeInfo"}}, "text": "Optimization Makes Everything Endogenous:\nCertain presentations of causal inference methodologies have sometimes been described as atheoretical, but in my opinion, while some practitioners seem comfortable \ufb02ying blind, the actual meth- ods employed in causal designs are always deeply dependent on the- ory and local institutional knowledge. It is my \ufb01rm belief, which I will emphasize over and over in this book, that without prior knowledge,\nBut does the fact that he cannot see the relationship mean there isn\u2019t one? Just because there is no observable relationship does not mean there is no causal one. Imagine that instead of perfectly coun- tering the wind by turning the rudder, she had instead \ufb02ipped a coin\u2014 heads she turns the rudder left, tails she turns the rudder right. What do you think this man would have seen if she was sailing her boat accord- ing to coin \ufb02ips? If she randomly moved the rudder on a windy day, then he would see a sailor zigzagging across the lake. Why would he see the relationship if the movement were randomized but not be able to see it otherwise? Because the sailor is endogenously moving the rud- der in response to the unobserved wind. And as such, the relationship between the rudder and the boat\u2019s direction is canceled\u2014even though there is a causal relationship between the two.\nThis sounds like a silly example, but in fact there are more serious versions of it. Consider a central bank reading tea leaves to discern when a recessionary wave is forming. Seeing evidence that a reces- sion is emerging, the bank enters into open-market operations, buy- ing bonds and pumping liquidity into the economy. Insofar as these actions are done optimally, these open-market operations will show no relationship whatsoever with actual output. In fact, in the ideal, banks may engage in aggressive trading in order to stop a recession, and we would be unable to see any evidence that it was working even though it was!\nHuman beings engaging in optimal behavior are the main rea- son correlations almost never reveal causal relationships, because rarely are human beings acting randomly. And as we will see, it is the presence of randomness that is crucial for identifying causal effect.\nCertain presentations of causal inference methodologies have sometimes been described as atheoretical, but in my opinion, while some practitioners seem comfortable \ufb02ying blind, the actual meth- ods employed in causal designs are always deeply dependent on the- ory and local institutional knowledge. It is my \ufb01rm belief, which I will emphasize over and over in this book, that without prior knowledge,\nestimated causal effects are rarely, if ever, believable. Prior knowledge is required in order to justify any claim of a causal \ufb01nding. And eco- nomic theory also highlights why causal inference is necessarily a thorny task. Let me explain.\nThere\u2019s broadly thought to be two types of data. There\u2019s experimen- tal data and non-experimental data. The latter is also sometimes called observational data. Experimental data is collected in something akin to a laboratory environment. In a traditional experiment, the researcher participates actively in the process being recorded. It\u2019s more di\ufb03cult to obtain data like this in the social sciences due to feasibility, \ufb01nancial cost, or moral objections, although it is more common now than was once the case. Examples include the Oregon Medicaid Experiment, the RAND health insurance experiment, the \ufb01eld experiment movement inspired by Esther Du\ufb02o, Michael Kremer, Abhijit Banerjee, and John List, and many others.\nObservational data is usually collected through surveys in a retro- spective manner, or as the by-product of some other business activity (\u201cbig data\u201d). In many observational studies, you collect data about what happened previously, as opposed to collecting data as it hap- pens, though with the increased use of web scraping, it may be pos- sible to get observational data closer to the exact moment in which some action occurred. But regardless of the timing, the researcher is a passive actor in the processes creating the data itself. She observes actions and results but is not in a position to interfere with the envi- ronment in which the units under consideration exist. This is the most common form of data that many of us will ever work with.\nEconomic theory tells us we should be suspicious of correlations found in observational data. In observational data, correlations are almost certainly not re\ufb02ecting a causal relationship because the vari- ables were endogenously chosen by people who were making deci- sions they thought were best. In pursuing some goal while facing con- straints, they chose certain things that created a spurious correlation with other things.", "start_char_idx": 0, "end_char_idx": 4765, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1371dc4d-518d-4ff8-a820-1ceea4b8e222": {"__data__": {"id_": "1371dc4d-518d-4ff8-a820-1ceea4b8e222", "embedding": null, "metadata": {"page number": "22 - 25"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "314f3329-a54b-4e56-8822-20973e9f563f", "node_type": "4", "metadata": {"page number": "22 - 25"}, "hash": "3cd2d10bfdfa70e3a080cfc8f73327dcc2bec9a6a79acc78ebee1965a62d6c88", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a909d11-e83f-465f-8b8d-3bfd44717207", "node_type": "1", "metadata": {"page number": "22 - 25"}, "hash": "623a43272ae9b9cf8d72c6fcbefc4780478f185feb7518e6eb691b42f782fc17", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "26a865ee-5356-492d-8e0e-cf9ece65d9a0", "node_type": "1", "metadata": {}, "hash": "abd1ea4656a249e05aa9e54e573fabd099f33d182e50d7823859cba8356dc975", "class_name": "RelatedNodeInfo"}}, "text": "But regardless of the timing, the researcher is a passive actor in the processes creating the data itself. She observes actions and results but is not in a position to interfere with the envi- ronment in which the units under consideration exist. This is the most common form of data that many of us will ever work with.\nEconomic theory tells us we should be suspicious of correlations found in observational data. In observational data, correlations are almost certainly not re\ufb02ecting a causal relationship because the vari- ables were endogenously chosen by people who were making deci- sions they thought were best. In pursuing some goal while facing con- straints, they chose certain things that created a spurious correlation with other things. And we see this problem re\ufb02ected in the potential outcomes model itself: a correlation, in order to be a measure of a causal effect, must be based on a choice that was made independent of the potential outcomes under consideration. Yet if the person is\nmaking some choice based on what she thinks is best, then it nec- essarily is based on potential outcomes, and the correlation does not remotely satisfy the conditions we need in order to say it is causal. To put it as bluntly as I can, economic theory says choices are endoge- nous, and therefore since they are, the correlations between those choices and outcomes in the aggregate will rarely, if ever, represent a causal effect.\nNow we are veering into the realm of epistemology. Identifying causal effects involves assumptions, but it also requires a particu- lar kind of belief about the work of scientists. Credible and valuable research requires that we believe that it is more important to do our work correctly than to try and achieve a certain outcome (e.g., con- \ufb01rmation bias, statistical signi\ufb01cance, asterisks). The foundations of scienti\ufb01c knowledge are scienti\ufb01c methodologies. True scientists do not collect evidence in order to prove what they want to be true or what others want to believe. That is a form of deception and manipulation called propaganda, and propaganda is not science. Rather, scienti\ufb01c methodologies are devices for forming a particular kind of belief. Sci- enti\ufb01c methodologies allow us to accept unexpected, and sometimes undesirable, answers. They are process oriented, not outcome ori- ented. And without these values, causal methodologies are also not believable.", "start_char_idx": 4016, "end_char_idx": 6424, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "26a865ee-5356-492d-8e0e-cf9ece65d9a0": {"__data__": {"id_": "26a865ee-5356-492d-8e0e-cf9ece65d9a0", "embedding": null, "metadata": {"page number": "25 - 26"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb4056ec-7ec0-47f8-97e4-4af53e34c7a3", "node_type": "4", "metadata": {"page number": "25 - 26"}, "hash": "dae526c55b24a4b4d2b1d2f15f9d61422ea2454bee8f0ed2b4457e5dd58975cf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1371dc4d-518d-4ff8-a820-1ceea4b8e222", "node_type": "1", "metadata": {"page number": "22 - 25"}, "hash": "cb1d05e4623aab33d0d6fa52fbf2047c257f6f2e05470ff9c425d4fd2e2cc20e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "23448f74-5eef-4900-a80d-b7221740d012", "node_type": "1", "metadata": {}, "hash": "f8ff2be72e99194570ef9b8c630ca45742651c0042e9c0e11b103f0163719da2", "class_name": "RelatedNodeInfo"}}, "text": "Example: Identifying Price Elasticity of Demand:\nOne of the cornerstones of scienti\ufb01c methodologies is empirical analysis.5 By empirical analysis, I mean the use of data to test a the- ory or to estimate a relationship between variables. The \ufb01rst step in conducting an empirical economic analysis is the careful formulation of the question we would like to answer. In some cases, we would like to develop and test a formal economic model that describes mathe- matically a certain relationship, behavior, or process of interest. Those models are valuable insofar as they both describe the phenomena of\n5 It is not the only cornerstone, or even necessarily the most important corner-\nstone, but empirical analysis has always played an important role in scienti\ufb01c work.\nmaking some choice based on what she thinks is best, then it nec- essarily is based on potential outcomes, and the correlation does not remotely satisfy the conditions we need in order to say it is causal. To put it as bluntly as I can, economic theory says choices are endoge- nous, and therefore since they are, the correlations between those choices and outcomes in the aggregate will rarely, if ever, represent a causal effect.\nNow we are veering into the realm of epistemology. Identifying causal effects involves assumptions, but it also requires a particu- lar kind of belief about the work of scientists. Credible and valuable research requires that we believe that it is more important to do our work correctly than to try and achieve a certain outcome (e.g., con- \ufb01rmation bias, statistical signi\ufb01cance, asterisks). The foundations of scienti\ufb01c knowledge are scienti\ufb01c methodologies. True scientists do not collect evidence in order to prove what they want to be true or what others want to believe. That is a form of deception and manipulation called propaganda, and propaganda is not science. Rather, scienti\ufb01c methodologies are devices for forming a particular kind of belief. Sci- enti\ufb01c methodologies allow us to accept unexpected, and sometimes undesirable, answers. They are process oriented, not outcome ori- ented. And without these values, causal methodologies are also not believable.", "start_char_idx": 0, "end_char_idx": 2172, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "23448f74-5eef-4900-a80d-b7221740d012": {"__data__": {"id_": "23448f74-5eef-4900-a80d-b7221740d012", "embedding": null, "metadata": {"page number": "26 - 30"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0e501736-f232-4e9b-99cd-511596548668", "node_type": "4", "metadata": {"page number": "26 - 30"}, "hash": "460dec67b3988b4e9ba189d30731e17614d6e2c2638c1f227778ba52d0ff54e1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "26a865ee-5356-492d-8e0e-cf9ece65d9a0", "node_type": "1", "metadata": {"page number": "25 - 26"}, "hash": "67ebe3563d9acbf94604e8acad56993f7ad25ddbf1704524eeb0dee5c046a53f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "191c5854-0afe-43e5-b645-c69381dc7791", "node_type": "1", "metadata": {}, "hash": "afdc1ebb4e1e874c0bd950ef24ab35342632ea5d91b4881418e56019351eb879", "class_name": "RelatedNodeInfo"}}, "text": "Example: Identifying Price Elasticity of Demand:\nOne of the cornerstones of scienti\ufb01c methodologies is empirical analysis.5 By empirical analysis, I mean the use of data to test a the- ory or to estimate a relationship between variables. The \ufb01rst step in conducting an empirical economic analysis is the careful formulation of the question we would like to answer. In some cases, we would like to develop and test a formal economic model that describes mathe- matically a certain relationship, behavior, or process of interest. Those models are valuable insofar as they both describe the phenomena of\n5 It is not the only cornerstone, or even necessarily the most important corner-\nstone, but empirical analysis has always played an important role in scienti\ufb01c work.\ninterest and make falsi\ufb01able (testable) predictions. A prediction is fal- si\ufb01able insofar as we can evaluate, and potentially reject, the prediction with data.6 A model is the framework with which we describe the rela- tionships we are interested in, the intuition for our results, and the hypotheses we would like to test.7\nAfter we have speci\ufb01ed a model, we turn it into what is called an econometric model, which can be estimated directly with data. One clear issue we immediately face is regarding the functional form of the model, or how to describe the relationships of the variables we are interested in through an equation. Another important issue is how we will deal with variables that cannot be directly or reasonably observed by the researcher, or that cannot be measured very well, but which play an important role in our model.\nA generically important contribution to our understanding of causal inference is the notion of comparative statics. Comparative statics are theoretical descriptions of causal effects contained within the model. These kinds of comparative statics are always based on the idea of ceteris paribus\u2014or \u201call else constant.\u201d When we are trying to describe the causal effect of some intervention, for instance, we are always assuming that the other relevant variables in the model are not changing. If they were changing, then they would be correlated with the variable of interest and it would confound our estimation.8\nTo illustrate this idea, let\u2019s begin with a basic economic model: supply and demand equilibrium and the problems it creates for esti- mating the price elasticity of demand. Policy-makers and business managers have a natural interest in learning the price elasticity of\n6 You can also obtain a starting point for empirical analysis through an intuitive\nand less formal reasoning process. But economics favors formalism and deductive\n7 Scienti\ufb01c models, be they economic ones or otherwise, are abstract, not realis-\ntic, representations of the world. That is a strength, not a weakness. George Box, the\nstatistician, once quipped that \u201call models are wrong, but some are useful.\u201d A model\u2019s\nusefulness is its ability to unveil hidden secrets about the world. No more and no less.\n8 One of the things implied by ceteris paribus that comes up repeatedly in this book is the idea of covariate balance. If we say that everything is the same except for\nthe movement of one variable, then everything is the same on both sides of that vari- able\u2019s changing value. Thus, when we invoke ceteris paribus, we are implicitly invoking covariate balance\u2014both the observable and the unobservable covariates.\ndemand because knowing it enables \ufb01rms to maximize pro\ufb01ts and governments to choose optimal taxes, and whether to restrict quan- tity altogether [Becker et al., 2006]. But the problem is that we do not observe demand curves, because demand curves are theoretical objects. More speci\ufb01cally, a demand curve is a collection of paired potential outcomes of price and quantity. We observe price and quan- tity equilibrium values, not the potential price and potential quanti- ties along the entire demand curve. Only by tracing out the potential outcomes along a demand curve can we calculate the elasticity.\nTo see this, consider this graphic from Philip Wright\u2019s Appendix B [Wright, 1928], which we\u2019ll discuss in greater detail later (Figure 2). The price elasticity of demand is the ratio of percentage changes in quantity to price for a single demand curve. Yet, when there are shifts in supply and demand, a sequence of quantity and price pairs emerges in history that re\ufb02ect neither the demand curve nor the supply curve. In fact, connecting the points does not re\ufb02ect any meaningful or useful object.\nFigure 2. Wright\u2019s graphical demonstration of the identi\ufb01cation problem. Figure from Wright, P. G. (1928). The Tariff on Animal and Vegetable Oils.", "start_char_idx": 0, "end_char_idx": 4662, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "191c5854-0afe-43e5-b645-c69381dc7791": {"__data__": {"id_": "191c5854-0afe-43e5-b645-c69381dc7791", "embedding": null, "metadata": {"page number": "26 - 30"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0e501736-f232-4e9b-99cd-511596548668", "node_type": "4", "metadata": {"page number": "26 - 30"}, "hash": "460dec67b3988b4e9ba189d30731e17614d6e2c2638c1f227778ba52d0ff54e1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "23448f74-5eef-4900-a80d-b7221740d012", "node_type": "1", "metadata": {"page number": "26 - 30"}, "hash": "02cb95b5431a07f6206112465bd557e2b3760139ab82fc2dc35a1b9bdadf9ba3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b38b4017-cade-407c-be15-4aab4ea8e80e", "node_type": "1", "metadata": {}, "hash": "f4b74b50fd0266cff257cca4cb49227348eb855a7164b241f53004a9fac48c05", "class_name": "RelatedNodeInfo"}}, "text": "Only by tracing out the potential outcomes along a demand curve can we calculate the elasticity.\nTo see this, consider this graphic from Philip Wright\u2019s Appendix B [Wright, 1928], which we\u2019ll discuss in greater detail later (Figure 2). The price elasticity of demand is the ratio of percentage changes in quantity to price for a single demand curve. Yet, when there are shifts in supply and demand, a sequence of quantity and price pairs emerges in history that re\ufb02ect neither the demand curve nor the supply curve. In fact, connecting the points does not re\ufb02ect any meaningful or useful object.\nFigure 2. Wright\u2019s graphical demonstration of the identi\ufb01cation problem. Figure from Wright, P. G. (1928). The Tariff on Animal and Vegetable Oils. The Macmillan Company.\nThe price elasticity of demand is the solution to the following\nBut in this example, the change in P is exogenous. For instance, it holds supply \ufb01xed, the prices of other goods \ufb01xed, income \ufb01xed, pref- erences \ufb01xed, input costs \ufb01xed, and so on. In order to estimate the price elasticity of demand, we need changes in P that are completely and utterly independent of the otherwise normal determinants of sup- ply and the other determinants of demand. Otherwise we get shifts in either supply or demand, which creates new pairs of data for which any correlation between P and Q will not be a measure of the elasticity of demand.\nThe problem is that the elasticity is an important object, and we need to know it, and therefore we need to solve this problem. So given this theoretical object, we must write out an econometric model as a starting point. One possible example of an econometric model would be a linear demand function:\nwhere \u03b1 is the intercept, \u03b4 is the elasticity of demand, X is a matrix of factors that determine demand like the prices of other goods or income, \u03b3 is the coe\ufb03cient on the relationship between X and Qd, and u is the error term.9\nForeshadowing the content of this mixtape, we need two things to estimate price elasticity of demand. First, we need numerous rows of data on price and quantity. Second, we need for the vari- ation in price in our imaginary data set to be independent of u. We call this kind of independence exogeneity. Without both, we cannot recover the price elasticity of demand, and therefore any decision that requires that information will be based on stabs in the dark.", "start_char_idx": 3919, "end_char_idx": 6304, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b38b4017-cade-407c-be15-4aab4ea8e80e": {"__data__": {"id_": "b38b4017-cade-407c-be15-4aab4ea8e80e", "embedding": null, "metadata": {"page number": "30 - 31"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3dc624e4-a621-459b-9343-5c9d0651aa5e", "node_type": "4", "metadata": {"page number": "30 - 31"}, "hash": "0a7818dd53869f65760a3fc864a23c7433439aaaf8b73e53b21e76ac4e961e7c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "191c5854-0afe-43e5-b645-c69381dc7791", "node_type": "1", "metadata": {"page number": "26 - 30"}, "hash": "e07b77d8922dc278d5e083538c98c95de7cb93f85946dd21ae3680b204138881", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6361436a-9595-4826-98a4-0d4a8fe63e6a", "node_type": "1", "metadata": {}, "hash": "3b26182cf4f0685bc36a8e4926868692f4cfbc278aabf980772af564eee5d54d", "class_name": "RelatedNodeInfo"}}, "text": "Conclusion:\nThis book is an introduction to research designs that can recover causal effects. But just as importantly, it provides you with hands- on practice to implement these designs. Implementing these designs means writing code in some type of software. I have chosen to illus- trate these designs using two popular software languages: Stata (most commonly used by economists) and R (most commonly used by everyone else).\nThe book contains numerous empirical exercises illustrated in the Stata and R programs. These exercises are either simulations (which don\u2019t need external data) or exercises requiring external data. The data needed for the latter have been made available to you at Github. The Stata examples will download \ufb01les usually at the start of the program using the following command: use https://github. com/scunning1975/mixtape/raw/master/DATAFILENAME.DTA, where DATAFILENAME.DTA is the name of a particular data set.\nFor R users, it is a somewhat different process to load data into memory. In an effort to organize and clean the code, my students Hugo Sant\u2019Anna and Terry Tsai created a function to simplify the data down- load process. This is partly based on a library called haven, which is a package for reading data \ufb01les. It is secondly based on a set of com- mands that create a function that will then download the data directly from Github.10\nSome readers may not be familiar with either Stata or R but nonetheless wish to follow along. I encourage you to use this oppor- tunity to invest in learning one or both of these languages. It is beyond the scope of this book to provide an introduction to these languages, but fortunately, there are numerous resources online. For instance, Christopher Baum has written an excellent introduction to Stata at https://fmwww.bc.edu/GStat/docs/StataIntro.pdf. Stata is popular among microeconomists, and given the amount of coauthor- ing involved in modern economic research, an argument could be\n10 This was done solely for aesthetic reasons. Often the URL was simply too long\nmade for investing in it solely for its ability to solve basic coordina- tion problems between you and potential coauthors. But a downside to Stata is that it is proprietary and must be purchased. And for some people, that may simply be too big of a barrier\u2014especially for anyone simply wanting to follow along with the book. R on the other hand is open-source and free. Tutorials on Basic R can be found at https://cran .r-project.org/doc/contrib/Paradis-rdebuts_en.pdf, and an introduc- tion to Tidyverse (which is used throughout the R programming) can be found at https://r4ds.had.co.nz. Using this time to learn R would likely be well worth your time.\nPerhaps you already know R and want to learn Stata. Or perhaps you know Stata and want to learn R. Then this book may be helpful because of the way in which both sets of code are put in sequence to accomplish the same basic tasks. But, with that said, in many sit- uations, although I have tried my best to reconcile results from Stata and R, I was not always able to do so. Ultimately, Stata and R are dif- ferent programming languages that sometimes yield different results because of different optimization procedures or simply because the programs are built slightly differently. This has been discussed occa- sionally in articles in which authors attempt to better understand what accounts for the differing results. I was not always able to fully recon- cile different results, and so I offer the two programs as simply alterna- tive approaches. You are ultimately responsible for anything you do on your own using either language for your research. I leave it to you ulti- mately to understand the method and estimating procedure contained within a given software and package.\nIn conclusion, simply \ufb01nding an association between two variables might be suggestive of a causal effect, but it also might not. Correla- tion doesn\u2019t mean causation unless key assumptions hold. Before we start digging into the causal methodologies themselves, though, I need to lay down a foundation in statistics and regression modeling. Buckle up! This is going to be fun.", "start_char_idx": 0, "end_char_idx": 4162, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6361436a-9595-4826-98a4-0d4a8fe63e6a": {"__data__": {"id_": "6361436a-9595-4826-98a4-0d4a8fe63e6a", "embedding": null, "metadata": {"page number": "32 - 111"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac", "node_type": "4", "metadata": {"page number": "32 - 111"}, "hash": "99ad2284755197ab9a1483b8b716ac5e3c39d1980b4b879df9d1e92b96483fd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b38b4017-cade-407c-be15-4aab4ea8e80e", "node_type": "1", "metadata": {"page number": "30 - 31"}, "hash": "a87d3d3feb04c84a73e467e489432d8e48564b888ef05a5e6d9a63ddaa928eda", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f8f892c-762f-4cfc-a2c6-0d93ade12309", "node_type": "1", "metadata": {}, "hash": "701e003123a29733617fa65d583ac34682cd6b12c87e296ff1af0db091773373", "class_name": "RelatedNodeInfo"}}, "text": "Probability and Regression Review:\nNumbers is hardly real and they never have feelings. But you push too hard, even numbers got limits.\nBasic probability theory. In practice, causal inference is based on sta- tistical models that range from the very simple to extremely advanced. And building such models requires some rudimentary knowledge of probability theory, so let\u2019s begin with some de\ufb01nitions. A random pro- cess is a process that can be repeated many times with different out- comes each time. The sample space is the set of all the possible out- comes of a random process. We distinguish between discrete and con- tinuous random processes Table 1 below. Discrete processes produce, integers, whereas continuous processes produce fractions as well.\nWe de\ufb01ne independent events two ways. The \ufb01rst refers to logical independence. For instance, two events occur but there is no reason to believe that the two events affect each other. When it is assumed that they do affect each other, this is a logical fallacy called post hoc ergo propter hoc, which is Latin for \u201cafter this, therefore because of this.\u201d This fallacy recognizes that the temporal ordering of events is not su\ufb03cient to be able to say that the \ufb01rst thing caused the second.\nThe second de\ufb01nition of an independent event is statistical inde- pendence. We\u2019ll illustrate the latter with an example from the idea of sampling with and without replacement. Let\u2019s use a randomly shuf- \ufb02ed deck of cards for an example. For a deck of 52 cards, what is the probability that the \ufb01rst card will be an ace?\nThere are 52 possible outcomes in the sample space, or the set of all possible outcomes of the random process. Of those 52 possible outcomes, we are concerned with the frequency of an ace occurring.\n4 52 Assume that the \ufb01rst card was an ace. Now we ask the ques- tion again. If we shu\ufb04e the deck, what is the probability the next card\nThere are four aces in the deck, so\ndrawn is also an ace? It is no longer\nwith replacement. We sampled without replacement. Thus the new probability is\nbecause we did not sample\nUnder sampling without replacement, the two events\u2014ace on Card 1 and an ace on Card 2 if Card 1 was an ace\u2014aren\u2019t independent events. To make the two events independent, you would have to put the ace back and shu\ufb04e the deck. So two events, A and B, are independent if and only if:\nAn example of two independent events would be rolling a 5 with one die after having rolled a 3 with another die. The two events are\nindependent, so the probability of rolling a 5 is always 0.17 regardless of what we rolled on the \ufb01rst die.1\nBut what if we want to know the probability of some event occur- ring that requires that multiple events \ufb01rst to occur? For instance, let\u2019s say we\u2019re talking about the Cleveland Cavaliers winning the NBA cham- pionship. In 2016, the Golden State Warriors were 3\u20131 in a best-of- seven playoff. What had to happen for the Warriors to lose the playoff? The Cavaliers had to win three in a row. In this instance, to \ufb01nd the probability, we have to take the product of all marginal probabilities, or Pr(\u00b7)n, where Pr(\u00b7) is the marginal probability of one event occurring, and n is the number of repetitions of that one event. If the unconditional probability of a Cleveland win is 0.5, and each game is independent, then the probability that Cleveland could come back from a 3\u20131 de\ufb01cit is the product of each game\u2019s probability of winning:\nAnother example may be helpful. In Texas Hold\u2019em poker, each player is dealt two cards facedown. When you are holding two of a kind, you say you have two \u201cin the pocket.\u201d So, what is the probability of being\ndealt pocket aces? It\u2019s\nLet\u2019s formalize what we\u2019ve been saying for a more generalized case. For independent events, to calculate joint probabilities, we multi- ply the marginal probabilities:\nwhere Pr(A, B) is the joint probability of both A and B occurring, and Pr(A) is the marginal probability of A event occurring.\nNow, for a slightly more di\ufb03cult application. What is the probability of rolling a 7 using two six-sided dice, and is it the same as the proba- bility of rolling a 3? To answer this, let\u2019s compare the two probabilities.", "start_char_idx": 0, "end_char_idx": 4181, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9f8f892c-762f-4cfc-a2c6-0d93ade12309": {"__data__": {"id_": "9f8f892c-762f-4cfc-a2c6-0d93ade12309", "embedding": null, "metadata": {"page number": "32 - 111"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac", "node_type": "4", "metadata": {"page number": "32 - 111"}, "hash": "99ad2284755197ab9a1483b8b716ac5e3c39d1980b4b879df9d1e92b96483fd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6361436a-9595-4826-98a4-0d4a8fe63e6a", "node_type": "1", "metadata": {"page number": "32 - 111"}, "hash": "ee003d97319a4a5ed9fb2ac20d0dbeb2c3d42aa8ecaea6c3eb3a5c007090ab94", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5ec549e9-4256-4d62-ab5e-27a400c61620", "node_type": "1", "metadata": {}, "hash": "d6919c472e36ac27f50882ce50dc81a0765250fdebddf5af863a6308ae4c96bb", "class_name": "RelatedNodeInfo"}}, "text": "In Texas Hold\u2019em poker, each player is dealt two cards facedown. When you are holding two of a kind, you say you have two \u201cin the pocket.\u201d So, what is the probability of being\ndealt pocket aces? It\u2019s\nLet\u2019s formalize what we\u2019ve been saying for a more generalized case. For independent events, to calculate joint probabilities, we multi- ply the marginal probabilities:\nwhere Pr(A, B) is the joint probability of both A and B occurring, and Pr(A) is the marginal probability of A event occurring.\nNow, for a slightly more di\ufb03cult application. What is the probability of rolling a 7 using two six-sided dice, and is it the same as the proba- bility of rolling a 3? To answer this, let\u2019s compare the two probabilities. We\u2019ll use a table to help explain the intuition. First, let\u2019s look at all the ways to get a 7 using two six-sided dice. There are 36 total possible outcomes (62 = 36) when rolling two dice. In Table 2 we see that there are six different ways to roll a 7 using only two dice. So the probability\n1 The probability rolling a 5 using one six-sided die is\nTable 2. Total number of ways to get a 7 with two six-sided dice.\nTable 3. Total number of ways to get a 3 using two six-sided dice.\nof rolling a 7 is 6/36 = 16.67%. Next, let\u2019s look at all the ways to roll a 3 using two six-sided dice. Table 3 shows that there are only two ways to get a 3 rolling two six-sided dice. So the probability of rolling a 3 is 2/36 = 5.56%. So, no, the probabilities of rolling a 7 and rolling a 3 are different.\nEvents and conditional probability. First, before we talk about the three ways of representing a probability, I\u2019d like to introduce some new ter- minology and concepts: events and conditional probabilities. Let A be some event. And let B be some other event. For two events, there are four possibilities.\n1. A and B: Both A and B occur. 2. \u223c A and B: A does not occur, but B occurs. 3. A and \u223c B: A occurs, but B does not occur. 4. \u223c A and \u223c B: Neither A nor B occurs.\nI\u2019ll use a couple of different examples to illustrate how to represent a probability.\nProbability tree. Let\u2019s think about a situation in which you are trying to get your driver\u2019s license. Suppose that in order to get a driver\u2019s license, you have to pass the written exam and the driving exam. However, if you fail the written exam, you\u2019re not allowed to take the driving exam. We can represent these two events in a probability tree.\nProbability trees are intuitive and easy to interpret.2 First, we see that the probability of passing the written exam is 0.75 and the proba- bility of failing the exam is 0.25. Second, at every branching off from a node, we can further see that the probabilities associated with a given branch are summing to 1.0. The joint probabilities are also all summing to 1.0. This is called the law of total probability and it is equal to the sum of all joint probability of A and Bn events occurring:\nWe also see the concept of a conditional probability in the driver\u2019s license tree. For instance, the probability of failing the driving exam, is represented as conditional on having passed the written exam, Pr(Fail | Pass) = 0.45.\nVenn diagrams and sets. A second way to represent multiple events occurring is with a Venn diagram. Venn diagrams were \ufb01rst conceived by John Venn in 1880. They are used to teach elementary set theory, as well as to express set relationships in probability and statistics. This example will involve two sets, A and B.\n2 The set notation \u222a means \u201cunion\u201d and refers to two events occurring together.\nThe University of Texas\u2019s football coach has been on the razor\u2019s edge with the athletic director and regents all season. After several mediocre seasons, his future with the school is in jeopardy. If the Longhorns don\u2019t make it to a great bowl game, he likely won\u2019t be rehired. But if they do, then he likely will be rehired. Let\u2019s discuss ele- mentary set theory using this coach\u2019s situation as our guiding example. But before we do, let\u2019s remind ourselves of our terms.", "start_char_idx": 3467, "end_char_idx": 7471, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5ec549e9-4256-4d62-ab5e-27a400c61620": {"__data__": {"id_": "5ec549e9-4256-4d62-ab5e-27a400c61620", "embedding": null, "metadata": {"page number": "32 - 111"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac", "node_type": "4", "metadata": {"page number": "32 - 111"}, "hash": "99ad2284755197ab9a1483b8b716ac5e3c39d1980b4b879df9d1e92b96483fd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9f8f892c-762f-4cfc-a2c6-0d93ade12309", "node_type": "1", "metadata": {"page number": "32 - 111"}, "hash": "00ada188524e021de27d1954cbabadc7cb4aa19fd27132e08f3c2179cc4b0727", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ae7fb1c9-03ef-41a0-a8ff-fbb0200e9ab8", "node_type": "1", "metadata": {}, "hash": "b60d1c814428fb21224be5075f86932b2c1771e4544da3434ffe29ae82bd7bcc", "class_name": "RelatedNodeInfo"}}, "text": "A second way to represent multiple events occurring is with a Venn diagram. Venn diagrams were \ufb01rst conceived by John Venn in 1880. They are used to teach elementary set theory, as well as to express set relationships in probability and statistics. This example will involve two sets, A and B.\n2 The set notation \u222a means \u201cunion\u201d and refers to two events occurring together.\nThe University of Texas\u2019s football coach has been on the razor\u2019s edge with the athletic director and regents all season. After several mediocre seasons, his future with the school is in jeopardy. If the Longhorns don\u2019t make it to a great bowl game, he likely won\u2019t be rehired. But if they do, then he likely will be rehired. Let\u2019s discuss ele- mentary set theory using this coach\u2019s situation as our guiding example. But before we do, let\u2019s remind ourselves of our terms. A and B are events, and U is the universal set of which A and B are subsets. Let A be the probability that the Longhorns get invited to a great bowl game and B be the probability that their coach is rehired. Let Pr(A) = 0.6 and let Pr(B) = 0.8. Let the probability that both A and B occur be Pr(A, B) = 0.5. Note, that A+ \u223cA = U, where \u223c A is the complement of A. The complement means that it is everything in the universal set that is not A. The same is said of B. The sum of B and \u223c B = U. Therefore:\nWe can rewrite out the following de\ufb01nitions:\nWhenever we want to describe a set of events in which either A or B could occur, it is: A \u222a B. And this is pronounced \u201cA union B,\u201d which means it is the new set that contains every element from A and every element from B. Any element that is in either set A or set B, then, is also in the new union set. And whenever we want to describe a set of events that occurred together\u2014the joint set\u2014it\u2019s A \u2229 B, which is pronounced \u201cA intersect B.\u201d This new set contains every element that is in both the A and B sets. That is, only things inside both A and B get added to the new set.\nNow let\u2019s look closely at a relationship involving the set A.\nNotice what this is saying: there are two ways to identify the A set. First, you can look at all the instances where A occurs with B. But then what about the rest of A that is not in B? Well, that\u2019s the A \u222a B situation, which covers the rest of the A set.\nA similar style of reasoning can help you understand the following\nTo get the A intersect B, we need three objects: the set of A units out- side of B, the set of B units outside A, and their joint set. You get all those, and you have A \u2229 B.\nNow it is just simple addition to \ufb01nd all missing values. Recall that A is your team making playoffs and Pr(A) = 0.6. And B is the probability that the coach is rehired, Pr(B) = 0.8. Also, Pr(A, B) = 0.5, which is the probability of both A and B occurring. Then we have:\nWhen working with sets, it is important to understand that probability is calculated by considering the share of the set (for example A) made up by the subset (for example A \u222a B). When we write down that the probability that A \u222a B occurs at all, it is with regards to U. But what if we were to ask the question \u201cWhat share of A is due to A \u222a B?\u201d Notice, then, that we would need to do this:\nCoach is rehired (\u223c B)\nCoach is not rehired Total (B)\nI left this intentionally unde\ufb01ned on the left side so as to focus on the calculation itself. But now let\u2019s de\ufb01ne what we are wanting to calculate: In a world where A has occurred, what is the probability that B will also occur? This is:\nNotice, these conditional probabilities are not as easy to see in the Venn diagram. We are essentially asking what percentage of a subset\u2014 e.g., Pr(A)\u2014is due to the joint set, for example, Pr(A, B). This reasoning is the very same reasoning used to de\ufb01ne the concept of a conditional probability.\nContingency tables. Another way that we can represent events is with a contingency table. Contingency tables are also sometimes called twoway tables.", "start_char_idx": 6627, "end_char_idx": 10555, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ae7fb1c9-03ef-41a0-a8ff-fbb0200e9ab8": {"__data__": {"id_": "ae7fb1c9-03ef-41a0-a8ff-fbb0200e9ab8", "embedding": null, "metadata": {"page number": "32 - 111"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac", "node_type": "4", "metadata": {"page number": "32 - 111"}, "hash": "99ad2284755197ab9a1483b8b716ac5e3c39d1980b4b879df9d1e92b96483fd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5ec549e9-4256-4d62-ab5e-27a400c61620", "node_type": "1", "metadata": {"page number": "32 - 111"}, "hash": "cf67c41e95c9ba564d61e0bde612773eeea6d309af38dc0c75bf992cba6fb8f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "00220262-ff96-47a7-94ce-47181c19cd06", "node_type": "1", "metadata": {}, "hash": "d2f2a834e6aa2f0f93553e5db9414f54f7b5d6fabb30b4b8ba5620e20e227a4f", "class_name": "RelatedNodeInfo"}}, "text": "But now let\u2019s de\ufb01ne what we are wanting to calculate: In a world where A has occurred, what is the probability that B will also occur? This is:\nNotice, these conditional probabilities are not as easy to see in the Venn diagram. We are essentially asking what percentage of a subset\u2014 e.g., Pr(A)\u2014is due to the joint set, for example, Pr(A, B). This reasoning is the very same reasoning used to de\ufb01ne the concept of a conditional probability.\nContingency tables. Another way that we can represent events is with a contingency table. Contingency tables are also sometimes called twoway tables. Table 4 is an example of a contingency table. We continue with our example about the worried Texas coach.\nRecall that Pr(A) = 0.6, Pr(B) = 0.8, and Pr(A, B) = 0.5. Note that to calculate conditional probabilities, we must know the frequency of the element in question (e.g., Pr(A, B)) relative to some other larger event (e.g., Pr(A)). So if we want to know what the conditional probability of B is given A, then it\u2019s:\nBut note that knowing the frequency of A \u222a B in a world where B occurs is to ask the following:\nSo, we can use what we have done so far to write out a de\ufb01nition of joint probability. Let\u2019s start with a de\ufb01nition of conditional probability \ufb01rst. Given two events, A and B:\nUsing equations 2.1 and 2.2, I can simply write down a de\ufb01nition of joint probabilities.\nAnd this is the formula for joint probability. Given equation 2.3, and using the de\ufb01nitions of (Pr(A, B and Pr(B, A)), I can also rearrange terms, make a substitution, and rewrite it as:\nEquation 2.8 is sometimes called the naive version of Bayes\u2019s rule. We will now decompose this equation more fully, though, by substituting equation 2.5 into equation 2.8.\nSubstituting equation 2.6 into the denominator for equation 2.9 yields:\nFinally, we note that using the de\ufb01nition of joint probability, that Pr(B, \u223c A) = Pr(B |\u223c A) Pr(\u223c A), which we substitute into the denominator of equation 2.10 to get:\nThat\u2019s a mouthful of substitutions, so what does equation 2.11 mean? This is the Bayesian decomposition version of Bayes\u2019s rule. Let\u2019s use our example again of Texas making a great bowl game. A is Texas mak- ing a great bowl game, and B is the coach getting rehired. And A \u2229 B is the joint probability that both events occur. We can make each calcu- lation using the contingency tables. The questions here is this: If the Texas coach is rehired, what\u2019s the probability that the Longhorns made a great bowl game? Or formally, Pr(A | B). We can use the Bayesian decomposition to \ufb01nd this probability.\nCheck this against the contingency table using the de\ufb01nition of joint probability:\nSo, if the coach is rehired, there is a 63 percent chance we made a great bowl game.3\nMonty Hall example. Let\u2019s use a different example, the Monty Hall example. This is a fun one, because most people \ufb01nd it counterintu-\n3 Why are they different? Because 0.83 is an approximation of Pr(B | A), which was\nitive. It even is used to stump mathematicians and statisticians.4 But Bayes\u2019s rule makes the answer very clear\u2014so clear, in fact, that it\u2019s somewhat surprising that Bayes\u2019s rule was actually once controversial [McGrayne, 2012].\nLet\u2019s assume three closed doors: door 1 (D1), door 2 (D2), and door 3 (D3). Behind one of the doors is a million dollars. Behind each of the other two doors is a goat. Monty Hall, the game-show host in this example, asks the contestants to pick a door. After they pick the door, but before he opens the door they picked, he opens one of the other doors to reveal a goat. He then asks the contestant, \u201cWould you like to switch doors?\u201d\nA common response to Monty Hall\u2019s offer is to say it makes no sense to change doors, because there\u2019s an equal chance that the mil- lion dollars is behind either door. Therefore, why switch?", "start_char_idx": 9965, "end_char_idx": 13772, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "00220262-ff96-47a7-94ce-47181c19cd06": {"__data__": {"id_": "00220262-ff96-47a7-94ce-47181c19cd06", "embedding": null, "metadata": {"page number": "32 - 111"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac", "node_type": "4", "metadata": {"page number": "32 - 111"}, "hash": "99ad2284755197ab9a1483b8b716ac5e3c39d1980b4b879df9d1e92b96483fd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ae7fb1c9-03ef-41a0-a8ff-fbb0200e9ab8", "node_type": "1", "metadata": {"page number": "32 - 111"}, "hash": "22282a63411cb9223ab2ee2ea9595b9f6feabef72fe78806841376b5cf503e24", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "871d9c76-eb20-42bf-82eb-854ca19267ad", "node_type": "1", "metadata": {}, "hash": "5949546cb9c1537f071cc339495e34a3411f2ca5ae513690f035f8eeb8fb218e", "class_name": "RelatedNodeInfo"}}, "text": "Let\u2019s assume three closed doors: door 1 (D1), door 2 (D2), and door 3 (D3). Behind one of the doors is a million dollars. Behind each of the other two doors is a goat. Monty Hall, the game-show host in this example, asks the contestants to pick a door. After they pick the door, but before he opens the door they picked, he opens one of the other doors to reveal a goat. He then asks the contestant, \u201cWould you like to switch doors?\u201d\nA common response to Monty Hall\u2019s offer is to say it makes no sense to change doors, because there\u2019s an equal chance that the mil- lion dollars is behind either door. Therefore, why switch? There\u2019s a 50\u201350 chance it\u2019s behind the door picked and there\u2019s a 50\u201350 chance it\u2019s behind the remaining door, so it makes no rational sense to switch. Right? Yet, a little intuition should tell you that\u2019s not the right answer, because it would seem that when Monty Hall opened that third door, he made a statement. But what exactly did he say?\nLet\u2019s formalize the problem using our probability notation. Assume that you chose door 1, D1. The probability that D1 had a million dollars when you made that choice is Pr(D1 = 1 million) = 1 3 event A1. And the probability that D1 has a million dollars at the start of\nthe game is a million dollars behind it. Thus, Pr(A1) = 1 3 probability, Pr(\u223c A1) = 2 3 D2, to reveal a goat. Then he asked, \u201cWould you like to change to door number 3?\u201d\nbecause the sample space is 3 doors, of which one has\n. Let\u2019s say that Monty Hall had opened door 2,\n. We will call that\nWe need to know the probability that door 3 has the million dollars and compare that to Door 1\u2019s probability. We will call the opening of\n4 There\u2019s an ironic story in which someone posed the Monty Hall question to the\ncolumnist, Marilyn vos Savant. Vos Savant had an extremely high IQ and so people would\nsend in puzzles to stump her. Without the Bayesian decomposition, using only logic, she got the answer right. Her column enraged people, though. Critics wrote in to mansplain\nhow wrong she was, but in fact it was they who were wrong.\ndoor 2 event B. We will call the probability that the million dollars is behind door i, Ai. We now write out the question just asked formally and decompose it using the Bayesian decomposition. We are ultimately interested in knowing what the probability is that door 1 has a million dollars (event A1) given that Monty Hall opened door 2 (event B), which is a conditional probability question. Let\u2019s write out that conditional probability using the Bayesian decomposition from equation 2.11.\nThere are basically two kinds of probabilities on the right side of the equation. There\u2019s the marginal probability that the million dollars is behind a given door, Pr(Ai). And there\u2019s the conditional probability that Monty Hall would open door 2 given that the million dollars is behind door Ai, Pr(B | Ai).\nit without our having any additional information is\nprior probability, or prior belief. It may also be called the unconditional probability.\nThe marginal probability that door i has the million dollars behind\n. We call this the\nThe conditional probability, Pr(B|Ai), requires a little more care- ful thinking. Take the \ufb01rst conditional probability, Pr(B | A1). If door 1 has the million dollars behind it, what\u2019s the probability that Monty Hall would open door 2?\nLet\u2019s think about the second conditional probability: Pr(B | A2). If the money is behind door 2, what\u2019s the probability that Monty Hall would open door 2?\nAnd then the last conditional probability, Pr(B | A3). In a world where the money is behind door 3, what\u2019s the probability Monty Hall will open door 2?\nEach of these conditional probabilities requires thinking carefully about the feasibility of the events in question. Let\u2019s examine the easiest question: Pr(B | A2). If the money is behind door 2, how likely is it for Monty Hall to open that same door, door 2? Keep in mind: this is a game show. So that gives you some idea about how the game-show host will behave. Do you think Monty Hall would open a door that had the million\ndollars behind it?", "start_char_idx": 13149, "end_char_idx": 17234, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "871d9c76-eb20-42bf-82eb-854ca19267ad": {"__data__": {"id_": "871d9c76-eb20-42bf-82eb-854ca19267ad", "embedding": null, "metadata": {"page number": "32 - 111"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac", "node_type": "4", "metadata": {"page number": "32 - 111"}, "hash": "99ad2284755197ab9a1483b8b716ac5e3c39d1980b4b879df9d1e92b96483fd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "00220262-ff96-47a7-94ce-47181c19cd06", "node_type": "1", "metadata": {"page number": "32 - 111"}, "hash": "05d517cc84e11a0a6667578b790ef86dea46ad4fab9f2308f61d1913d1c0c00d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "41de045d-dcd8-4859-beaa-a5b245df59ab", "node_type": "1", "metadata": {}, "hash": "868bb88a47eddde6e60bf316b99692df36687572a7d1a41ad31fc02efab08023", "class_name": "RelatedNodeInfo"}}, "text": "Let\u2019s think about the second conditional probability: Pr(B | A2). If the money is behind door 2, what\u2019s the probability that Monty Hall would open door 2?\nAnd then the last conditional probability, Pr(B | A3). In a world where the money is behind door 3, what\u2019s the probability Monty Hall will open door 2?\nEach of these conditional probabilities requires thinking carefully about the feasibility of the events in question. Let\u2019s examine the easiest question: Pr(B | A2). If the money is behind door 2, how likely is it for Monty Hall to open that same door, door 2? Keep in mind: this is a game show. So that gives you some idea about how the game-show host will behave. Do you think Monty Hall would open a door that had the million\ndollars behind it? It makes no sense to think he\u2019d ever open a door that\nactually had the money behind it\u2014he will always open a door with a\ngoat. So don\u2019t you think he\u2019s only opening doors with goats? Let\u2019s see\nwhat happens if take that intuition to its logical extreme and conclude\nthat Monty Hall never opens a door if it has a million dollars. He only\nopens a door if the door has a goat. Under that assumption, we can proceed to estimate Pr(A1 | B) by substituting values for Pr(B | Ai) and Pr(Ai) into the right side of equation 2.12.\nWhat then is Pr(B | A1)? That is, in a world where you have chosen door 1, and the money is behind door 1, what is the probability that he\nwould open door 2? There are two doors he could open if the money\nis behind door 1\u2014he could open either door 2 or door 3, as both have a goat behind them. So Pr(B | A1) = 0.5.\nWhat about the second conditional probability, Pr(B | A2)? If the money is behind door 2, what\u2019s the probability he will open it? Under\nour assumption that he never opens the door if it has a million dol-\nlars, we know this probability is 0.0. And \ufb01nally, what about the third probability, Pr(B | A3)? What is the probability he opens door 2 given that the money is behind door 3? Now consider this one carefully\u2014\nthe contestant has already chosen door 1, so he can\u2019t open that one.\nAnd he can\u2019t open door 3, because that has the money behind it. The\nonly door, therefore, he could open is door 2. Thus, this probability is 1.0. Furthermore, all marginal probabilities, Pr(Ai), equal 1/3, allow- ing us to solve for the conditional probability on the left side through\nAha. Now isn\u2019t that just a little bit surprising? The probability that the\ncontestant chose the correct door is\nopened door 2.\n, just as it was before Monty Hall\nBut what about the probability that door 3, the door you\u2019re holding,\nhas the million dollars? Have your beliefs about that likelihood changed\nnow that door 2 has been removed from the equation? Let\u2019s crank\nthrough our Bayesian decomposition and see whether we learned\nInterestingly, while your beliefs about the door you originally chose\nhaven\u2019t changed, your beliefs about the other door have changed. The prior probability, Pr(A3) = 1 3 ing to a new probability of Pr(A3 | B) = 2 3 bility is called the posterior probability, or posterior belief. And it simply\n, increased through a process called updat-\nmeans that having witnessed B, you learned information that allowed\nyou to form a new belief about which door the money might be behind.\nAs was mentioned in footnote 14 regarding the controversy around\nvos Sant\u2019s correct reasoning about the need to switch doors, deduc-\ntions based on Bayes\u2019s rule are often surprising even to smart people\u2014\nprobably because we lack coherent ways to correctly incorporate infor-\nmation into probabilities. Bayes\u2019s rule shows us how to do that in a\nway that is logical and accurate. But besides being insightful, Bayes\u2019s\nrule also opens the door for a different kind of reasoning about cause\nand effect. Whereas most of this book has to do with estimating\neffects from known causes, Bayes\u2019s rule reminds us that we can form\nreasonable beliefs about causes from known effects.\nSummation operator. The tools we use to reason about causality rest atop a bedrock of probabilities.", "start_char_idx": 16481, "end_char_idx": 20517, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "41de045d-dcd8-4859-beaa-a5b245df59ab": {"__data__": {"id_": "41de045d-dcd8-4859-beaa-a5b245df59ab", "embedding": null, "metadata": {"page number": "32 - 111"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac", "node_type": "4", "metadata": {"page number": "32 - 111"}, "hash": "99ad2284755197ab9a1483b8b716ac5e3c39d1980b4b879df9d1e92b96483fd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "871d9c76-eb20-42bf-82eb-854ca19267ad", "node_type": "1", "metadata": {"page number": "32 - 111"}, "hash": "4eabc8fbd8b0bfdeaa725fe089919a36d6fed3b3a45c0e2b20e58654d4c09f0f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6324ae8-415b-4378-92cf-d521c070a73a", "node_type": "1", "metadata": {}, "hash": "56f842e77880e0e6e79abb1cb552477fac8058ba4dd69ecbf3139eb425e1b8fe", "class_name": "RelatedNodeInfo"}}, "text": "As was mentioned in footnote 14 regarding the controversy around\nvos Sant\u2019s correct reasoning about the need to switch doors, deduc-\ntions based on Bayes\u2019s rule are often surprising even to smart people\u2014\nprobably because we lack coherent ways to correctly incorporate infor-\nmation into probabilities. Bayes\u2019s rule shows us how to do that in a\nway that is logical and accurate. But besides being insightful, Bayes\u2019s\nrule also opens the door for a different kind of reasoning about cause\nand effect. Whereas most of this book has to do with estimating\neffects from known causes, Bayes\u2019s rule reminds us that we can form\nreasonable beliefs about causes from known effects.\nSummation operator. The tools we use to reason about causality rest atop a bedrock of probabilities. We are often working with mathematical tools and concepts from statistics such as expectations and probabilities. One of the most common tools we will use in this book is the linear regression model, but before we can dive into that, we have to build out some simple notation.5 We\u2019ll begin with the summa- tion operator. The Greek letter (cid:7) (the capital Sigma) denotes the sum- mation operator. Let x1, x2, . . . , xn be a sequence of numbers. We can compactly write a sum of numbers using the summation operator as:\nThe letter i is called the index of summation. Other letters, such as j or k, are sometimes used as indices of summation. The subscript variable simply represents a speci\ufb01c value of a random variable, x. The num- bers 1 and n are the lower limit and the upper limit, respectively, of the summation. The expression (cid:7)n i=1xi can be stated in words as \u201csum the numbers xi for all values of i from 1 to n.\u201d An example can help clarify:\nThe summation operator has three properties. The \ufb01rst property\nis called the constant rule. Formally, it is:\nc = nc\nLet\u2019s consider an example. Say that we are given:\nA second property of the summation operator is:\n5 For a more complete review of regression, see Wooldridge [2010] and Wooldridge\n[2015]. I stand on the shoulders of giants.\nAgain let\u2019s use an example. Say we are given:\nWe can apply both of these properties to get the following third property:\n(axi + byi) = a\nBefore leaving the summation operator, it is useful to also note things which are not properties of this operator. First, the summation of a ratio is not the ratio of the summations themselves.\nSecond, the summation of some squared variable is not equal to the squaring of its summation.\nWe can use the summation indicator to make a number of calcu- lations, some of which we will do repeatedly over the course of this book. For instance, we can use the summation operator to calculate the average:\nwhere x is the average (mean) of the random variable xi. Another cal- culation we can make is a random variable\u2019s deviations from its own\nTable 5. Sum of deviations equaling 0.\nmean. The sum of the deviations from the mean is always equal to 0:\nYou can see this in Table 5.\nConsider a sequence of two numbers {y1, y2, . . . , yn} and {x1, x2, . . . , xn}. Now we can consider double summations over possible values of x\u2019s and y\u2019s. For example, consider the case where n = m = 2. Then, (cid:7) 2 j=1 xiyj is equal to x1y1 + x1y2 + x2y1 + x2y2. This is because i=1\nOne result that will be very useful throughout the book is:\nAn overly long, step-by-step proof is below. Note that the summation index is suppressed after the \ufb01rst line for easier reading.\nA more general version of this result is:\n(xi \u2212 x)yi =\nExpected value. The expected value of a random variable, also called the expectation and sometimes the population mean, is simply the weighted average of the possible values that the variable can take, with the weights being given by the probability of each value occurring in the population. Suppose that the variable X can take on values x1, x2, . . . , xk, each with probability f(x1), f(x2), . . . , f(xk), respectively.", "start_char_idx": 19746, "end_char_idx": 23684, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6324ae8-415b-4378-92cf-d521c070a73a": {"__data__": {"id_": "e6324ae8-415b-4378-92cf-d521c070a73a", "embedding": null, "metadata": {"page number": "32 - 111"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac", "node_type": "4", "metadata": {"page number": "32 - 111"}, "hash": "99ad2284755197ab9a1483b8b716ac5e3c39d1980b4b879df9d1e92b96483fd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "41de045d-dcd8-4859-beaa-a5b245df59ab", "node_type": "1", "metadata": {"page number": "32 - 111"}, "hash": "04770e862145813cdd8527fabc2bc064ab24aab4d4f80ea5ff319645f4e637bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a4ccd7f7-bb4c-41af-9d03-49777119f215", "node_type": "1", "metadata": {}, "hash": "885b443a3fb94343d2f29a44f686b3b6d0298e28e600a18e4d3911bc3d7ffeb0", "class_name": "RelatedNodeInfo"}}, "text": "This is because i=1\nOne result that will be very useful throughout the book is:\nAn overly long, step-by-step proof is below. Note that the summation index is suppressed after the \ufb01rst line for easier reading.\nA more general version of this result is:\n(xi \u2212 x)yi =\nExpected value. The expected value of a random variable, also called the expectation and sometimes the population mean, is simply the weighted average of the possible values that the variable can take, with the weights being given by the probability of each value occurring in the population. Suppose that the variable X can take on values x1, x2, . . . , xk, each with probability f(x1), f(x2), . . . , f(xk), respectively. Then we de\ufb01ne the expected value of X as:\nLet\u2019s look at a numerical example. If X takes on values of \u22121, 0, and 2, with probabilities 0.3, 0.3, and 0.4, respectively.6 Then the expected value of X equals:\nIn fact, you could take the expectation of a function of that variable, too, such as X2. Note that X2 takes only the values 1, 0, and 4, with probabil- ities 0.3, 0.3, and 0.4. Calculating the expected value of X2 therefore is:\nThe \ufb01rst property of expected value is that for any constant c, E(c) = c. The second property is that for any two constants a and b, then E(aX + b) = E(aX) + E(b) = aE(X) + b. And the third property is that if we have numerous constants, a1, . . . , an and many random variables, X1, . . . , Xn, then the following is true:\nWe can also express this using the expectation operator:\nAnd in the special case where ai = 1, then\n6 The law of total probability requires that all marginal probabilities sum to unity.\nVariance. The expectation operator, E(\u00b7), is a population concept. It refers to the whole group of interest, not just to the sample available to us. Its meaning is somewhat similar to that of the average of a random variable in the population. Some additional properties for the expec- tation operator can be explained assuming two random variables, W and H.\nConsider the variance of a random variable, W:\nWe can show\nIn a given sample of data, we can estimate the variance by the following calculation:\nwhere we divide by n \u2212 1 because we are making a degree-of-freedom adjustment from estimating the mean. But in large samples, this degree-of-freedom adjustment has no practical effect on the value of S2 where S2 is the average (after a degree of freedom correction) over the sum of all squared deviations from the mean.7\nA few more properties of variance. First, the variance of a line is:\nAnd the variance of a constant is 0 (i.e., V(c) = 0 for any constant,\nc). The variance of the sum of two random variables is equal to:\n7 Whenever possible, I try to use the \u201chat\u201d to represent an estimated statistic. Hence (cid:12)S2 instead of just S2. But it is probably more common to see the sample variance represented as S2.\nCovariance. The last part of equation 2.22 is called the covariance. The covariance measures the amount of linear dependence between two random variables. We represent it with the C(X, Y) operator. The expression C(X, Y) >0 indicates that two variables move in the same direction, whereas C(X, Y) <0 indicates that they move in opposite directions. Thus we can rewrite equation 2.22 as:\nWhile it\u2019s tempting to say that a zero covariance means that two random variables are unrelated, that is incorrect. They could have a nonlinear relationship. The de\ufb01nition of covariance is\nAs we said, population. The covariance between two linear functions is:\nif X and Y are independent, then C(X, Y) = 0 in the\nThe two constants, a1 and a2, zero out because their mean is them- selves and so the difference equals 0.\nInterpreting the magnitude of the covariance can be tricky. For that, we are better served by looking at correlation. We de\ufb01ne correlation as and Z = Y \u2212 E(Y) follows.", "start_char_idx": 22996, "end_char_idx": 26825, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a4ccd7f7-bb4c-41af-9d03-49777119f215": {"__data__": {"id_": "a4ccd7f7-bb4c-41af-9d03-49777119f215", "embedding": null, "metadata": {"page number": "32 - 111"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac", "node_type": "4", "metadata": {"page number": "32 - 111"}, "hash": "99ad2284755197ab9a1483b8b716ac5e3c39d1980b4b879df9d1e92b96483fd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6324ae8-415b-4378-92cf-d521c070a73a", "node_type": "1", "metadata": {"page number": "32 - 111"}, "hash": "ec06b91f0e893ba405e9cc36e1a5170352137cfd9e217c83561c75724993e122", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "23f4472d-5730-4497-8f25-f1d3a2b8935b", "node_type": "1", "metadata": {}, "hash": "f6ec005cea3b57a9b5dd494eaac9641f3bb913b1fa8b12ee2e74a5717d54aff3", "class_name": "RelatedNodeInfo"}}, "text": "The expression C(X, Y) >0 indicates that two variables move in the same direction, whereas C(X, Y) <0 indicates that they move in opposite directions. Thus we can rewrite equation 2.22 as:\nWhile it\u2019s tempting to say that a zero covariance means that two random variables are unrelated, that is incorrect. They could have a nonlinear relationship. The de\ufb01nition of covariance is\nAs we said, population. The covariance between two linear functions is:\nif X and Y are independent, then C(X, Y) = 0 in the\nThe two constants, a1 and a2, zero out because their mean is them- selves and so the difference equals 0.\nInterpreting the magnitude of the covariance can be tricky. For that, we are better served by looking at correlation. We de\ufb01ne correlation as and Z = Y \u2212 E(Y) follows. Let W = X \u2212 E(X) \u221a V(Y) V(X) Corr(W, Z) = C(X, Y)\nThe correlation coe\ufb03cient is bounded by \u22121 and 1. A positive (nega- tive) correlation indicates that the variables move in the same (oppo- site) ways. The closer the coe\ufb03cient is to 1 or \u22121, the stronger the linear relationship is.\nPopulation model. We begin with cross-sectional analysis. We will assume that we can collect a random sample from the population of\ninterest. Assume that there are two variables, x and y, and we want to see how y varies with changes in x.8\nThere are three questions that immediately come up. One, what if y is affected by factors other than x? How will we handle that? Two, what is the functional form connecting these two variables? Three, if we are interested in the causal effect of x on y, then how can we distinguish that from mere correlation? Let\u2019s start with a speci\ufb01c model.\nThis model is assumed to hold in the population. Equation 2.25 de\ufb01nes a linear bivariate regression model. For models concerned with captur- ing causal effects, the terms on the left side are usually thought of as the effect, and the terms on the right side are thought of as the causes. Equation 2.25 explicitly allows for other factors to affect y by including a random variable called the error term, u. This equation also explicitly models the functional form by assuming that y is linearly dependent on x. We call the \u03b20 coe\ufb03cient the intercept parameter, and we call the \u03b21 coe\ufb03cient the slope parameter. These describe a pop- ulation, and our goal in empirical work is to estimate their values. We never directly observe these parameters, because they are not data (I will emphasize this throughout the book). What we can do, though, is estimate these parameters using data and assumptions. To do this, we need credible assumptions to accurately estimate these parameters with data. We will return to this point later. In this simple regression framework, all unobserved variables that determine y are subsumed by the error term u.\nFirst, we make a simplifying assumption without loss of generality.\nLet the expected value of u be zero in the population. Formally:\nwhere E(\u00b7) is the expected value operator discussed earlier. If we nor- malize the u random variable to be 0, it is of no consequence. Why? Because the presence of \u03b20 (the intercept term) always allows us this \ufb02exibility. If the average of u is different from 0\u2014for instance, say that\n8 This is not necessarily causal language. We are speaking \ufb01rst and generally in\nterms of two random variables systematically moving together in some measurable way.\nit\u2019s \u03b10\u2014then we adjust the intercept. Adjusting the intercept has no effect on the \u03b21 slope parameter, though. For instance:\nwhere \u03b10 = E(u). The new error term is u \u2212 \u03b10, and the new intercept term is \u03b20 + \u03b10. But while those two terms changed, notice what did not change: the slope, \u03b21.\nMean independence. An assumption that meshes well with our elemen- tary treatment of statistics involves the mean of the error term for each \u201cslice\u201d of the population determined by values of x:\nE(u | x) = E(u) for all values x\nwhere E(u | x) means the \u201cexpected value of u given x.\u201d If equation 2.27 holds, then we say that u is mean independent of x.\nAn example might help here.", "start_char_idx": 26050, "end_char_idx": 30083, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "23f4472d-5730-4497-8f25-f1d3a2b8935b": {"__data__": {"id_": "23f4472d-5730-4497-8f25-f1d3a2b8935b", "embedding": null, "metadata": {"page number": "32 - 111"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac", "node_type": "4", "metadata": {"page number": "32 - 111"}, "hash": "99ad2284755197ab9a1483b8b716ac5e3c39d1980b4b879df9d1e92b96483fd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a4ccd7f7-bb4c-41af-9d03-49777119f215", "node_type": "1", "metadata": {"page number": "32 - 111"}, "hash": "de5267998ca41f4ca3e6619a71d5d32c08e207fc1f6572bf39b239bdb3e9481f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b57161ab-f09b-4efe-bdd0-33f8c6b5b3bb", "node_type": "1", "metadata": {}, "hash": "ca8af5b4bfb8889c04d1700333e53d091b76e72282554dce267c36690d245e4f", "class_name": "RelatedNodeInfo"}}, "text": "it\u2019s \u03b10\u2014then we adjust the intercept. Adjusting the intercept has no effect on the \u03b21 slope parameter, though. For instance:\nwhere \u03b10 = E(u). The new error term is u \u2212 \u03b10, and the new intercept term is \u03b20 + \u03b10. But while those two terms changed, notice what did not change: the slope, \u03b21.\nMean independence. An assumption that meshes well with our elemen- tary treatment of statistics involves the mean of the error term for each \u201cslice\u201d of the population determined by values of x:\nE(u | x) = E(u) for all values x\nwhere E(u | x) means the \u201cexpected value of u given x.\u201d If equation 2.27 holds, then we say that u is mean independent of x.\nAn example might help here. Let\u2019s say we are estimating the effect of schooling on wages, and u is unobserved ability. Mean indepen- dence requires that E(ability | x = 8) = E(ability | x = 12) = E(ability | x = 16) so that the average ability is the same in the different portions of the population with an eighth-grade education, a twelfth-grade education, and a college education. Because people choose how much school- ing to invest in based on their own unobserved skills and attributes, equation 2.27 is likely violated\u2014at least in our example.\nBut let\u2019s say we are willing to make this assumption. Then com- bining this new assumption, E(u | x) = E(u) (the nontrivial assumption to make), with E(u) = 0 (the normalization and trivial assumption), and you get the following new assumption:\nE(u | x) = 0, for all values x\nEquation 2.28 is called the zero conditional mean assumption and is a key identifying assumption in regression models. Because the conditional expected value is a linear operator, E(u | x) = 0 implies that\nwhich shows the population regression function is a linear function of x, or what Angrist and Pischke [2009] call the conditional expectation\nfunction.9 This relationship is crucial for the intuition of the parameter, \u03b21, as acausal parameter .\nOrdinary least squares. Given data on x and y, how can we estimate the population parameters, \u03b20 and \u03b21? Let the pairs of (xi, and yi) : i = 1, 2, . . . , n be random samples of size from the population. Plug any observation into the population equation:\nwhere i indicates a particular observation. We observe yi and xi but not ui. We just know that ui is there. We then use the two population restrictions that we discussed earlier:\nto obtain estimating equations for \u03b20 and \u03b21. We talked about the \ufb01rst condition already. The second one, though, means that the mean value of x does not change with different slices of the error term. This independence assumption implies E(xu) = 0, we get E(u) = 0, and C(x, u) = 0. Notice that if C(x, u) = 0, then that implies x and u are independent.10 Next we plug in for u, which is equal to y \u2212 \u03b20 \u2212 \u03b21x:\nThese are the two conditions in the population that effectively deter- mine \u03b20 and \u03b21. And again, note that the notation here is population concepts. We don\u2019t have access to populations, though we do have their sample counterparts:\n9 Notice that the conditional expectation passed through the linear function leav-\ning a constant, because of the \ufb01rst property of the expectation operator, and a constant times x. This is because the conditional expectation of E[X | X] = X. This leaves us with E[u | X] which under zero conditional mean is equal to 0.\nwhere (cid:12)\u03b20 and (cid:12)\u03b21 are the estimates from the data.11 These are two linear equations in the two unknowns (cid:12)\u03b20 and (cid:12)\u03b21. Recall the properties of the summation operator as we work through the following sample proper- ties of these two equations. We begin with equation 2.29 and pass the summation operator through.\nwhere y = 1 i=1 yi which is the average of the n numbers {yi : 1, . . . , n}. n For emphasis we will call y the sample average. We have already shown that the \ufb01rst equation equals zero (equation 2.29), so this implies y = (cid:12)\u03b20 + (cid:12)\u03b21x.", "start_char_idx": 29415, "end_char_idx": 33318, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b57161ab-f09b-4efe-bdd0-33f8c6b5b3bb": {"__data__": {"id_": "b57161ab-f09b-4efe-bdd0-33f8c6b5b3bb", "embedding": null, "metadata": {"page number": "32 - 111"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac", "node_type": "4", "metadata": {"page number": "32 - 111"}, "hash": "99ad2284755197ab9a1483b8b716ac5e3c39d1980b4b879df9d1e92b96483fd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "23f4472d-5730-4497-8f25-f1d3a2b8935b", "node_type": "1", "metadata": {"page number": "32 - 111"}, "hash": "150a6722f19f9c9d84228e94e9cc5eb29dc601d90da623402fe7a52cc2fad518", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4e8aa477-bec7-431f-bceb-a78000db563b", "node_type": "1", "metadata": {}, "hash": "7f382f9680a8eab5face2a5e8d5df1b52015a7c20598b155b34ab9cb13bf997d", "class_name": "RelatedNodeInfo"}}, "text": "Recall the properties of the summation operator as we work through the following sample proper- ties of these two equations. We begin with equation 2.29 and pass the summation operator through.\nwhere y = 1 i=1 yi which is the average of the n numbers {yi : 1, . . . , n}. n For emphasis we will call y the sample average. We have already shown that the \ufb01rst equation equals zero (equation 2.29), so this implies y = (cid:12)\u03b20 + (cid:12)\u03b21x. So we now use this equation to write the intercept in terms of the slope:\nWe now plug (cid:12)\u03b20 into the second equation, gives us the following (with some simple algebraic manipulation):\n11 Notice that we are dividing by n, not n \u2212 1. There is no degrees-of-freedom correction, in other words, when using samples to calculate means. There is a degrees-\nof-freedom correction when we start calculating higher moments.\nSo the equation to solve is12\n(xi \u2212 x)(yi \u2212 y) n (xi \u2212 x)2 i=1 = Sample covariance(xi, yi) Sample variance(xi) The previous formula for (cid:12)\u03b21 is important because it shows us how to take data that we have and compute the slope estimate. The esti- mate, (cid:12)\u03b21, is commonly referred to as the ordinary least squares (OLS) slope estimate. It can be computed whenever the sample variance of xi isn\u2019t 0. In other words, it can be computed if xi is not constant across all values of i. The intuition is that the variation in x is what permits us to identify its impact in y. This also means, though, that we cannot determine the slope in a relationship if we observe a sample in which everyone has the same years of schooling, or whatever causal variable we are interested in.\nn i=1\n(xi \u2212 x)2 (cid:9)= 0, we can write:\nOnce we have calculated (cid:12)\u03b21, we can compute the intercept value, (cid:12)\u03b20, as (cid:12)\u03b20 = y \u2212 (cid:12)\u03b21x. This is the OLS intercept estimate because it is calculated using sample averages. Notice that it is straightforward because (cid:12)\u03b20 is linear in (cid:12)\u03b21. With computers and statistical programming languages and software, we let our computers do these calculations because even when n is small, these calculations are quite tedious.\nFor any candidate estimates, (cid:12)\u03b20, (cid:12)\u03b21, we de\ufb01ne a \ufb01tted value for each\nRecall that i = {1, . . ., n}, so we have n of these equations. This is the value we predict for yi given that x = xi. But there is prediction error because y (cid:9)= yi. We call that mistake the residual, and here use the (cid:12)ui notation for it. So the residual equals:\nWhile both the residual and the error term are represented with a u, it is important that you know the differences. The residual is the prediction error based on our \ufb01tted (cid:12)y and the actual y. The residual is therefore easily calculated with any sample of data. But u without the hat is the error term, and it is by de\ufb01nition unobserved by the researcher. Whereas the residual will appear in the data set once generated from a few steps of regression and manipulation, the error term will never appear in the data set. It is all of the determinants of our outcome not captured by our model. This is a crucial distinction, and strangely enough it is so subtle that even some seasoned researchers struggle to express it.\nSuppose we measure the size of the mistake, for each i, by squar- ing it. Squaring it will, after all, eliminate all negative values of the mistake so that everything is a positive value. This becomes useful when summing the mistakes if we don\u2019t want positive and negative values to cancel one another out. So let\u2019s do that: square the mistake 2: and add them all up to get\nn i=1\nThis equation is called the sum of squared residuals because the resid- ual is (cid:12)ui = yi \u2212(cid:12)y. But the residual is based on estimates of the slope and\nthe intercept. We can imagine any number of estimates of those val- ues.", "start_char_idx": 32877, "end_char_idx": 36719, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4e8aa477-bec7-431f-bceb-a78000db563b": {"__data__": {"id_": "4e8aa477-bec7-431f-bceb-a78000db563b", "embedding": null, "metadata": {"page number": "32 - 111"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac", "node_type": "4", "metadata": {"page number": "32 - 111"}, "hash": "99ad2284755197ab9a1483b8b716ac5e3c39d1980b4b879df9d1e92b96483fd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b57161ab-f09b-4efe-bdd0-33f8c6b5b3bb", "node_type": "1", "metadata": {"page number": "32 - 111"}, "hash": "8885501e88529de1fe8da4de278d6cab51714c2ff0b6b81964024d94deae5f35", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "842c2091-a976-448e-8902-e9069d66d7d4", "node_type": "1", "metadata": {}, "hash": "77ffbd81c88d16f93ccced5ee92ff53c1e8869498c3d8fe410588e7626b41876", "class_name": "RelatedNodeInfo"}}, "text": "It is all of the determinants of our outcome not captured by our model. This is a crucial distinction, and strangely enough it is so subtle that even some seasoned researchers struggle to express it.\nSuppose we measure the size of the mistake, for each i, by squar- ing it. Squaring it will, after all, eliminate all negative values of the mistake so that everything is a positive value. This becomes useful when summing the mistakes if we don\u2019t want positive and negative values to cancel one another out. So let\u2019s do that: square the mistake 2: and add them all up to get\nn i=1\nThis equation is called the sum of squared residuals because the resid- ual is (cid:12)ui = yi \u2212(cid:12)y. But the residual is based on estimates of the slope and\nthe intercept. We can imagine any number of estimates of those val- ues. But what if our goal is to minimize the sum of squared residuals by choosing (cid:12)\u03b20 and (cid:12)\u03b21? Using calculus, it can be shown that the solutions to that problem yield parameter estimates that are the same as what we obtained before.\nOnce we have the numbers (cid:12)\u03b20 and (cid:12)\u03b21 for a given data set, we write\nLet\u2019s consider a short simulation.\n1 set seed 1 2 clear 3 set obs 10000 4 gen x = rnormal() 5 gen u = rnormal() 6 gen y = 5.5*x + 12*u 7 reg y x 8 predict yhat1 9 gen yhat2 = -0.0750109 + 5.598296*x // Compare yhat1 and yhat2 10 sum yhat* 11 predict uhat1, residual 12 gen uhat2=y-yhat2 13 sum uhat* 14 twoway (lfit y x, lcolor(black) lwidth(medium)) (scatter y x, mcolor(black) /// 15 msize(tiny) msymbol(point)), title(OLS Regression Line) 16 rvfplot, yline(0)\n(continued)\nR (continued)\nlm(y ~ x, .) %>% ggplot(aes(x=x, y=y)) + ggtitle(\"OLS Regression Line\") + geom_point(size = 0.05, color = \"black\", alpha = 0.5) + geom_smooth(method = lm, color = \"black\") + annotate(\"text\", x = -1.5, y = 30, color = \"red\",\nyhat1 = predict(lm(y ~ x, .)), yhat2 = 0.0732608 + 5.685033*x, uhat1 = residuals(lm(y ~ x, .)), uhat2 = y - yhat2\nLet\u2019s look at the output from this. First, if you summarize the data, you\u2019ll see that the \ufb01tted values are produced both using Stata\u2019s Predict command and manually using the Generate command. I wanted the reader to have a chance to better understand this, so did it both ways. But second, let\u2019s look at the data and paste on top of it the estimated coe\ufb03cients, the y-intercept and slope on x in Figure 3. The estimated coe\ufb03cients in both are close to the hard coded values built into the data-generating process.\nOnce we have the estimated coe\ufb03cients and we have the OLS regression line, we can predict y (outcome) for any (sensible) value of x. So plug in certain values of x, and we can immediately calculate what y will probably be with some error. The value of OLS here lies in how large that error is: OLS minimizes the error for a linear function. In fact, it is the best such guess at y for all linear estimators because it minimizes the prediction error. There\u2019s always prediction error, in other words, with any estimator, but OLS is the least worst.\nNotice that the intercept is the predicted value of y if and when x = 0. In this sample, that value is \u22120.0750109.13 The slope allows us to predict changes in y for any reasonable change in x according to:\n13 It isn\u2019t exactly 0 even though u and x are independent. Think of it as u and x are independent in the population, but not in the sample.", "start_char_idx": 35904, "end_char_idx": 39277, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "842c2091-a976-448e-8902-e9069d66d7d4": {"__data__": {"id_": "842c2091-a976-448e-8902-e9069d66d7d4", "embedding": null, "metadata": {"page number": "32 - 111"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac", "node_type": "4", "metadata": {"page number": "32 - 111"}, "hash": "99ad2284755197ab9a1483b8b716ac5e3c39d1980b4b879df9d1e92b96483fd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4e8aa477-bec7-431f-bceb-a78000db563b", "node_type": "1", "metadata": {"page number": "32 - 111"}, "hash": "a81f3c7f08f9402dc78b5f5ec75a18291ad4bfe03a972e08777338ecc5ad27b1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "239192fa-bcca-47cb-8d57-8b18fe4ff79e", "node_type": "1", "metadata": {}, "hash": "48259e7bf3dfb94030e169cf57e8fd2617759dabc2a85a8d285dac392fd49095", "class_name": "RelatedNodeInfo"}}, "text": "The value of OLS here lies in how large that error is: OLS minimizes the error for a linear function. In fact, it is the best such guess at y for all linear estimators because it minimizes the prediction error. There\u2019s always prediction error, in other words, with any estimator, but OLS is the least worst.\nNotice that the intercept is the predicted value of y if and when x = 0. In this sample, that value is \u22120.0750109.13 The slope allows us to predict changes in y for any reasonable change in x according to:\n13 It isn\u2019t exactly 0 even though u and x are independent. Think of it as u and x are independent in the population, but not in the sample. This is because sample characteristics tend to be slightly different from population properties due to sampling\nAnd if (cid:10)x = 1, then x increases by one unit, and so (cid:10)(cid:12)y = 5.598296 in our numerical example because (cid:12)\u03b21 = 5.598296.\nNow that we have calculated (cid:12)\u03b20 and (cid:12)\u03b21, we get the OLS \ufb01tted values\nby plugging xi into the following equation for i = 1, . . . , n:\nThe OLS residuals are also calculated by:\nMost residuals will be different from 0 (i.e., they do not lie on the regres- sion line). You can see this in Figure 3. Some are positive, and some are negative. A positive residual indicates that the regression line (and hence, the predicted values) underestimates the true value of yi. And if the residual is negative, then the regression line overestimates the true value.\nRecall that we de\ufb01ned the \ufb01tted value as (cid:12)yi and the residual, (cid:12)ui, as yi \u2212 (cid:12)yi. Notice that the scatter-plot relationship between the residuals and the \ufb01tted values created a spherical pattern, suggesting that they are not correlated (Figure 4). This is mechanical\u2014least squares pro- duces residuals which are uncorrelated with \ufb01tted values. There\u2019s no magic here, just least squares.\nAlgebraic Properties of OLS. Remember how we obtained (cid:12)\u03b20 and (cid:12)\u03b21? When an intercept is included, we have:\nThe OLS residual always adds up to zero, by construction.\nSometimes seeing is believing, so let\u2019s look at this together. Type the following into Stata verbatim.\n1 clear 2 set seed 1234 3 set obs 10 4 gen x = 9*rnormal() 5 gen u = 36*rnormal() 6 gen y = 3 + 2*x + u 7 reg y x 8 predict yhat 9 predict residuals, residual 10 su residuals 11 list 12 collapse (sum) x u y yhat residuals 13 list\nx = 9*rnorm(10), u = 36*rnorm(10), y = 3 + 2*x + u, yhat = predict(lm(y ~ x)), uhat = residuals(lm(y ~ x))\nOutput from this can be summarized as in the following table\nNotice the difference between the u, (cid:12)y, and (cid:12)u columns. When we sum these ten lines, neither the error term nor the \ufb01tted values of y sum to zero. But the residuals do sum to zero. This is, as we said, one of the algebraic properties of OLS\u2014coe\ufb03cients were optimally chosen to ensure that the residuals sum to zero.\nBecause yi = (cid:12)yi + (cid:12)ui by de\ufb01nition (which we can also see in Table\n6), we can take the sample average of both sides:\nyi = 1 n\nand so y =(cid:12)y because the residuals sum to zero. Similarly, the way that we obtained our estimates yields\nThe sample covariance (and therefore the sample correlation) between the explanatory variables and the residuals is always zero (see Table 6).\nTable 6. Simulated data showing the sum of residuals equals zero.\nBecause the (cid:12)yi are linear functions of the xi, the \ufb01tted values and residuals are uncorrelated too (see Table 6):\nBoth properties hold by construction.", "start_char_idx": 38624, "end_char_idx": 42139, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "239192fa-bcca-47cb-8d57-8b18fe4ff79e": {"__data__": {"id_": "239192fa-bcca-47cb-8d57-8b18fe4ff79e", "embedding": null, "metadata": {"page number": "32 - 111"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac", "node_type": "4", "metadata": {"page number": "32 - 111"}, "hash": "99ad2284755197ab9a1483b8b716ac5e3c39d1980b4b879df9d1e92b96483fd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "842c2091-a976-448e-8902-e9069d66d7d4", "node_type": "1", "metadata": {"page number": "32 - 111"}, "hash": "e43600bf27b9e0a64d4b6cb0463d6fe3f6bbad4ae34ab9786bc0bf2fdae0d979", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c6495d9b-d117-4e65-87b6-cc8a4ac65f3b", "node_type": "1", "metadata": {}, "hash": "6dddcb8dcf35edc3fd257b2c05f1d195b4e694ddbdbdfa6388ea4446f9f5ff25", "class_name": "RelatedNodeInfo"}}, "text": "This is, as we said, one of the algebraic properties of OLS\u2014coe\ufb03cients were optimally chosen to ensure that the residuals sum to zero.\nBecause yi = (cid:12)yi + (cid:12)ui by de\ufb01nition (which we can also see in Table\n6), we can take the sample average of both sides:\nyi = 1 n\nand so y =(cid:12)y because the residuals sum to zero. Similarly, the way that we obtained our estimates yields\nThe sample covariance (and therefore the sample correlation) between the explanatory variables and the residuals is always zero (see Table 6).\nTable 6. Simulated data showing the sum of residuals equals zero.\nBecause the (cid:12)yi are linear functions of the xi, the \ufb01tted values and residuals are uncorrelated too (see Table 6):\nBoth properties hold by construction. In other words, (cid:12)\u03b20 and (cid:12)\u03b21 were selected to make them true.14\nA third property is that if we plug in the average for x, we predict the sample average for y. That is, the point (x, y) is on the OLS regression line, or:\nGoodness-of-\ufb01t. For each observation, we write\nDe\ufb01ne the total sum of squares (SST), explained sum of squares (SSE), and residual sum of squares (SSR) as\n14 Using the Stata code from Table 6, you can show these algebraic properties\nyourself. I encourage you to do so by creating new variables equaling the product of\nthese terms and collapsing as we did with the other variables. That sort of exercise may\nhelp convince you that the aforementioned algebraic properties always hold.\nThese are sample variances when divided by n\u22121.15 SST n \u2212 1 SSR n \u2212 1\nis the sample variance of (cid:12)yi, and variance of yi, is the sample variance of (cid:12)ui. With some simple manipulation rewrite equation 2.34:\nis the sample\nSince equation 2.34 shows that the \ufb01tted values are uncorrelated with the residuals, we can write the following equation:\nAssuming SST > 0, we can de\ufb01ne the fraction of the total variation in yi that is explained by xi (or the OLS regression line) as\nwhich is called the R-squared of the regression. It can be shown to be equal to the square of the correlation between yi and (cid:12)yi. Therefore 0 \u2264 R2 \u2264 1. An R-squared of zero means no linear relationship between yi and xi, and an R-squared of one means a perfect linear relationship (e.g., yi = xi + 2). As R2 increases, the yi are closer and closer to falling on the OLS regression line.\nI would encourage you not to \ufb01xate on R-squared in research projects where the aim is to estimate some causal effect, though. It\u2019s a useful summary measure, but it does not tell us about causality. Remember, you aren\u2019t trying to explain variation in y if you are trying to estimate some causal effect. The R2 tells us how much of the variation in yi is explained by the explanatory variables. But if we are interested\nin the causal effect of a single variable, R2 is irrelevant. For causal inference, we need equation 2.28.\nExpected value of OLS. Up until now, we motivated simple regression using a population model. But our analysis has been purely algebraic, based on a sample of data. So residuals always average to zero when we apply OLS to a sample, regardless of any underlying model. But our job gets tougher. Now we have to study the statistical properties of the OLS estimator, referring to a population model and assuming random sampling.16\nThe \ufb01eld of mathematical statistics is concerned with questions. How do estimators behave across different samples of data? On aver- age, for instance, will we get the right answer if we repeatedly sam- ple? We need to \ufb01nd the expected value of the OLS estimators\u2014in effect, the average outcome across all possible random samples\u2014 and determine whether we are right, on average. This leads natu- rally to a characteristic called unbiasedness, which is desirable of all estimators.\nRemember, our objective is to estimate \u03b21, which is the slope population parameter that describes the relationship between y and x. Our estimate, (cid:12)\u03b21, is an estimator of that parameter obtained for a speci\ufb01c sample.", "start_char_idx": 41383, "end_char_idx": 45388, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c6495d9b-d117-4e65-87b6-cc8a4ac65f3b": {"__data__": {"id_": "c6495d9b-d117-4e65-87b6-cc8a4ac65f3b", "embedding": null, "metadata": {"page number": "32 - 111"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac", "node_type": "4", "metadata": {"page number": "32 - 111"}, "hash": "99ad2284755197ab9a1483b8b716ac5e3c39d1980b4b879df9d1e92b96483fd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "239192fa-bcca-47cb-8d57-8b18fe4ff79e", "node_type": "1", "metadata": {"page number": "32 - 111"}, "hash": "92223c2bdd5ef194d81bf5bae1aa1550d759c5771e9dc6c7e216e0286c38565b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6c2354b8-13d3-4794-bcc3-fc867663d61a", "node_type": "1", "metadata": {}, "hash": "a2f37a7221d730aa37e18284869ec0d97f7646779ab467578cab6f40343e06e2", "class_name": "RelatedNodeInfo"}}, "text": "But our job gets tougher. Now we have to study the statistical properties of the OLS estimator, referring to a population model and assuming random sampling.16\nThe \ufb01eld of mathematical statistics is concerned with questions. How do estimators behave across different samples of data? On aver- age, for instance, will we get the right answer if we repeatedly sam- ple? We need to \ufb01nd the expected value of the OLS estimators\u2014in effect, the average outcome across all possible random samples\u2014 and determine whether we are right, on average. This leads natu- rally to a characteristic called unbiasedness, which is desirable of all estimators.\nRemember, our objective is to estimate \u03b21, which is the slope population parameter that describes the relationship between y and x. Our estimate, (cid:12)\u03b21, is an estimator of that parameter obtained for a speci\ufb01c sample. Different samples will generate different estimates ((cid:12)\u03b21) for the \u201ctrue\u201d (and unobserved) \u03b21. Unbiasedness means that if we could take as many random samples on Y as we want from the popula- tion and compute an estimate each time, the average of the estimates would be equal to \u03b21.\nThere are several assumptions required for OLS to be unbiased. The \ufb01rst assumption is called linear in the parameters. Assume a population model\n16 This section is a review of traditional econometrics pedagogy. We cover it for\nthe sake of completeness. Traditionally, econometricians motivated their discuss of\nwhere \u03b20 and \u03b21 are the unknown population parameters. We view x and u as outcomes of random variables generated by some data- generating process. Thus, since y is a function of x and u, both of which are random, then y is also random. Stating this assumption formally shows that our goal is to estimate \u03b20 and \u03b21.\nOur second assumption is random sampling. We have a random sample of size n, {(xi, yi):i = 1, . . . , n}, following the population model. We know how to use this data to estimate \u03b20 and \u03b21 by OLS. Because each i is a draw from the population, we can write, for each i:\nNotice that ui here is the unobserved error for observation i. It is not the residual that we compute from the data.\nThe third assumption is called sample variation in the explanatory variable. That is, the sample outcomes on xi are not all the same value. This is the same as saying that the sample variance of x is not zero. In practice, this is no assumption at all. If the xi all have the same value (i.e., are constant), we cannot learn how x affects y in the population. Recall that OLS is the covariance of y and x divided by the variance in x, and so if x is constant, then we are dividing by zero, and the OLS estimator is unde\ufb01ned.\nWith the fourth assumption our assumptions start to have real teeth. It is called the zero conditional mean assumption and is prob- ably the most critical assumption in causal inference. In the popula- tion, the error term has zero mean given any value of the explanatory variable:\nThis is the key assumption for showing that OLS is unbiased, with the zero value being of no importance once we assume that E(u | x) does not change with x. Note that we can compute OLS estimates whether or not this assumption holds, even if there is an underlying population model.\nSo, how do we show that (cid:12)\u03b21 is an unbiased estimate of \u03b21 (equation 2.37)? We need to show that under the four assumptions we just\noutlined, the expected value of (cid:12)\u03b21, when averaged across random sam- ples, will center on the true value of \u03b21. This is a subtle yet critical concept. Unbiasedness in this context means that if we repeatedly sample data from a population and run a regression on each new sam- ple, the average over all those estimated coe\ufb03cients will equal the true value of \u03b21. We will discuss the answer as a series of steps.\nStep 1: Write down a formula for (cid:12)\u03b21. It is convenient to use the\nLet\u2019s get rid of some of this notational clutter by de\ufb01ning SSTx (i.e., total variation in the xi) and rewrite this as:\nn i=1\n(xi \u2212 x)yi SSTx\nStep 2: Replace each yi with yi = \u03b20 + \u03b21xi + ui, which uses the \ufb01rst linear assumption and the fact that we have sampled data (our second assumption).", "start_char_idx": 44525, "end_char_idx": 48703, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6c2354b8-13d3-4794-bcc3-fc867663d61a": {"__data__": {"id_": "6c2354b8-13d3-4794-bcc3-fc867663d61a", "embedding": null, "metadata": {"page number": "32 - 111"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac", "node_type": "4", "metadata": {"page number": "32 - 111"}, "hash": "99ad2284755197ab9a1483b8b716ac5e3c39d1980b4b879df9d1e92b96483fd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c6495d9b-d117-4e65-87b6-cc8a4ac65f3b", "node_type": "1", "metadata": {"page number": "32 - 111"}, "hash": "bb0e5d70f7b0f41976beb83a1c4c0e5bad47aa1d9cb539095d53308b34d7d679", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b59b7994-6a71-477e-b62a-f32f5d971d9e", "node_type": "1", "metadata": {}, "hash": "946268f8cd27c94c297728d015eee52845ff46bd0050f64068217f417e4cc12c", "class_name": "RelatedNodeInfo"}}, "text": "This is a subtle yet critical concept. Unbiasedness in this context means that if we repeatedly sample data from a population and run a regression on each new sam- ple, the average over all those estimated coe\ufb03cients will equal the true value of \u03b21. We will discuss the answer as a series of steps.\nStep 1: Write down a formula for (cid:12)\u03b21. It is convenient to use the\nLet\u2019s get rid of some of this notational clutter by de\ufb01ning SSTx (i.e., total variation in the xi) and rewrite this as:\nn i=1\n(xi \u2212 x)yi SSTx\nStep 2: Replace each yi with yi = \u03b20 + \u03b21xi + ui, which uses the \ufb01rst linear assumption and the fact that we have sampled data (our second assumption). The numerator becomes:\n(xi \u2212 x)yi =\n(xi \u2212 x)xi +\nNote, we used this.17\nn i=1\n(xi \u2212 x) = 0 and\nn i=1\n(xi \u2212 x)xi =\n(xi \u2212 x)2 to do\n17 Told you we would use this result a lot.\nWe have shown that:\nn i=1\nn i=1 SSTx (xi \u2212 x)ui SSTx\nNote that the last piece is the slope coe\ufb03cient from the OLS regression of ui on xi, i: 1, . . . , n.18 We cannot do this regression because the ui are (xi \u2212 x) not observed. Now de\ufb01ne wi = SSTx\nso that we have the following:\nThis has showed us the following: First, (cid:12)\u03b21 is a linear function of the unobserved errors, ui. The wi are all functions of {x1, . . . , xn}. Second, the random difference between \u03b21 and the estimate of it, (cid:12)\u03b21, is due to this linear function of the unobservables.\nStep 3: Find E((cid:12)\u03b21). Under the random sampling assumption and the zero conditional mean assumption, E(ui | x1, . . . , xn) = 0, that means conditional on each of the x variables: (cid:4) ui | x1, . . . , xn\nbecause wi is a function of {x1, . . . , xn}. This would be true if in the population u and x are correlated.\nNow we can complete the proof: conditional on {x1, . . . , xn}, (cid:8)\n18 I \ufb01nd it interesting that we see so many\nterms when working with regres-\nsion. They show up constantly. Keep your eyes peeled.\nRemember, \u03b21 is the \ufb01xed constant in the population. The estima- tor, (cid:12)\u03b21, varies across samples and is the random outcome: before we collect our data, we do not know what (cid:12)\u03b21 will be. Under the four aforementioned assumptions, E( (cid:12)\u03b20) = \u03b20 and E((cid:12)\u03b21) = \u03b21.\nI \ufb01nd it helpful to be concrete when we work through exercises like\nthis. So let\u2019s visualize this. Let\u2019s create a Monte Carlo simulation. We\nhave the following population model:\nwhere x \u223c Normal(0, 9), u \u223c Normal(0, 36). Also, x and u are indepen- dent. The following Monte Carlo simulation will estimate OLS on a sample of data 1,000 times. The true \u03b2 parameter equals 2. But what will the average (cid:12)\u03b2 equal when we use repeated sampling?\n1 clear all 2 program define ols, rclass 3 version 14.2 4 syntax [, obs(integer 1) mu(real 0) sigma(real 1) ] 5 6 7 8 9 10 11 12 13 14 15 simulate beta=_b[x], reps(1000): ols 16 su 17 hist beta\nclear drop _all set obs 10000 gen x = 9*rnormal() gen u = 36*rnormal() gen y = 3 + 2*x + u reg y x end\nggplot()+ geom_histogram(aes(x), binwidth = 0.01)\nTable 7 gives us the mean value of (cid:12)\u03b21 over the 1,000 repetitions (repeated sampling). Your results will differ from mine here only in the randomness involved in the simulation.", "start_char_idx": 48038, "end_char_idx": 51224, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b59b7994-6a71-477e-b62a-f32f5d971d9e": {"__data__": {"id_": "b59b7994-6a71-477e-b62a-f32f5d971d9e", "embedding": null, "metadata": {"page number": "32 - 111"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac", "node_type": "4", "metadata": {"page number": "32 - 111"}, "hash": "99ad2284755197ab9a1483b8b716ac5e3c39d1980b4b879df9d1e92b96483fd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6c2354b8-13d3-4794-bcc3-fc867663d61a", "node_type": "1", "metadata": {"page number": "32 - 111"}, "hash": "11a6cac002115c3acb51cef45f47af48ff07287ed1c6499cbfbab6d41ac86802", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0baf671a-06b2-40fb-9d52-b47aa81b097b", "node_type": "1", "metadata": {}, "hash": "1a2a6ffaed53a57d7f51971eb13ac1f8077bf7d85bc2183075cf96f1b301eb9d", "class_name": "RelatedNodeInfo"}}, "text": "1 clear all 2 program define ols, rclass 3 version 14.2 4 syntax [, obs(integer 1) mu(real 0) sigma(real 1) ] 5 6 7 8 9 10 11 12 13 14 15 simulate beta=_b[x], reps(1000): ols 16 su 17 hist beta\nclear drop _all set obs 10000 gen x = 9*rnormal() gen u = 36*rnormal() gen y = 3 + 2*x + u reg y x end\nggplot()+ geom_histogram(aes(x), binwidth = 0.01)\nTable 7 gives us the mean value of (cid:12)\u03b21 over the 1,000 repetitions (repeated sampling). Your results will differ from mine here only in the randomness involved in the simulation. But your results should be sim- ilar to what is shown here. While each sample had a different estimated slope, the average for (cid:12)\u03b21 over all the samples was 1.998317, which is close to the true value of 2 (see equation 2.38). The standard devia- tion in this estimator was 0.0398413, which is close to the standard error recorded in the regression itself.19 Thus, we see that the estimate is the mean value of the coe\ufb03cient from repeated sampling, and the standard error is the standard deviation from that repeated estimation. We can see the distribution of these coe\ufb03cient estimates in Figure 5. The problem is, we don\u2019t know which kind of sample we have. Do we have one of the \u201calmost exactly 2\u201d samples, or do we have one of the \u201cpretty different from 2\u201d samples? We can never know whether we are close to the population value. We hope that our sample is \u201ctypical\u201d and\n19 The standard error I found from running this on one sample of data was\nproduces a slope estimate close to (cid:12)\u03b21, but we can\u2019t know. Unbiased- ness is a property of the procedure of the rule. It is not a property of the estimate itself. For example, say we estimated an that 8.2% return on schooling. It is tempting to say that 8.2% is an unbiased estimate of the return to schooling, but that\u2019s technically incorrect. The rule used to get (cid:12)\u03b21 = 0.082 is unbiased (if we believe that u is unrelated to schooling), not the actual estimate itself.\nLaw of iterated expectations. The conditional expectation function (CEF) is the mean of some outcome y with some covariate x held \ufb01xed. Let\u2019s focus more intently on this function.20 Let\u2019s get the notation and\n20 I highly encourage the interested reader to study Angrist and Pischke [2009],\nwho have an excellent discussion of LIE there.\nsome of the syntax out of the way. As noted earlier, we write the CEF as E(yi | xi). Note that the CEF is explicitly a function of xi. And because xi is random, the CEF is random\u2014although sometimes we work with partic- ular values for xi, like E(yi | xi = 8 years schooling) or E(yi | xi = Female). When there are treatment variables, then the CEF takes on two values: E(yi | di = 0) and E(yi | di = 1). But these are special cases only.\nAn important complement to the CEF is the law of iterated expec-\ntations (LIE). This law says that an unconditional expectation can be written as the unconditional average of the CEF. In other words, E(yi) = E{E(yi | xi)}. This is a fairly simple idea: if you want to know the uncondi- tional expectation of some random variable y, you can simply calculate\ncovariate x. Let\u2019s look at an example. Let\u2019s say that average grade-point\nfor females is 3.5, average GPA for males is a 3.2, half the population\nis female, and half is male. Then:\nYou probably use LIE all the time and didn\u2019t even know it. The proof is not complicated. Let xi and yi each be continuously distributed. The joint density is de\ufb01ned as fxy(u, t). The conditional distribution of y given x = u is de\ufb01ned as fy(t | xi = u). The marginal densities are gy(t) and gx(u).\nCheck out how easy this proof is.", "start_char_idx": 50693, "end_char_idx": 54308, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0baf671a-06b2-40fb-9d52-b47aa81b097b": {"__data__": {"id_": "0baf671a-06b2-40fb-9d52-b47aa81b097b", "embedding": null, "metadata": {"page number": "32 - 111"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac", "node_type": "4", "metadata": {"page number": "32 - 111"}, "hash": "99ad2284755197ab9a1483b8b716ac5e3c39d1980b4b879df9d1e92b96483fd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b59b7994-6a71-477e-b62a-f32f5d971d9e", "node_type": "1", "metadata": {"page number": "32 - 111"}, "hash": "7dd0505cf61a61eab78000e6a4608d38ea71d54c6ba54e6675ca79ef7b991d7e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e476494e-51c6-4d8d-a91d-7a0d245b3b45", "node_type": "1", "metadata": {}, "hash": "b9ec49827aba809648b505af875d70c84b7fc7ad4235a08527de3d4eb0896973", "class_name": "RelatedNodeInfo"}}, "text": "In other words, E(yi) = E{E(yi | xi)}. This is a fairly simple idea: if you want to know the uncondi- tional expectation of some random variable y, you can simply calculate\ncovariate x. Let\u2019s look at an example. Let\u2019s say that average grade-point\nfor females is 3.5, average GPA for males is a 3.2, half the population\nis female, and half is male. Then:\nYou probably use LIE all the time and didn\u2019t even know it. The proof is not complicated. Let xi and yi each be continuously distributed. The joint density is de\ufb01ned as fxy(u, t). The conditional distribution of y given x = u is de\ufb01ned as fy(t | xi = u). The marginal densities are gy(t) and gx(u).\nCheck out how easy this proof is. The \ufb01rst line uses the de\ufb01nition of expectation. The second line uses the de\ufb01nition of conditional expec- tation. The third line switches the integration order. The fourth line uses the de\ufb01nition of joint density. The \ufb01fth line replaces the prior line with the subsequent expression. The sixth line integrates joint density over the support of x which is equal to the marginal density of y. So restating the law of iterated expectations: E(yi) = E{E(y | xi)}.\nCEF decomposition property. The \ufb01rst property of the CEF we will dis- cuss is the CEF decomposition property. The power of LIE comes from the way it breaks a random variable into two pieces\u2014the CEF and a residual with special properties. The CEF decomposition property states that\nyi = E(yi | xi) + \u03b5i\nwhere (i) \u03b5i is mean independent of xi, That is,\nE(\u03b5i | xi) = 0\nand (ii) \u03b5i is not correlated with any function of xi.\nThe theorem says that any random variable yi can be decomposed into a piece that is explained by xi (the CEF) and a piece that is left over and orthogonal to any function of xi. I\u2019ll prove the (i) part \ufb01rst. Recall that \u03b5i = yi \u2212 E(yi | xi) as we will make a substitution in the second line below.\nE(\u03b5i | xi) = E\nyi \u2212 E(yi | xi) | xi\n= E(yi | xi) \u2212 E(yi | xi)\nThe second part of the theorem states that \u03b5i is uncorrelated with any function of xi. Let h(xi) be any function of xi. Then E(h(xi)\u03b5i) =\nE{h(xi)E(\u03b5i | xi)} The second term in the interior product is equal to zero by mean independence.21\nCEF prediction property. The second property is the CEF prediction property. This states that E(yi | xi) = arg minm(xi) E[(y \u2212 m(xi))2], where m(xi) is any function of xi. In words, this states that the CEF is the mini- mum mean squared error of yi given xi. By adding E(yi | xi)\u2212E(yi | xi) = 0 to the right side we get\nE(yi | xi) \u2212 m(xi)\nI personally \ufb01nd this easier to follow with simpler notation. So replace this expression with the following terms:\nDistribute the terms, rearrange them, and replace the terms with their original values until you get the following:\nE(yi | xi) + m(xi)\nE(yi | xi) \u2212 m(xi) (cid:3)\nNow minimize the function with respect to m(xi). When minimizing this function with respect to m(xi), note that the \ufb01rst term (yi \u2212E(yi | xi))2 doesn\u2019t matter because it does not depend on m(xi). So it will zero out. The second and third terms, though, do depend on m(xi). So rewrite 2(E(yi | xi) \u2212 m(xi)) as h(xi). Also set \u03b5i equal to [yi \u2212 E(yi | xi)] and substitute\n+ h(xi)\u03b5i +\nE(yi | xi) + m(xi)\nNow minimizing this function and setting it equal to zero we get\n21 Let\u2019s take a concrete example of this proof. Let h(xi) = \u03b1 + \u03b3 xi. Then take the joint expectation E(h(xi)\u03b5i) = E([\u03b1 +\u03b3 xi]\u03b5i).", "start_char_idx": 53623, "end_char_idx": 57000, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e476494e-51c6-4d8d-a91d-7a0d245b3b45": {"__data__": {"id_": "e476494e-51c6-4d8d-a91d-7a0d245b3b45", "embedding": null, "metadata": {"page number": "32 - 111"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac", "node_type": "4", "metadata": {"page number": "32 - 111"}, "hash": "99ad2284755197ab9a1483b8b716ac5e3c39d1980b4b879df9d1e92b96483fd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0baf671a-06b2-40fb-9d52-b47aa81b097b", "node_type": "1", "metadata": {"page number": "32 - 111"}, "hash": "0fa9266381b678122b3fa9e5cd88046fcd91f466a67495e9fe5f05901e082cd0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "48635a48-89aa-42ca-a979-c9fbc2fc90ef", "node_type": "1", "metadata": {}, "hash": "e975f6c46299256273eb3ce079fcffe4a8229492ad3caa88d6e7be149d7de50b", "class_name": "RelatedNodeInfo"}}, "text": "When minimizing this function with respect to m(xi), note that the \ufb01rst term (yi \u2212E(yi | xi))2 doesn\u2019t matter because it does not depend on m(xi). So it will zero out. The second and third terms, though, do depend on m(xi). So rewrite 2(E(yi | xi) \u2212 m(xi)) as h(xi). Also set \u03b5i equal to [yi \u2212 E(yi | xi)] and substitute\n+ h(xi)\u03b5i +\nE(yi | xi) + m(xi)\nNow minimizing this function and setting it equal to zero we get\n21 Let\u2019s take a concrete example of this proof. Let h(xi) = \u03b1 + \u03b3 xi. Then take the joint expectation E(h(xi)\u03b5i) = E([\u03b1 +\u03b3 xi]\u03b5i). Then take conditional expectations E(\u03b1 | xi)+ E(\u03b3 | xi)E(xi | xi)E(\u03b5 | xi)} =\u03b1 + xiE(\u03b5i | xi) = 0 after we pass the conditional expectation through.\nANOVA theory. The \ufb01nal property of the CEF that we will discuss is the analysis of variance theorem, or ANOVA. According to this theo- rem, the unconditional variance in some random variable is equal to the variance in the conditional expectation plus the expectation of the conditional variance, or\nwhere V is the variance and V(yi | xi) is the conditional variance.\nLinear CEF theorem. As you probably know by now, the use of least squares in applied work is extremely common. That\u2019s because regres- sion has several justi\ufb01cations. We discussed one\u2014unbiasedness under certain assumptions about the error term. But I\u2019d like to present some slightly different arguments. Angrist and Pischke [2009] argue that linear regression may be useful even if the underlying CEF itself is not linear, because regression is a good approximation of the CEF. So keep an open mind as I break this down a little bit more.\nAngrist and Pischke [2009] give several arguments for using regression, and the linear CEF theorem is probably the easiest. Let\u2019s assume that we are sure that the CEF itself is linear. So what? Well, if the CEF is linear, then the linear CEF theorem states that the population regression is equal to that linear CEF. And if the CEF is linear, and if the population regression equals it, then of course you should use the pop- ulation regression to estimate CEF. If you need a proof for what could just as easily be considered common sense, I provide one. If E(yi | xi) is linear, then E(yi | xi) = x(cid:12)(cid:12)\u03b2 for some vector (cid:12)\u03b2. By the decomposition property, you get:\nAnd then when you solve this, you get (cid:12)\u03b2 = \u03b2. Hence E(y | x) = x(cid:12)\u03b2.\nBest linear predictor theorem. There are a few other linear theorems that are worth bringing up in this context. For instance, recall that the CEF is the minimum mean squared error predictor of y given x in the class of all functions, according to the CEF prediction property. Given\nthis, the population regression function is the best that we can do in the class of all linear functions.22\nRegression CEF theorem. I would now like to cover one more attribute of regression. The function X\u03b2 provides the minimum mean squared error linear approximation to the CEF. That is,\nSo? Let\u2019s try and back up for a second, though, and get the big picture, as all these linear theorems can leave the reader asking, \u201cSo what?\u201d I\u2019m telling you all of this because I want to present to you an argument that regression is appealing; even though it\u2019s linear, it can still be justi- \ufb01ed when the CEF itself isn\u2019t. And since we don\u2019t know with certainty that the CEF is linear, this is actually a nice argument to at least con- sider. Regression is ultimately nothing more than a crank turning data into estimates, and what I\u2019m saying here is that crank produces some- thing desirable even under bad situations. Let\u2019s look a little bit more at this crank, though, by reviewing another theorem which has become popularly known as the regression anatomy theorem.\nRegression anatomy theorem. In addition to our discussion of the CEF and regression theorems, we now dissect the regression itself. Here we discuss the regression anatomy theorem.", "start_char_idx": 56453, "end_char_idx": 60344, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "48635a48-89aa-42ca-a979-c9fbc2fc90ef": {"__data__": {"id_": "48635a48-89aa-42ca-a979-c9fbc2fc90ef", "embedding": null, "metadata": {"page number": "32 - 111"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac", "node_type": "4", "metadata": {"page number": "32 - 111"}, "hash": "99ad2284755197ab9a1483b8b716ac5e3c39d1980b4b879df9d1e92b96483fd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e476494e-51c6-4d8d-a91d-7a0d245b3b45", "node_type": "1", "metadata": {"page number": "32 - 111"}, "hash": "e2365c393c47126c276ccdff081112a4c3953146341f1ce9395811bde40dc008", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0688844b-95cc-4293-acca-fab7610e0ee7", "node_type": "1", "metadata": {}, "hash": "b4e5b7a22125aa9783238ce45d40a6f6396f3215eb040e5fe76714b04fe88b8b", "class_name": "RelatedNodeInfo"}}, "text": "And since we don\u2019t know with certainty that the CEF is linear, this is actually a nice argument to at least con- sider. Regression is ultimately nothing more than a crank turning data into estimates, and what I\u2019m saying here is that crank produces some- thing desirable even under bad situations. Let\u2019s look a little bit more at this crank, though, by reviewing another theorem which has become popularly known as the regression anatomy theorem.\nRegression anatomy theorem. In addition to our discussion of the CEF and regression theorems, we now dissect the regression itself. Here we discuss the regression anatomy theorem. The regression anatomy theorem is based on earlier work by Frisch and Waugh [1933] and Lovell [1963].23 I \ufb01nd the theorem more intuitive when I think through a spe- ci\ufb01c example and offer up some data visualization. In my opinion, the theorem helps us interpret the individual coe\ufb03cients of a multiple lin- ear regression model. Say that we are interested in the causal effect of family size on labor supply. We want to regress labor supply on family size:\nwhere Y is labor supply, and X is family size.\n23 A helpful proof of the Frisch-Waugh-Lovell theorem can be found in Lovell\nIf family size is truly random, then the number of kids in a family is uncorrelated with the unobserved error term.24 This implies that when we regress labor supply on family size, our estimate, (cid:12)\u03b21, can be inter- preted as the causal effect of family size on labor supply. We could just plot the regression coe\ufb03cient in a scatter plot showing all i pairs of data; the slope coe\ufb03cient would be the best linear \ufb01t of the data for this data cloud. Furthermore, under randomized number of children, the slope would also tell us the average causal effect of family size on labor supply.\nBut most likely, family size isn\u2019t random, because so many peo- ple choose the number of children to have in their family\u2014instead of, say, \ufb02ipping a coin. So how do we interpret (cid:12)\u03b21 if the family size is not random? Often, people choose their family size according to some- thing akin to an optimal stopping rule. People pick both how many kids to have, when to have them, and when to stop having them. In some instances, they may even attempt to pick the gender. All of these choices are based on a variety of unobserved and observed economic factors that may themselves be associated with one\u2019s decision to enter the labor market. In other words, using the language we\u2019ve been using up until now, it\u2019s unlikely that E(u | X) = E(u) = 0.\nBut let\u2019s say that we have reason to think that the number of kids in a family is conditionally random. To make this tractable for the sake of pedagogy, let\u2019s say that a particular person\u2019s family size is as good as randomly chosen once we condition on race and age.25 While unreal- istic, I include it to illustrate an important point regarding multivariate regressions. If this assumption were to be true, then we could write the following equation:\nwhere Y is labor supply, X is number of kids, R is race, A is age, and u is the population error term.\n24 While randomly having kids may sound fun, I encourage you to have kids when\nyou want to have them. Contact your local high school health teacher to learn more about\na number of methods that can reasonably minimize the number of random children you\nIf we want to estimate the average causal effect of family size on labor supply, then we need two things. First, we need a sample of data containing all four of those variables. Without all four of the variables, we cannot estimate this regression model. Second, we need for the number of kids, X, to be randomly assigned for a given set of race and age.\nNow, how do we interpret (cid:12)\u03b21? And how might we visualize this coef- \ufb01cient given that there are six dimensions to the data? The regression anatomy theorem both tells us what this coe\ufb03cient estimate actually means and also lets us visualize the data in only two dimensions.\nTo explain the intuition of the regression anatomy theorem, let\u2019s write down a population model with multiple variables. Assume that your main multiple regression model of interest has K covariates.", "start_char_idx": 59719, "end_char_idx": 63901, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0688844b-95cc-4293-acca-fab7610e0ee7": {"__data__": {"id_": "0688844b-95cc-4293-acca-fab7610e0ee7", "embedding": null, "metadata": {"page number": "32 - 111"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac", "node_type": "4", "metadata": {"page number": "32 - 111"}, "hash": "99ad2284755197ab9a1483b8b716ac5e3c39d1980b4b879df9d1e92b96483fd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "48635a48-89aa-42ca-a979-c9fbc2fc90ef", "node_type": "1", "metadata": {"page number": "32 - 111"}, "hash": "3ec361ffa5d86c9d9e5e1f6364f61162f79d111ae0798c53ee9b3fb05f1df07f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0cc7cfbf-9128-4a51-bb5c-d2be612c1a91", "node_type": "1", "metadata": {}, "hash": "864abb19c11d93ee2b4f77fa9f4f5f65879ca771682569420a28deed0e8dcce2", "class_name": "RelatedNodeInfo"}}, "text": "First, we need a sample of data containing all four of those variables. Without all four of the variables, we cannot estimate this regression model. Second, we need for the number of kids, X, to be randomly assigned for a given set of race and age.\nNow, how do we interpret (cid:12)\u03b21? And how might we visualize this coef- \ufb01cient given that there are six dimensions to the data? The regression anatomy theorem both tells us what this coe\ufb03cient estimate actually means and also lets us visualize the data in only two dimensions.\nTo explain the intuition of the regression anatomy theorem, let\u2019s write down a population model with multiple variables. Assume that your main multiple regression model of interest has K covariates. We can then write it as:\nNow assume an auxiliary regression in which the variable x1i regressed on all the remaining independent variables:\nis\nand \u02dcx1i = x1i \u2212(cid:12)x1i is the residual from that auxiliary regression. Then the parameter \u03b21 can be rewritten as:\nNotice that again we see that the coe\ufb03cient estimate is a scaled covariance, only here, the covariance is with respect to the outcome and residual from the auxiliary regression, and the scale is the variance of that same residual.\nTo prove the theorem, note that E[\u02dcxki] =E [xki] \u2212E [(cid:12)xki] =E [fi], and plug yi and residual \u02dcxki from xki auxiliary regression into the covariance cov(yi, xki):\n\u03b2k = cov(\u03b20 + \u03b21x1i + \u00b7 \u00b7 \u00b7 + \u03b2kxki + \u00b7 \u00b7 \u00b7 + \u03b2KxKi + ei, \u02dcxki) var(\u02dcxki) = cov(\u03b20 + \u03b21x1i + \u00b7 \u00b7 \u00b7 + \u03b2kxki + \u00b7 \u00b7 \u00b7 + \u03b2KxKi + ei, fi) var(fi)\nSince by construction E[fi] = 0, it follows that the term \u03b20E[fi] = 0. Since fi is a linear combination of all the independent variables with the exception of xki, it must be that\nConsider now the term E[eifi]. This can be written as\nE[eifi] =E [eifi]\n= E[eixki] \u2212E [ei\nSince ei is uncorrelated with any independent variable, it is also uncorrelated with xki. Accordingly, we have E[eixki] = 0. With regard to the second term of the subtraction, substituting the predicted value\nfrom the xki auxiliary regression, we get\nOnce again, since ei is uncorrelated with any independent vari- able, the expected value of the terms is equal to zero. It follows that E[eifi] =0.\n\u02dcxki], The only remaining term, then, is [\u03b2kxkifi], which equals E[\u03b2kxki since fi = \u02dcxki. The term xki can be substituted by rewriting the auxiliary regression model, xki, such that\nThis gives\nwhich follows directly from the orthogonality between E[xki | X\u2212k] and \u02dcxki. From previous derivations we \ufb01nally get\ncov(yi, \u02dcxki) = \u03b2k var(\u02dcxki)\nwhich completes the proof.\nI \ufb01nd it helpful to visualize things. Let\u2019s look at an example in Stata\nusing its popular automobile data set.", "start_char_idx": 63174, "end_char_idx": 65850, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0cc7cfbf-9128-4a51-bb5c-d2be612c1a91": {"__data__": {"id_": "0cc7cfbf-9128-4a51-bb5c-d2be612c1a91", "embedding": null, "metadata": {"page number": "32 - 111"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac", "node_type": "4", "metadata": {"page number": "32 - 111"}, "hash": "99ad2284755197ab9a1483b8b716ac5e3c39d1980b4b879df9d1e92b96483fd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0688844b-95cc-4293-acca-fab7610e0ee7", "node_type": "1", "metadata": {"page number": "32 - 111"}, "hash": "92a8646e0fc92476b8f58207413a51cd64f27c88c2ae02e7b269c7262629c93c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d5ae99f1-0b74-4749-a7f2-2a07423f145d", "node_type": "1", "metadata": {}, "hash": "8a59c33218b30ac83eee031f74851de98700dcebd21338df23de2cc53da72c54", "class_name": "RelatedNodeInfo"}}, "text": "It follows that E[eifi] =0.\n\u02dcxki], The only remaining term, then, is [\u03b2kxkifi], which equals E[\u03b2kxki since fi = \u02dcxki. The term xki can be substituted by rewriting the auxiliary regression model, xki, such that\nThis gives\nwhich follows directly from the orthogonality between E[xki | X\u2212k] and \u02dcxki. From previous derivations we \ufb01nally get\ncov(yi, \u02dcxki) = \u03b2k var(\u02dcxki)\nwhich completes the proof.\nI \ufb01nd it helpful to visualize things. Let\u2019s look at an example in Stata\nusing its popular automobile data set. I\u2019ll show you:\n1 ssc install reganat, replace 2 sysuse auto.dta, replace 3 regress price length 4 regress price length weight headroom mpg 5 reganat price length weight headroom mpg, dis(length) biline\n1 library(tidyverse) 2 library(haven) 3 4 read_data <- function(df) 5 { 6 7 8 9 10 } 11 12 13 auto <- read_data(\"auto.dta\") %>% 14 mutate(length = length - mean(length)) 15 16 lm1 <- lm(price ~ length, auto) 17 lm2 <- lm(price ~ length + weight + headroom + mpg, auto) 18 19\ndf, sep = \"\")\n(continued)\nR (continued)\n20 coef_lm1 <- lm1$coefficients 21 coef_lm2 <- lm2$coefficients 22 resid_lm2 <- lm2$residuals 23 24 y_single <- tibble(price = coef_lm1[1] + coef_lm1[2]*auto$length, 25 26 27 y_multi <- tibble(price = coef_lm1[1] + coef_lm2[2]*auto$length, 28 29 30 31 ggplot(auto) + 32 33 34\nLet\u2019s walk through both the regression output that I\u2019ve reproduced in Table 8 as well as a nice visualization of the slope parameters in what I\u2019ll call the short bivariate regression and the longer multivari- ate regression. The short regression of price on car length yields a coe\ufb03cient of 57.20 on length. For every additional inch, a car is $57 more expensive, which is shown by the upward-sloping, dashed line in Figure 6. The slope of that line is 57.20.\nIt will eventually become second nature for you to talk about includ- ing more variables on the right side of a regression as \u201ccontrolling for\u201d\nthose variables. But in this regression anatomy exercise, I hope to give a different interpretation of what you\u2019re doing when you in fact \u201ccon- trol for\u201d variables in a regression. First, notice how the coe\ufb03cient on length changed signs and increased in magnitude once we controlled for the other variables. Now, the effect on length is \u221294.5. It appears that the length was confounded by several other variables, and once we conditioned on them, longer cars actually were cheaper. You can see a visual representation of this in Figure 6, where the multivariate slope is negative.\nSo what exactly going on in this visualization? Well, for one, it has condensed the number of dimensions (variables) from four to only two. It did this through the regression anatomy process that we described earlier. Basically, we ran the auxiliary regression, used the residuals cov(yi, \u02dcxi) var (\u02dcxi) . This allowed us to show scatter plots of the auxiliary residuals paired with their outcome observations and to slice the slope through them (Figure 6). Notice that this is a useful way to preview the multidimensional cor- relation between two variables from a multivariate regression. Notice\nfrom it, and then calculated the slope coe\ufb03cient as\nthat the solid black line is negative and the slope from the bivariate regression is positive. The regression anatomy theorem shows that these two estimators\u2014one being a multivariate OLS and the other being a bivariate regression price and a residual\u2014are identical.\nVariance of the OLS estimators. That more or less summarizes what we want to discuss regarding the linear regression. Under a zero condi- tional mean assumption, we could epistemologically infer that the rule used to produce the coe\ufb03cient from a regression in our sample was unbiased. That\u2019s nice because it tells us that we have good reason to believe that result.", "start_char_idx": 65346, "end_char_idx": 69110, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d5ae99f1-0b74-4749-a7f2-2a07423f145d": {"__data__": {"id_": "d5ae99f1-0b74-4749-a7f2-2a07423f145d", "embedding": null, "metadata": {"page number": "32 - 111"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac", "node_type": "4", "metadata": {"page number": "32 - 111"}, "hash": "99ad2284755197ab9a1483b8b716ac5e3c39d1980b4b879df9d1e92b96483fd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0cc7cfbf-9128-4a51-bb5c-d2be612c1a91", "node_type": "1", "metadata": {"page number": "32 - 111"}, "hash": "a19b4cd29ae47785daf6000b137319cd5b2cbeb190942f3ea3b9a3b3bd253621", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b55d4b25-7e28-4c65-af3a-bc83aa6e492c", "node_type": "1", "metadata": {}, "hash": "6607a8b6e69ffb6e9776a363735549f71a5d65d5e9e1501738c49950095ea093", "class_name": "RelatedNodeInfo"}}, "text": "Notice that this is a useful way to preview the multidimensional cor- relation between two variables from a multivariate regression. Notice\nfrom it, and then calculated the slope coe\ufb03cient as\nthat the solid black line is negative and the slope from the bivariate regression is positive. The regression anatomy theorem shows that these two estimators\u2014one being a multivariate OLS and the other being a bivariate regression price and a residual\u2014are identical.\nVariance of the OLS estimators. That more or less summarizes what we want to discuss regarding the linear regression. Under a zero condi- tional mean assumption, we could epistemologically infer that the rule used to produce the coe\ufb03cient from a regression in our sample was unbiased. That\u2019s nice because it tells us that we have good reason to believe that result. But now we need to build out this epistemological justi\ufb01cation so as to capture the inherent uncertainty in the sampling process itself. This added layer of uncertainty is often called inference. Let\u2019s turn to it now.\nRemember the simulation we ran earlier in which we resampled a population and estimated regression coe\ufb03cients a thousand times? We produced a histogram of those 1,000 estimates in Figure 5. The mean of the coe\ufb03cients was around 1.998, which was very close to the true effect of 2 (hard-coded into the data-generating process). But the standard deviation was around 0.04. This means that, basically, in repeated sampling of some population, we got different estimates. But the average of those estimates was close to the true effect, and their spread had a standard deviation of 0.04. This concept of spread in repeated sampling is probably the most useful thing to keep in mind as we move through this section.\nUnder the four assumptions we discussed earlier, the OLS esti- mators are unbiased. But these assumptions are not su\ufb03cient to tell us anything about the variance in the estimator itself. The assump- tions help inform our beliefs that the estimated coe\ufb03cients, on aver- age, equal the parameter values themselves. But to speak intelligently about the variance of the estimator, we need a measure of dispersion in the sampling distribution of the estimators. As we\u2019ve been saying, this leads us to the variance and ultimately to the standard deviation. We could characterize the variance of the OLS estimators under the four assumptions. But for now, it\u2019s easiest to introduce an assumption that simpli\ufb01es the calculations. We\u2019ll keep the assumption ordering we\u2019ve been using and call this the \ufb01fth assumption.\nThe \ufb01fth assumption is the homoskedasticity or constant variance\nassumption. This assumption stipulates that our population error term,\nu, has the same variance given any value of the explanatory variable,\nx. Formally, this is:\nI always had an unusually When I was \ufb01rst learning this material, hard time wrapping my head around \u03c3 2. Part of it was because of my humanities background; I didn\u2019t really have an appreciation for random\nvariables that were dispersed. I wasn\u2019t used to taking a lot of num-\nbers and trying to measure distances between them, so things were slow to click. So if you\u2019re like me, try this. Think of \u03c3 2 as just a posi- tive number like 2 or 8. That number is measuring the spreading out\nof underlying errors themselves. In other words, the variance of the\nerrors conditional on the explanatory variable is simply some \ufb01nite,\npositive number. And that number is measuring the variance of the\nstuff other than x that in\ufb02uence the value of y itself. And because we\nassume the zero conditional mean assumption, whenever we assume\nhomoskedasticity, we can also write:\nNow, under the \ufb01rst, fourth, and \ufb01fth assumptions, we can write:\nSo the average, or expected, value of y is allowed to change with x, but\nif the errors are homoskedastic, then the variance does not change\nwith x. The constant variance assumption may not be realistic; it must\nbe determined on a case-by-case basis.\nTheorem: Sampling variance of OLS. Under assumptions 1 and 2,\nwe get:\nn i=1 \u03c3 2\nTo show this, write, as before,\nwhere wi = . We are treating this as nonrandom in the deriva- tion. Because \u03b21 is a constant, it does not affect V((cid:12)\u03b21).", "start_char_idx": 68287, "end_char_idx": 72491, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b55d4b25-7e28-4c65-af3a-bc83aa6e492c": {"__data__": {"id_": "b55d4b25-7e28-4c65-af3a-bc83aa6e492c", "embedding": null, "metadata": {"page number": "32 - 111"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac", "node_type": "4", "metadata": {"page number": "32 - 111"}, "hash": "99ad2284755197ab9a1483b8b716ac5e3c39d1980b4b879df9d1e92b96483fd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d5ae99f1-0b74-4749-a7f2-2a07423f145d", "node_type": "1", "metadata": {"page number": "32 - 111"}, "hash": "477c8679a8909b2d74e1af6ce72ea8d76dc07e4128516ce2da92cd0189e1d7fd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ca22f0a-0101-4f58-99b7-03f60a42796f", "node_type": "1", "metadata": {}, "hash": "2578214c743f7f366838934b24910b1f39280c0ae6dec531082a16586975c016", "class_name": "RelatedNodeInfo"}}, "text": "And because we\nassume the zero conditional mean assumption, whenever we assume\nhomoskedasticity, we can also write:\nNow, under the \ufb01rst, fourth, and \ufb01fth assumptions, we can write:\nSo the average, or expected, value of y is allowed to change with x, but\nif the errors are homoskedastic, then the variance does not change\nwith x. The constant variance assumption may not be realistic; it must\nbe determined on a case-by-case basis.\nTheorem: Sampling variance of OLS. Under assumptions 1 and 2,\nwe get:\nn i=1 \u03c3 2\nTo show this, write, as before,\nwhere wi = . We are treating this as nonrandom in the deriva- tion. Because \u03b21 is a constant, it does not affect V((cid:12)\u03b21). Now, we need to use the fact that, for uncorrelated random variables, the variance of the sum is the sum of the variances. The {ui : i = 1, . . . , n} are actually independent across i and uncorrelated. Remember: if we know x, we\nknow w. So:\nwhere the penultimate equality condition used the \ufb01fth assumption so\nthat the variance of ui does not depend on xi. Now we have:\nn i=1\nWe have shown:\nA couple of points. First, this is the \u201cstandard\u201d formula for the variance of the OLS slope estimator. It is not valid if the \ufb01fth assumption, of homoskedastic errors, doesn\u2019t hold. The homoskedasticity assump- tion is needed, in other words, to derive this standard formula. But the homoskedasticity assumption is not used to show unbiasedness of the OLS estimators. That requires only the \ufb01rst four assumptions.\nUsually, we are interested in \u03b21. We can easily study the two factors\nthat affect its variance: the numerator and the denominator.\nAs the error variance increases\u2014that is, as \u03c3 2 increases\u2014so does the variance in our estimator. The more \u201cnoise\u201d in the relationship between y and x (i.e., the larger the variability in u), the harder it is to learn some- thing about \u03b21. In contrast, more variation in {xi} is a good thing. As SSTx rises, V((cid:12)\u03b21) \u2193. Notice that\nas getting close to the population variance of x, \u03c3 2 This means:\nis the sample variance in x. We can think of this\nx , as n gets large.\nwhich means that as n grows, V((cid:12)\u03b21) shrinks at the rate of why more data is a good thing: it shrinks the sampling variance of our estimators.\nThe standard deviation of (cid:12)\u03b21 is the square root of the variance. So:\n. This is\nThis turns out to be the measure of variation that appears in con\ufb01dence intervals and test statistics.\nIn the formula, V((cid:12)\u03b21) = , we can compute SSTx from {xi : i = 1, . . . , n}. But we need to estimate \u03c3 2. Recall that \u03c3 2 = E(u2). Therefore, if we could observe a sample on the errors, {ui : i = 1, . . . , n}, an unbiased estimator of \u03c3 2 would be the sample average:\nNext we look at estimating the error variance.\nBut this isn\u2019t an estimator that we can compute from the data we\nobserve, because ui are unobserved. How about replacing each ui with its \u201cestimate,\u201d the OLS residual (cid:12)ui?\nWhereas ui cannot be computed, (cid:12)ui can be computed from the data because it depends on the estimators, (cid:12)\u03b20 and (cid:12)\u03b21. But, except by sheer coincidence, ui (cid:9)= (cid:12)ui for any i.\nNote that E( (cid:12)\u03b20) = \u03b20 and E((cid:12)\u03b21) = \u03b21, but the estimators almost always differ from the population values in a sample. So what about this as an estimator of \u03c3 2?\nIt is a true estimator and easily computed from the data after OLS. As it\nturns out, this estimator is slightly biased: its expected value is a little less than \u03c3 2.", "start_char_idx": 71821, "end_char_idx": 75290, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ca22f0a-0101-4f58-99b7-03f60a42796f": {"__data__": {"id_": "6ca22f0a-0101-4f58-99b7-03f60a42796f", "embedding": null, "metadata": {"page number": "32 - 111"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac", "node_type": "4", "metadata": {"page number": "32 - 111"}, "hash": "99ad2284755197ab9a1483b8b716ac5e3c39d1980b4b879df9d1e92b96483fd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b55d4b25-7e28-4c65-af3a-bc83aa6e492c", "node_type": "1", "metadata": {"page number": "32 - 111"}, "hash": "6e3cd989abf113a6407d53cc53ecd3172b5a6316a2416652fc5f910b22a5fe23", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cf428dc9-624f-4ad5-8b8c-42bac12cdb94", "node_type": "1", "metadata": {}, "hash": "fd4862606ca8f03fe6e3c4cb8719ba47b29eaa7ce2c53d223cd9383acb5ed9d6", "class_name": "RelatedNodeInfo"}}, "text": "How about replacing each ui with its \u201cestimate,\u201d the OLS residual (cid:12)ui?\nWhereas ui cannot be computed, (cid:12)ui can be computed from the data because it depends on the estimators, (cid:12)\u03b20 and (cid:12)\u03b21. But, except by sheer coincidence, ui (cid:9)= (cid:12)ui for any i.\nNote that E( (cid:12)\u03b20) = \u03b20 and E((cid:12)\u03b21) = \u03b21, but the estimators almost always differ from the population values in a sample. So what about this as an estimator of \u03c3 2?\nIt is a true estimator and easily computed from the data after OLS. As it\nturns out, this estimator is slightly biased: its expected value is a little less than \u03c3 2. The estimator does not account for the two restrictions on the residuals used to obtain (cid:12)\u03b20 and (cid:12)\u03b21:\nThere is no such restriction on the unobserved errors. The unbiased estimator, therefore, of \u03c3 2 uses a degrees-of-freedom adjustment. The residuals have only n \u2212 2, not n, degrees of freedom. Therefore:\nWe now propose the following theorem. The unbiased estimator\nof \u03c3 2 under the \ufb01rst \ufb01ve assumptions is:\nIn most software packages, regression output will include:\nThis is an estimator of sd(u), the standard deviation of the population error. One small glitch is that (cid:12)\u03c3 is not unbiased for \u03c3 .26 This will not matter for our purposes: (cid:12)\u03c3 is called the standard error of the regres- sion, which means that it is an estimate of the standard deviation of the error in the regression. The software package Stata calls it the root mean squared error.\nGiven (cid:12)\u03c3 , we can now estimate sd((cid:12)\u03b21) and sd( (cid:12)\u03b20). The estimates of these are called the standard errors of the (cid:12)\u03b2j. We will use these a lot. Almost all regression packages report the standard errors in a column next to the coe\ufb03cient estimates. We can just plug (cid:12)\u03c3 in for \u03c3 :\n26 There does exist an unbiased estimator of \u03c3 , but it\u2019s tedious and hardly anyone\nin economics seems to use it. See Holtzman [1950].\nwhere both the numerator and the denominator are computed from the data. For reasons we will see, it is useful to report the standard errors below the corresponding coe\ufb03cient, usually in parentheses.\nRobust standard errors. How realistic is it that the variance in the errors is the same for all slices of the explanatory variable, x? The short answer here is that it is probably unrealistic. Heterogeneity is just something I\u2019ve come to accept as the rule, not the exception, so if anything, we should be opting in to believing in homoskedasticity, not opting out. You can just take it as a given that errors are never homoskedastic and move forward to the solution.\nThis isn\u2019t completely bad news, because the unbiasedness of our regressions based on repeated sampling never depended on assum- ing anything about the variance of the errors. Those four assump- tions, and particularly the zero conditional mean assumption, guaran- teed that the central tendency of the coe\ufb03cients under repeated sam- pling would equal the true parameter, which for this book is a causal parameter. The problem is with the spread of the coe\ufb03cients. With- out homoskedasticity, OLS no longer has the minimum mean squared errors, which means that the estimated standard errors are biased. Using our sampling metaphor, then, the distribution of the coe\ufb03cients is probably larger than we thought. Fortunately, there is a solution. Let\u2019s write down the variance equation under heterogeneous variance (cid:7) terms:\nn i=1\nNotice the i subscript in our \u03c3 2 stant. When \u03c3 2 i\n. But when that isn\u2019t true, then we have a problem called het- SST2 x eroskedastic errors. A valid estimator of Var((cid:12)\u03b21) for heteroskedasticity of any form (including homoskedasticity) is\ni term; that means variance is not a con- = \u03c3 2 for all i, this formula reduces to the usual form,\nn i=1\nwhich is easily computed from the data after the OLS regression.", "start_char_idx": 74665, "end_char_idx": 78532, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf428dc9-624f-4ad5-8b8c-42bac12cdb94": {"__data__": {"id_": "cf428dc9-624f-4ad5-8b8c-42bac12cdb94", "embedding": null, "metadata": {"page number": "32 - 111"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac", "node_type": "4", "metadata": {"page number": "32 - 111"}, "hash": "99ad2284755197ab9a1483b8b716ac5e3c39d1980b4b879df9d1e92b96483fd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ca22f0a-0101-4f58-99b7-03f60a42796f", "node_type": "1", "metadata": {"page number": "32 - 111"}, "hash": "5d1c7f9d10bbbbc8bd451a23d4db93fa6ea5bfaba15622d7d55b8002f8c27c9c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "809d651a-50e5-4947-80c3-a06e1dcf8121", "node_type": "1", "metadata": {}, "hash": "8a90d97124383a340105bca094660961f611911328953e44f243e755bb1db45d", "class_name": "RelatedNodeInfo"}}, "text": "Using our sampling metaphor, then, the distribution of the coe\ufb03cients is probably larger than we thought. Fortunately, there is a solution. Let\u2019s write down the variance equation under heterogeneous variance (cid:7) terms:\nn i=1\nNotice the i subscript in our \u03c3 2 stant. When \u03c3 2 i\n. But when that isn\u2019t true, then we have a problem called het- SST2 x eroskedastic errors. A valid estimator of Var((cid:12)\u03b21) for heteroskedasticity of any form (including homoskedasticity) is\ni term; that means variance is not a con- = \u03c3 2 for all i, this formula reduces to the usual form,\nn i=1\nwhich is easily computed from the data after the OLS regression. We\nhave Friedhelm Eicker, Peter J. Huber, and Halbert White to thank for this solution (White [1980]).27 The solution for heteroskedasticity goes\nby several names, but the most common is \u201crobust\u201d standard error.\nCluster robust standard errors. People will try to scare you by chal-\nlenging how you constructed your standard errors. Heteroskedastic\nerrors, though, aren\u2019t the only thing you should be worried about when\nit comes to inference. Some phenomena do not affect observations\nindividually, but they do affect groups of observations that involve indi-\nviduals. And then they affect those individuals within the group in a\ncommon way. Say you want to estimate the effect of class size on stu-\ndent achievement, but you know that there exist unobservable things\n(like the teacher) that affect all the students equally. If we can commit\nual student unobservables are correlated within a class, then we have\na situation in which we need to cluster the standard errors. Before we\ndive into an example, I\u2019d like to start with a simulation to illustrate the\nAs a baseline for this simulation, let\u2019s begin by simulating nonclus-\ntered data and analyze least squares estimates of that nonclustered\ndata. This will help \ufb01rm up our understanding of the problems that occur with least squares when data is clustered.28\n27 No one even bothers to cite White [1980] anymore, just like how no one cites\nLeibniz or Newton when using calculus. Eicker, Huber, and White created a solution so\nvaluable that it got separated from the original papers when it was absorbed into the\n28 Hat tip to Ben Chidmi, who helped create this simulation in Stata.\n1 clear all 2 set seed 20140 3 * Set the number of simulations 4 local n_sims = 1000 5 set obs `n_sims' 6 7 * Create the variables that will contain the results of each simulation 8 generate beta_0 = . 9 generate beta_0_l = . 10 generate beta_0_u = . 11 generate beta_1 = . 12 generate beta_1_l = . 13 generate beta_1_u = . 14 15 16 * Provide the true population parameters 17 local beta_0_true = 0.4 18 local beta_1_true = 0 19 local rho = 0.5 20 21 * Run the linear regression 1000 times and save the parameters beta_0 and\nforvalues i = 1(1) `n_sims' {\npreserve clear set obs 100 generate x = rnormal(0,1) generate e = rnormal(0, sqrt(1 - `rho')) generate y = `beta_0_true' + `beta_1_true'*x + e regress y x local b0 = _b[_cons] local b1 = _b[x] local df = e(df_r) local critical_value = invt(`df', 0.975) restore\n(continued)\nSTATA (continued)\n36 37 38 39 40 41 42 43 44 } 45 gen false = (beta_1_l > 0 ) 46 replace false = 2 if beta_1_u < 0 47 replace false = 3 if false == 0 48 tab false 49 50 * Plot the parameter estimate 51 hist beta_1,", "start_char_idx": 77887, "end_char_idx": 81211, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "809d651a-50e5-4947-80c3-a06e1dcf8121": {"__data__": {"id_": "809d651a-50e5-4947-80c3-a06e1dcf8121", "embedding": null, "metadata": {"page number": "32 - 111"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac", "node_type": "4", "metadata": {"page number": "32 - 111"}, "hash": "99ad2284755197ab9a1483b8b716ac5e3c39d1980b4b879df9d1e92b96483fd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf428dc9-624f-4ad5-8b8c-42bac12cdb94", "node_type": "1", "metadata": {"page number": "32 - 111"}, "hash": "ee0c090284995c06c462773a3db7dc0c3c80a5577ab5cc2bea07ccbe89252203", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9ce80a1a-9b2a-4fa1-915e-93878bf37a06", "node_type": "1", "metadata": {}, "hash": "ce9323a2beda77fa718e97ed478d802673458dfe44688f2c7693fea31c007e77", "class_name": "RelatedNodeInfo"}}, "text": "1) generate e = rnormal(0, sqrt(1 - `rho')) generate y = `beta_0_true' + `beta_1_true'*x + e regress y x local b0 = _b[_cons] local b1 = _b[x] local df = e(df_r) local critical_value = invt(`df', 0.975) restore\n(continued)\nSTATA (continued)\n36 37 38 39 40 41 42 43 44 } 45 gen false = (beta_1_l > 0 ) 46 replace false = 2 if beta_1_u < 0 47 replace false = 3 if false == 0 48 tab false 49 50 * Plot the parameter estimate 51 hist beta_1, frequency addplot(pci 0 0 100 0) title(\"Least squares estimates of\nreplace beta_0 = `b0' in `i' replace beta_0_l = beta_0 - `critical_value'*_se[_cons] replace beta_0_u = beta_0 + `critical_value'*_se[_cons] replace beta_1 = `b1' in `i' replace beta_1_l = beta_1 - `critical_value'*_se[x] replace beta_1_u = beta_1 + `critical_value'*_se[x]\nnon-clustered data\") subtitle(\" Monte Carlo simulation of the slope\") legend(label(1 \"Distribution of least squares estimates\") label(2 \"True population parameter\")) xtitle(\"Parameter estimate\")\n52 53 sort beta_1 54 gen int sim_ID = _n 55 gen beta_1_True = 0 56 * Plot of the Confidence Interval 57 twoway rcap beta_1_l beta_1_u sim_ID if beta_1_l > 0 | beta_1_u < 0 , horizontal\nlcolor(pink) || || ///\n58 rcap beta_1_l beta_1_u sim_ID if beta_1_l < 0 & beta_1_u > 0 , horizontal ysc(r(0))\n59 connected sim_ID beta_1 || || /// 60 line sim_ID beta_1_True, lpattern(dash) lcolor(black) lwidth(1) /// 61 title(\"Least squares estimates of non-clustered data\") subtitle(\" 95% Confidence\n62 legend(label(1 \"Missed\") label(2 \"Hit\") label(3 \"OLS estimates\") label(4 \"True\n# Function to generate clustered data # Required package: mvtnorm\n# predictor var consists of individual- and cluster-level components x <- values_i[ , 1] + rep(values_cl[ , 1], each = n / n_cluster)\n# error consists of individual- and cluster-level components error <- values_i[ , 2] + rep(values_cl[ , 2], each = n / n_cluster)\n# data generating process y <- param[1] + param[2]*x + error\n(continued)\nR (continued)\n39 40 # Simulate a dataset with clusters and fit OLS 41 # Calculate cluster-robust SE when cluster_robust = TRUE 42 cluster_sim <- function(param = c(.1, .5), n = 1000, n_cluster = 50, 43 rho = .5, cluster_robust = FALSE) { 44 # Required packages: mvtnorm, multiwayvcov 45 df <- gen_cluster(param = param, n = n , n_cluster = n_cluster, rho = rho) 46 fit <- lm(y ~ x, data = df) 47 b1 <- coef(fit)[2] 48 if (!cluster_robust) { 49 Sigma <- vcov(fit) 50 se <- sqrt(diag(Sigma)[2]) 51 b1_ci95 <- confint(fit)[2, ] 52 } else { # cluster-robust SE 53 Sigma <- cluster.vcov(fit, ~ cluster) 54 se <- sqrt(diag(Sigma)[2]) 55 t_critical <- qt(.025, df = n - 2, lower.tail = FALSE) 56 lower <- b1 - t_critical*se 57 upper <- b1 + t_critical*se 58 b1_ci95 <- c(lower, upper) 59 60 61 } 62 63 # Function to iterate the simulation. A data frame is returned.", "start_char_idx": 80774, "end_char_idx": 83578, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9ce80a1a-9b2a-4fa1-915e-93878bf37a06": {"__data__": {"id_": "9ce80a1a-9b2a-4fa1-915e-93878bf37a06", "embedding": null, "metadata": {"page number": "32 - 111"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac", "node_type": "4", "metadata": {"page number": "32 - 111"}, "hash": "99ad2284755197ab9a1483b8b716ac5e3c39d1980b4b879df9d1e92b96483fd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "809d651a-50e5-4947-80c3-a06e1dcf8121", "node_type": "1", "metadata": {"page number": "32 - 111"}, "hash": "0c8b1db92ffdeebe3832fd01cefbe167cf206ec8cac5bc3fa119787496f27916", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "080e709e-2c6d-4206-ac48-2aac0cccbbd4", "node_type": "1", "metadata": {}, "hash": "d4ec2403cefc785a494b2bca3805b2f87645f7ad211e769fe2cefbf28dd1b608", "class_name": "RelatedNodeInfo"}}, "text": "n_cluster = n_cluster, rho = rho) 46 fit <- lm(y ~ x, data = df) 47 b1 <- coef(fit)[2] 48 if (!cluster_robust) { 49 Sigma <- vcov(fit) 50 se <- sqrt(diag(Sigma)[2]) 51 b1_ci95 <- confint(fit)[2, ] 52 } else { # cluster-robust SE 53 Sigma <- cluster.vcov(fit, ~ cluster) 54 se <- sqrt(diag(Sigma)[2]) 55 t_critical <- qt(.025, df = n - 2, lower.tail = FALSE) 56 lower <- b1 - t_critical*se 57 upper <- b1 + t_critical*se 58 b1_ci95 <- c(lower, upper) 59 60 61 } 62 63 # Function to iterate the simulation. A data frame is returned. 64 run_cluster_sim <- function(n_sims = 1000, param = c(.1, .5), n = 1000, 65 66 67 68 69 70 71 72 73 74 75 76 }\n# Required packages: mvtnorm, multiwayvcov, dplyr df <- replicate(n_sims, cluster_sim(param = param, n = n, rho = rho,\ndf <- as.data.frame(t(df)) names(df) <- c('b1', 'se_b1', 'ci95_lower', 'ci95_upper') df <- df %>% mutate(id = 1:n(),\nparam_caught = ci95_lower <= param[2] & ci95_upper >= param[2])\n(continued)\nR (continued)\n77 78 # Distribution of the estimator and confidence intervals 79 sim_params <- c(.4, 0) # beta1 = 0: no effect of x on y 80 sim_nocluster <- run_cluster_sim(n_sims = 10000, param = sim_params, rho = 0) 81 hist_nocluster <- ggplot(sim_nocluster, aes(b1)) + 82 83 84 print(hist_nocluster) 85 86 ci95_nocluster <- ggplot(sample_n(sim_nocluster, 100), 87 88 89 geom_hline(yintercept = sim_params[2], linetype = 'dashed') + 90 91 geom_pointrange() + labs(x = 'sim ID', y ='b1', title = 'Randomly Chosen 100 95% CIs') + 92 scale_color_discrete(name = 'True param value', labels = c('missed', 'hit')) + 93 94 coord_flip() 95 print(ci95_nocluster) 96 97 sim_nocluster %>% summarize(type1_error = 1 - sum(param_caught)/n()) 98 99\ngeom_histogram(color = 'black') + geom_vline(xintercept = sim_params[2], color = 'red')\nAs we can see in Figure 7, the least squares estimate is centered\nSetting the signi\ufb01cance level at 5%, we should incorrectly reject the null that \u03b21 = 0 about 5% of the time in our simulations. But let\u2019s check the con\ufb01dence intervals. As can be seen in Figure 8, about 95% of the 95% con\ufb01dence intervals contain the true value of \u03b21, which is zero. In words, this means that we incorrectly reject the null about 5% of the time.\nBut what happens when we use least squares with clustered data? To see that, let\u2019s resimulate our data with observations that are no longer independent draws in a given cluster of observations.\nFigure 7. Distribution of the least squares estimator over 1,000 random draws.\nFigure 8. Distribution of the 95% con\ufb01dence intervals with shading showing those that\nare incorrectly rejecting the null.\n1 clear all 2 set seed 20140 3 local n_sims = 1000 4 set obs `n_sims' 5 6 * Create the variables that will contain the results of each simulation 7 generate beta_0 = . 8 generate beta_0_l = . 9 generate beta_0_u = . 10 generate beta_1 = . 11 generate beta_1_l = . 12 generate beta_1_u = . 13 14 15 * Provide the true population parameters 16 local beta_0_true = 0.4 17 local beta_1_true = 0 18 local rho = 0.5 19 20 * Simulate a linear regression.", "start_char_idx": 83048, "end_char_idx": 86098, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "080e709e-2c6d-4206-ac48-2aac0cccbbd4": {"__data__": {"id_": "080e709e-2c6d-4206-ac48-2aac0cccbbd4", "embedding": null, "metadata": {"page number": "32 - 111"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac", "node_type": "4", "metadata": {"page number": "32 - 111"}, "hash": "99ad2284755197ab9a1483b8b716ac5e3c39d1980b4b879df9d1e92b96483fd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9ce80a1a-9b2a-4fa1-915e-93878bf37a06", "node_type": "1", "metadata": {"page number": "32 - 111"}, "hash": "e2eef37bdfba2621ec1f9c571d92f564f305b93b5f8344ad32670ddba53b525f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6d916272-dd5b-4712-93ff-eb8347135643", "node_type": "1", "metadata": {}, "hash": "48e1fc4c137fe318dad007985cbc43c86e3f891f7a7c979b0022f151ab15cfe6", "class_name": "RelatedNodeInfo"}}, "text": "Figure 7. Distribution of the least squares estimator over 1,000 random draws.\nFigure 8. Distribution of the 95% con\ufb01dence intervals with shading showing those that\nare incorrectly rejecting the null.\n1 clear all 2 set seed 20140 3 local n_sims = 1000 4 set obs `n_sims' 5 6 * Create the variables that will contain the results of each simulation 7 generate beta_0 = . 8 generate beta_0_l = . 9 generate beta_0_u = . 10 generate beta_1 = . 11 generate beta_1_l = . 12 generate beta_1_u = . 13 14 15 * Provide the true population parameters 16 local beta_0_true = 0.4 17 local beta_1_true = 0 18 local rho = 0.5 19 20 * Simulate a linear regression. Clustered data (x and e are clustered) 21 22 23 quietly { 24 forvalues i = 1(1) `n_sims' { 25 26 27 28 29 30 31 32 33 34 35\npreserve clear set obs 50\n(continued)\nSTATA (continued)\n36 37 38 39 40 41 42 43 44 45 * Least Squares Estimates 46 regress y x 47 local b0 = _b[_cons] 48 local b1 = _b[x] 49 local df = e(df_r) local critical_value = invt(`df', 0.975) 50 51 * Save the results 52 restore replace beta_0 = `b0' in `i' 53 replace beta_0_l = beta_0 - `critical_value'*_se[_cons] 54 replace beta_0_u = beta_0 + `critical_value'*_se[_cons] 55 replace beta_1 = `b1' in `i' 56 replace beta_1_l = beta_1 - `critical_value'*_se[x] 57 replace beta_1_u = beta_1 + `critical_value'*_se[x] 58 59 } 60 } 61 62 gen false = (beta_1_l > 0 ) 63 replace false = 2 if beta_1_u < 0 64 replace false = 3 if false == 0 65 tab false 66 67 * Plot the parameter estimate 68 hist beta_1, frequency addplot(pci 0 0 100 0) title(\"Least squares estimates of clustered Data\") subtitle(\" Monte Carlo simulation of the slope\") legend(label(1 \"Distribution of least squares estimates\") label(2 \"True population parameter\")) xtitle(\"Parameter estimate\")\n4 5 library('arm') 6 library('mvtnorm') 7 library('lme4') 8 library('multiwayvcov') 9 library('clusterSEs') 10 library('ggplot2') 11 library('dplyr') 12 library('haven') 13 14 #Data with clusters 15 sim_params <- c(.4, 0) # beta1 = 0: no effect of x on y 16 sim_cluster_ols <- run_cluster_sim(n_sims = 10000, param = sim_params) 17 hist_cluster_ols <- hist_nocluster %+% sim_cluster_ols 18 print(hist_cluster_ols)\nAs can be seen in Figure 9, the least squares estimate has a nar- rower spread than that of the estimates when the data isn\u2019t clustered. But to see this a bit more clearly, let\u2019s look at the con\ufb01dence intervals again.\nFigure 9. Distribution of the least squares estimator over 1,000 random draws.", "start_char_idx": 85450, "end_char_idx": 87934, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6d916272-dd5b-4712-93ff-eb8347135643": {"__data__": {"id_": "6d916272-dd5b-4712-93ff-eb8347135643", "embedding": null, "metadata": {"page number": "32 - 111"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac", "node_type": "4", "metadata": {"page number": "32 - 111"}, "hash": "99ad2284755197ab9a1483b8b716ac5e3c39d1980b4b879df9d1e92b96483fd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "080e709e-2c6d-4206-ac48-2aac0cccbbd4", "node_type": "1", "metadata": {"page number": "32 - 111"}, "hash": "4deffacaf13775fd511d7c793cf118175ec5d0366de8f4400d1f7b4f9e24b7ca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1faed875-be71-4f56-9041-1c4934363585", "node_type": "1", "metadata": {}, "hash": "ddfa80fc325bddc66d73afb6ef4782dab33658a30210b0914148fdf53431c7f8", "class_name": "RelatedNodeInfo"}}, "text": "But to see this a bit more clearly, let\u2019s look at the con\ufb01dence intervals again.\nFigure 9. Distribution of the least squares estimator over 1,000 random draws.\n1 sort beta_1 2 gen int sim_ID = _n 3 gen beta_1_True = 0 4 5 * Plot of the Confidence Interval 6 twoway rcap beta_1_l beta_1_u sim_ID if beta_1_l > 0 | beta_1_u < 0 , horizontal\nlcolor(pink) || || ///\n7 rcap beta_1_l beta_1_u sim_ID if beta_1_l < 0 & beta_1_u > 0 , horizontal ysc(r(0))\n8 connected sim_ID beta_1 || || /// 9 line sim_ID beta_1_True, lpattern(dash) lcolor(black) lwidth(1) /// 10 title(\"Least squares estimates of clustered data\") subtitle(\" 95% Confidence\n11 legend(label(1 \"Missed\") label(2 \"Hit\") label(3 \"OLS estimates\") label(4 \"True\ning those estimates that incorrectly reject the null.\nFigure 10 shows the distribution of 95% con\ufb01dence intervals from the least squares estimates. As can be seen, a much larger number of estimates incorrectly rejected the null hypothesis when the data was clustered. The standard deviation of the estimator shrinks under clus- tered data, causing us to reject the null incorrectly too often. So what can we do?\n1 * Robust Estimates 2 clear all 3 local n_sims = 1000 4 set obs `n_sims' 5 6 * Create the variables that will contain the results of each simulation 7 generate beta_0_robust = . 8 generate beta_0_l_robust = . 9 generate beta_0_u_robust = . 10 generate beta_1_robust = .\n(continued)\nSTATA (continued)\n11 generate beta_1_l_robust = . 12 generate beta_1_u_robust = . 13 14 * Provide the true population parameters 15 local beta_0_true = 0.4 16 local beta_1_true = 0 17 local rho = 0.5 18 19 quietly { 20 forvalues i = 1(1) `n_sims' { 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 * Robust Estimates 37 clear all 38 local n_sims = 1000 39 set obs `n_sims' 40 41 * Create the variables that will contain the results of each simulation 42 generate beta_0_robust = . 43 generate beta_0_l_robust = . 44 generate beta_0_u_robust = . 45 generate beta_1_robust = . 46 generate beta_1_l_robust = . 47 generate beta_1_u_robust = .", "start_char_idx": 87775, "end_char_idx": 89823, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1faed875-be71-4f56-9041-1c4934363585": {"__data__": {"id_": "1faed875-be71-4f56-9041-1c4934363585", "embedding": null, "metadata": {"page number": "32 - 111"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac", "node_type": "4", "metadata": {"page number": "32 - 111"}, "hash": "99ad2284755197ab9a1483b8b716ac5e3c39d1980b4b879df9d1e92b96483fd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6d916272-dd5b-4712-93ff-eb8347135643", "node_type": "1", "metadata": {"page number": "32 - 111"}, "hash": "29d5833b3948f44034880c30711c60d46bb006add287c7a953c7247f6395dc47", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "48922572-24b8-43ce-95d0-f146f47a9671", "node_type": "1", "metadata": {}, "hash": "7e5aa5486319f05ad8355bc6d3fb5ce9ed686b78a06e6bd150f10cc400a9c0b8", "class_name": "RelatedNodeInfo"}}, "text": "43 generate beta_0_l_robust = . 44 generate beta_0_u_robust = . 45 generate beta_1_robust = . 46 generate beta_1_l_robust = . 47 generate beta_1_u_robust = . 48\npreserve clear set obs 50\n(continued)\nSTATA (continued)\npreserve clear set obs 50\n(continued)\nSTATA (continued)\n85 86 87 88 89 } 90 } 91 92 * Plot the histogram of the parameters estimates of the robust least squares 93 gen false = (beta_1_l_robust > 0 ) 94 replace false = 2 if beta_1_u_robust < 0 95 replace false = 3 if false == 0 96 tab false 97 98 * Plot the parameter estimate 99 hist beta_1_robust, frequency addplot(pci 0 0 110 0) title(\"Robust least\nreplace beta_1_robust = `b1_robust' in `i' replace beta_1_l_robust = beta_1_robust - `critical_value'*_se[x] replace beta_1_u_robust = beta_1_robust + `critical_value'*_se[x]\nsquares estimates of clustered data\") subtitle(\" Monte Carlo simulation of the slope\") legend(label(1 \"Distribution of robust least squares estimates\") label(2 \"True population parameter\")) xtitle(\"Parameter estimate\")\n100 101 sort beta_1_robust 102 gen int sim_ID = _n 103 gen beta_1_True = 0 104 105 * Plot of the Confidence Interval 106 twoway rcap beta_1_l_robust beta_1_u_robust sim_ID if beta_1_l_robust > 0 |\nbeta_1_u_robust < 0, horizontal lcolor(pink) || || rcap beta_1_l_robust beta_1_u_robust sim_ID if beta_1_l_robust < 0 & beta_1_u_robust > 0 , horizontal ysc(r(0)) || || connected sim_ID beta_1_robust || || line sim_ID beta_1_True, lpattern(dash) lcolor(black) lwidth(1) title(\"Robust least squares estimates of clustered data\") subtitle(\" 95% Confidence interval of the slope\") legend(label(1 \"Missed\") label(2 \"Hit\") label(3 \"Robust estimates\") label(4 \"True population parameter\")) xtitle(\"Parameter estimates\") ytitle(\"Simulation\")\n4 5 library('arm') 6 library('mvtnorm') 7 library('lme4') 8 library('multiwayvcov') 9 library('clusterSEs') 10 library('ggplot2') 11 library('dplyr') 12 library('haven') 13 14 #clustered robust 15 sim_params <- c(.4, 0) # beta1 = 0: no effect of x on y 16 sim_cluster_robust <- run_cluster_sim(n_sims = 10000, param = sim_params, 17 18 19 hist_cluster_robust <- hist_nocluster %+% sim_cluster_ols 20 print(hist_cluster_robust) 21 22 #Confidence Intervals 23 ci95_cluster_robust <- ci95_nocluster %+% sample_n(sim_cluster_robust, 100) 24 print(ci95_cluster_robust) 25 26 sim_cluster_robust %>% summarize(type1_error = 1 - sum(param_caught)/n())\nNow in this case, notice that we included the \u201c, cluster(cluster_ID)\u201d\nsyntax in our regression command. Before we dive in to what this syn-\ntax did, let\u2019s look at how the con\ufb01dence intervals changed. Figure 11\nshows the distribution of the 95% con\ufb01dence intervals where, again,\nthe darkest region represents those estimates that incorrectly rejected\nthe null. Now, when there are observations whose errors are correlated\nwithin a cluster, we \ufb01nd that estimating the model using least squares\nleads us back to a situation in which the type I error has decreased\nFigure 11. Distribution of 1,000 95% con\ufb01dence intervals from a clustered robust least\nsquares regression, with dashed region representing those estimates that\nincorrectly reject the null.\nThis leads us to a natural question: what did the adjustment of the estimator\u2019s variance do that caused the type I error to decrease by so much? Whatever it\u2019s doing, it sure seems to be working! Let\u2019s dive in to this adjustment with an example.", "start_char_idx": 89666, "end_char_idx": 93050, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "48922572-24b8-43ce-95d0-f146f47a9671": {"__data__": {"id_": "48922572-24b8-43ce-95d0-f146f47a9671", "embedding": null, "metadata": {"page number": "32 - 111"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac", "node_type": "4", "metadata": {"page number": "32 - 111"}, "hash": "99ad2284755197ab9a1483b8b716ac5e3c39d1980b4b879df9d1e92b96483fd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1faed875-be71-4f56-9041-1c4934363585", "node_type": "1", "metadata": {"page number": "32 - 111"}, "hash": "6ac4e351c92c0dea0ca577c6931835d9f9e86c120bf150a67b9c671d8e2c2eda", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "36a0c3a4-c944-4edd-9e06-bc216346c954", "node_type": "1", "metadata": {}, "hash": "162591813ac01d3604053496d21d2138d92f0b067284bde31aeb43ab745b577b", "class_name": "RelatedNodeInfo"}}, "text": "Figure 11\nshows the distribution of the 95% con\ufb01dence intervals where, again,\nthe darkest region represents those estimates that incorrectly rejected\nthe null. Now, when there are observations whose errors are correlated\nwithin a cluster, we \ufb01nd that estimating the model using least squares\nleads us back to a situation in which the type I error has decreased\nFigure 11. Distribution of 1,000 95% con\ufb01dence intervals from a clustered robust least\nsquares regression, with dashed region representing those estimates that\nincorrectly reject the null.\nThis leads us to a natural question: what did the adjustment of the estimator\u2019s variance do that caused the type I error to decrease by so much? Whatever it\u2019s doing, it sure seems to be working! Let\u2019s dive in to this adjustment with an example. Consider the following model:\nwhich equals zero if g = g(cid:12) and equals \u03c3(ij)g if g (cid:9)= g(cid:12).\nLet\u2019s stack the data by cluster \ufb01rst.\nThe OLS estimator is still (cid:12)\u03b2 = E[X(cid:12)X]\u22121X(cid:12)Y. We just stacked the data, which doesn\u2019t affect the estimator itself. But it does change the variance.\nWith this in mind, we can now write the variance-covariance matrix for clustered data as\nAdjusting for clustered data will be quite common in your applied work given the ubiquity of clustered data in the \ufb01rst place. It\u2019s abso- lutely essential for working in the panel contexts, or in repeated cross- sections like the difference-in-differences design. But it also turns out to be important for experimental design, because often, the treatment will be at a higher level of aggregation than the microdata itself. In the real world, though, you can never assume that errors are indepen- dent draws from the same distribution. You need to know how your variables were constructed in the \ufb01rst place in order to choose the cor- rect error structure for calculating your standard errors. If you have aggregate variables, like class size, then you\u2019ll need to cluster at that level. If some treatment occurred at the state level, then you\u2019ll need to cluster at that level. There\u2019s a large literature available that looks at even more complex error structures, such as multi-way clustering [Cameron et al., 2011].\nBut even the concept of the sample as the basis of standard errors may be shifting. It\u2019s becoming increasingly less the case that researchers work with random samples; they are more likely work- ing with administrative data containing the population itself, and thus the concept of sampling uncertainty becomes strained.29 For instance, Manski and Pepper [2018] wrote that \u201crandom sampling assumptions . . . are not natural when considering states or counties as units of observation.\u201d So although a metaphor of a superpopulation may be useful for extending these classical uncertainty concepts, the ubiq- uity of digitized administrative data sets has led econometricians and statisticians to think about uncertainty in other ways.\nNew work by Abadie et al. [2020] explores how sampling-based concepts of the standard error may not be the right way to think about\n29 Usually we appeal to superpopulations in such situations where the observed\npopulation is simply itself a draw from some \u201csuper\u201d population.\nuncertainty in the context of causal inference, or what they call design- based uncertainty. This work in many ways anticipates the next two chapters because of its direct reference to the concept of the counter- factual. Design-based uncertainty is a re\ufb02ection of not knowing which values would have occurred had some intervention been different in counterfactual. And Abadie et al. [2020] derive standard errors for design-based uncertainty, as opposed to sampling-based uncertainty. As luck would have it, those standard errors are usually smaller.\nLet\u2019s now move into these fundamental concepts of causality used in applied work and try to develop the tools to understand how counterfactuals and causality work together.", "start_char_idx": 92256, "end_char_idx": 96204, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "36a0c3a4-c944-4edd-9e06-bc216346c954": {"__data__": {"id_": "36a0c3a4-c944-4edd-9e06-bc216346c954", "embedding": null, "metadata": {"page number": "112 - 113"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b54e1c91-a86f-40ac-afbe-d6b1e34d2c64", "node_type": "4", "metadata": {"page number": "112 - 113"}, "hash": "bc620540d853e925d6439198171f0743789888b8980144c7cb931ee33b729746", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "48922572-24b8-43ce-95d0-f146f47a9671", "node_type": "1", "metadata": {"page number": "32 - 111"}, "hash": "af571f704719ac96287a4251f7b3d516df5b996b781ac56a9da15374a0a749bf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9924e15b-0480-4bdf-adaf-e3481237fcc6", "node_type": "1", "metadata": {}, "hash": "e1f84c84932bc0ed44dc689c0fbe133eddb949157bbcd300bc6d9fbe78e82bb6", "class_name": "RelatedNodeInfo"}}, "text": "Directed Acyclic Graphs:\nEveryday it rains, so everyday the pain Went ignored and I\u2019m sure ignorance was to blame But life is a chain, cause and effected.\nThe history of graphical causal modeling goes back to the early twentieth century and Sewall Wright, one of the fathers of modern genetics and son of the economist Philip Wright. Sewall developed path diagrams for genetics, and Philip, it is believed, adapted them for econometric identi\ufb01cation [Matsueda, 2012].1\nBut despite that promising start, the use of graphical modeling for causal inference has been largely ignored by the economics profes- sion, with a few exceptions [Heckman and Pinto, 2015; Imbens, 2019]. It was revitalized for the purpose of causal inference when computer scientist and Turing Award winner Judea Pearl adapted them for his work on arti\ufb01cial intelligence. He explained this in his mangum opus, which is a general theory of causal inference that expounds on the use- fulness of his directed graph notation [Pearl, 2009]. Since graphical models are immensely helpful for designing a credible identi\ufb01cation strategy, I have chosen to include them for your consideration. Let\u2019s\n1 I will discuss the Wrights again in the chapter on instrumental variables. They\nwere an interesting pair.\nreview graphical models, one of Pearl\u2019s contributions to the theory of causal inference.2", "start_char_idx": 0, "end_char_idx": 1356, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9924e15b-0480-4bdf-adaf-e3481237fcc6": {"__data__": {"id_": "9924e15b-0480-4bdf-adaf-e3481237fcc6", "embedding": null, "metadata": {"page number": "113 - 135"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "955ddc19-f5ce-411b-b542-1a22027b6054", "node_type": "4", "metadata": {"page number": "113 - 135"}, "hash": "4e5a8b8dbb9310db94862aedc9b794c9d940cbf7b391dc117f873999721fd335", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "36a0c3a4-c944-4edd-9e06-bc216346c954", "node_type": "1", "metadata": {"page number": "112 - 113"}, "hash": "b7b90cb06013993e4455d7745620108454cda3303a59f8c5ff3fd143ec8576d0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e4561bb4-b35d-454e-81e6-42023f5607c2", "node_type": "1", "metadata": {}, "hash": "2c79b2ca7800ef0e513908e5bf23a4cb020ec66f558fdc0cb023cb383c22eb22", "class_name": "RelatedNodeInfo"}}, "text": "Introduction to DAG Notation:\nUsing directed acyclic graphical (DAG) notation requires some up- front statements. The \ufb01rst thing to notice is that in DAG notation, causality runs in one direction. Speci\ufb01cally, it runs forward in time. There are no cycles in a DAG. To show reverse causality, one would need to create multiple nodes, most likely with two versions of the same node separated by a time index. Similarly, simultaneity, such as in supply and demand models, is not straightforward with DAGs [Heck- man and Pinto, 2015]. To handle either simultaneity or reverse causal- ity, it is recommended that you take a completely different approach to the problem than the one presented in this chapter. Third, DAGs explain causality in terms of counterfactuals. That is, a causal effect is de\ufb01ned as a comparison between two states of the world\u2014one state that actually happened when some intervention took on some value and another state that didn\u2019t happen (the \u201ccounterfactual\u201d) under some other intervention.\nThink of a DAG as like a graphical representation of a chain of causal effects. The causal effects are themselves based on some underlying, unobserved structured process, one an economist might call the equilibrium values of a system of behavioral equations, which are themselves nothing more than a model of the world. All of this is captured e\ufb03ciently using graph notation, such as nodes and arrows. Nodes represent random variables, and those random variables are assumed to be created by some data-generating process.3 Arrows rep- resent a causal effect between two random variables moving in the intuitive direction of the arrow. The direction of the arrow captures the direction of causality.\n2 If you \ufb01nd this material interesting, I highly recommend Morgan and Winship\n3 I leave out some of those details, though, because their presence (usually just\nerror terms pointing to the variables) clutters the graph unnecessarily.\nreview graphical models, one of Pearl\u2019s contributions to the theory of causal inference.2\nUsing directed acyclic graphical (DAG) notation requires some up- front statements. The \ufb01rst thing to notice is that in DAG notation, causality runs in one direction. Speci\ufb01cally, it runs forward in time. There are no cycles in a DAG. To show reverse causality, one would need to create multiple nodes, most likely with two versions of the same node separated by a time index. Similarly, simultaneity, such as in supply and demand models, is not straightforward with DAGs [Heck- man and Pinto, 2015]. To handle either simultaneity or reverse causal- ity, it is recommended that you take a completely different approach to the problem than the one presented in this chapter. Third, DAGs explain causality in terms of counterfactuals. That is, a causal effect is de\ufb01ned as a comparison between two states of the world\u2014one state that actually happened when some intervention took on some value and another state that didn\u2019t happen (the \u201ccounterfactual\u201d) under some other intervention.\nThink of a DAG as like a graphical representation of a chain of causal effects. The causal effects are themselves based on some underlying, unobserved structured process, one an economist might call the equilibrium values of a system of behavioral equations, which are themselves nothing more than a model of the world. All of this is captured e\ufb03ciently using graph notation, such as nodes and arrows. Nodes represent random variables, and those random variables are assumed to be created by some data-generating process.3 Arrows rep- resent a causal effect between two random variables moving in the intuitive direction of the arrow. The direction of the arrow captures the direction of causality.\n2 If you \ufb01nd this material interesting, I highly recommend Morgan and Winship\n3 I leave out some of those details, though, because their presence (usually just\nerror terms pointing to the variables) clutters the graph unnecessarily.\nCausal effects can happen in two ways. They can either be direct (e.g., D \u2192 Y), or they can be mediated by a third variable (e.g., D \u2192 X \u2192 Y). When they are mediated by a third variable, we are captur- ing a sequence of events originating with D, which may or may not be important to you depending on the question you\u2019re asking.\nA DAG is meant to describe all causal relationships relevant to the effect of D on Y. What makes the DAG distinctive is both the explicit commitment to a causal effect pathway and the complete commit- ment to the lack of a causal pathway represented by missing arrows. In other words, a DAG will contain both arrows connecting variables and choices to exclude arrows.", "start_char_idx": 0, "end_char_idx": 4644, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e4561bb4-b35d-454e-81e6-42023f5607c2": {"__data__": {"id_": "e4561bb4-b35d-454e-81e6-42023f5607c2", "embedding": null, "metadata": {"page number": "113 - 135"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "955ddc19-f5ce-411b-b542-1a22027b6054", "node_type": "4", "metadata": {"page number": "113 - 135"}, "hash": "4e5a8b8dbb9310db94862aedc9b794c9d940cbf7b391dc117f873999721fd335", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9924e15b-0480-4bdf-adaf-e3481237fcc6", "node_type": "1", "metadata": {"page number": "113 - 135"}, "hash": "c53545c80ce1e1370f0fc6a591356ed863daa1a97337b9ab2fcac2d6df1d1ab0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d2ee2a9-4e5d-46ec-a55f-7b509822f836", "node_type": "1", "metadata": {}, "hash": "55db0c8a1f2f3ba6a7ecac2c9a21d311b310a7b0af39530e7dbe9be82fe4b9d3", "class_name": "RelatedNodeInfo"}}, "text": "Causal effects can happen in two ways. They can either be direct (e.g., D \u2192 Y), or they can be mediated by a third variable (e.g., D \u2192 X \u2192 Y). When they are mediated by a third variable, we are captur- ing a sequence of events originating with D, which may or may not be important to you depending on the question you\u2019re asking.\nA DAG is meant to describe all causal relationships relevant to the effect of D on Y. What makes the DAG distinctive is both the explicit commitment to a causal effect pathway and the complete commit- ment to the lack of a causal pathway represented by missing arrows. In other words, a DAG will contain both arrows connecting variables and choices to exclude arrows. And the lack of an arrow necessarily means that you think there is no such relationship in the data\u2014this is one of the strongest beliefs you can hold. A complete DAG will have all direct causal effects among the variables in the graph as well as all common causes of any pair of variables in the graph.\nAt this point, you may be wondering where the DAG comes from. It\u2019s an excellent question. It may be the question. A DAG is supposed to be a theoretical representation of the state-of-the-art knowledge about the phenomena you\u2019re studying. It\u2019s what an expert would say is the thing itself, and that expertise comes from a variety of sources. Examples include economic theory, other scienti\ufb01c models, conversa- tions with experts, your own observations and experiences, literature reviews, as well as your own intuition and hypotheses.\nI have included this material in the book because I have found DAGs to be useful for understanding the critical role that prior knowl- edge plays in identifying causal effects. But there are other reasons too. One, I have found that DAGs are very helpful for communicating research designs and estimators if for no other reason than pictures speak a thousand words. This is, in my experience, especially true for instrumental variables, which have a very intuitive DAG representation. Two, through concepts such as the backdoor criterion and collider bias, a well-designed DAG can help you develop a credible research design for identifying the causal effects of some intervention. As a bonus, I also think a DAG provides a bridge between various empirical schools, such as the structural and reduced form groups. And \ufb01nally, DAGs drive home the point that assumptions are necessary for any\nand all identi\ufb01cation of causal effects, which economists have been hammering at for years [Wolpin, 2013].\nA simple DAG. Let\u2019s begin with a simple DAG to illustrate a few basic ideas. I will expand on it to build slightly more complex ones later.\nIn this DAG, we have three random variables: X, D, and Y. There is a direct path from D to Y, which represents a causal effect. That path is represented by D \u2192 Y. But there is also a second path from D to Y called the backdoor path. The backdoor path is D \u2190 X \u2192 Y. While the direct path is a causal effect, the backdoor path is not causal. Rather, it is a process that creates spurious correlations between D and Y that are driven solely by \ufb02uctuations in the X random variable.\nThe idea of the backdoor path is one of the most important things we can learn from the DAG. It is similar to the notion of omitted variable bias in that it represents a variable that determines the outcome and the treatment variable. Just as not controlling for a variable like that in a regression creates omitted variable bias, leaving a backdoor open creates bias. The backdoor path is D \u2190 X \u2192 Y. We therefore call X a confounder because it jointly determines D and Y, and so confounds our ability to discern the effect of D on Y in na\u00efve comparisons.\nThink of the backdoor path like this: Sometimes when D takes on different values, Y takes on different values because D causes Y. But sometimes D and Y take on different values because X takes on dif- ferent values, and that bit of the correlation between D and Y is purely spurious. The existence of two causal pathways is contained within the correlation between D and Y.\nLet\u2019s look at a second DAG, which is subtly different from the \ufb01rst. In the previous example, X was observed. We know it was observed because the direct edges from X to D and Y were solid lines. But sometimes there exists a confounder that is unobserved, and when there is, we represent its direct edges with dashed lines. Consider the following DAG:\nSame as before, U is a noncollider along the backdoor path from D to Y, but unlike before, U is unobserved to the researcher.", "start_char_idx": 3948, "end_char_idx": 8505, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8d2ee2a9-4e5d-46ec-a55f-7b509822f836": {"__data__": {"id_": "8d2ee2a9-4e5d-46ec-a55f-7b509822f836", "embedding": null, "metadata": {"page number": "113 - 135"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "955ddc19-f5ce-411b-b542-1a22027b6054", "node_type": "4", "metadata": {"page number": "113 - 135"}, "hash": "4e5a8b8dbb9310db94862aedc9b794c9d940cbf7b391dc117f873999721fd335", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e4561bb4-b35d-454e-81e6-42023f5607c2", "node_type": "1", "metadata": {"page number": "113 - 135"}, "hash": "0d1332a9e59c8afaeadbc7b8ea6f964f22a2ae6c4dd5bc4d97b9b47f4d8d78d5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a3f93cad-494e-4d42-a10a-033da91d5748", "node_type": "1", "metadata": {}, "hash": "bde36e4d2b30fdf9b7b798b0f0bce07607b79cac97e93147fb865887b0bbaf96", "class_name": "RelatedNodeInfo"}}, "text": "Think of the backdoor path like this: Sometimes when D takes on different values, Y takes on different values because D causes Y. But sometimes D and Y take on different values because X takes on dif- ferent values, and that bit of the correlation between D and Y is purely spurious. The existence of two causal pathways is contained within the correlation between D and Y.\nLet\u2019s look at a second DAG, which is subtly different from the \ufb01rst. In the previous example, X was observed. We know it was observed because the direct edges from X to D and Y were solid lines. But sometimes there exists a confounder that is unobserved, and when there is, we represent its direct edges with dashed lines. Consider the following DAG:\nSame as before, U is a noncollider along the backdoor path from D to Y, but unlike before, U is unobserved to the researcher. It exists, but it may simply be missing from the data set. In this situation, there are two pathways from D to Y. There\u2019s the direct pathway, D \u2192 Y, which is the causal effect, and there\u2019s the backdoor pathway, D \u2190 U \u2192 Y. And since U is unobserved, that backdoor pathway is open.\nLet\u2019s now move to another example, one that is slightly more realis- tic. A classical question in labor economics is whether college educa- tion increases earnings. According to the Becker human capital model [Becker, 1994], education increases one\u2019s marginal product, and since workers are paid their marginal product in competitive markets, educa- tion also increases their earnings. But college education is not random; it is optimally chosen given an individual\u2019s subjective preferences and resource constraints. We represent that with the following DAG. As always, let D be the treatment (e.g., college education) and Y be the out- come of interest (e.g., earnings). Furthermore, let PE be parental edu- cation, I be family income, and B be unobserved background factors, such as genetics, family environment, and mental ability.\nThis DAG is telling a story. And one of the things I like about DAGs is that they invite everyone to listen to the story together. Here is my inter- pretation of the story being told. Each person has some background. It\u2019s not contained in most data sets, as it measures things like intelli- gence, contentiousness, mood stability, motivation, family dynamics, and other environmental factors\u2014hence, it is unobserved in the pic- ture. Those environmental factors are likely correlated between parent and child and therefore subsumed in the variable B.\nBackground causes a child\u2019s parent to choose her own optimal level of education, and that choice also causes the child to choose their level of education through a variety of channels. First, there is the shared background factors, B. Those background factors cause the child to choose a level of education, just as her parent had. Second, there\u2019s a direct effect, perhaps through simple modeling of achieve- ment or setting expectations, a kind of peer effect. And third, there\u2019s the effect that parental education has on family earnings, I, which in turn affects how much schooling the child receives. Family earnings may itself affect the child\u2019s future earnings through bequests and other transfers, as well as external investments in the child\u2019s productivity.\nThis is a simple story to tell, and the DAG tells it well, but I want to alert your attention to some subtle points contained in this DAG. The DAG is actually telling two stories. It is telling what is happening, and it is telling what is not happening. For instance, notice that B has no direct effect on the child\u2019s earnings except through its effect on schooling. Is this realistic, though? Economists have long maintained that unob- served ability both determines how much schooling a child gets and directly affects the child\u2019s future earnings, insofar as intelligence and motivation can in\ufb02uence careers. But in this DAG, there is no relation- ship between background and earnings, which is itself an assumption. And you are free to call foul on this assumption if you think that back- ground factors affect both schooling and the child\u2019s own productivity, which itself should affect wages. So what if you think that there should be an arrow from B to Y? Then you would draw one and rewrite all the backdoor paths between D and Y.\nNow that we have a DAG, what do we do? I like to list out all direct and indirect paths (i.e., backdoor paths) between D and Y. Once I have all those, I have a better sense of where my problems are. So:\n1.", "start_char_idx": 7655, "end_char_idx": 12179, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a3f93cad-494e-4d42-a10a-033da91d5748": {"__data__": {"id_": "a3f93cad-494e-4d42-a10a-033da91d5748", "embedding": null, "metadata": {"page number": "113 - 135"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "955ddc19-f5ce-411b-b542-1a22027b6054", "node_type": "4", "metadata": {"page number": "113 - 135"}, "hash": "4e5a8b8dbb9310db94862aedc9b794c9d940cbf7b391dc117f873999721fd335", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8d2ee2a9-4e5d-46ec-a55f-7b509822f836", "node_type": "1", "metadata": {"page number": "113 - 135"}, "hash": "0bc69eb09ba728c29e7f8cfd6d1bf1960b34498bfc745ef644c0bec47834b836", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9a42df33-55b6-41a0-936a-ab2f5d38e7ab", "node_type": "1", "metadata": {}, "hash": "cc9d40a4742ea93116a224e3565563700336762fcd4aca60e0a526b3cb5bb13f", "class_name": "RelatedNodeInfo"}}, "text": "Economists have long maintained that unob- served ability both determines how much schooling a child gets and directly affects the child\u2019s future earnings, insofar as intelligence and motivation can in\ufb02uence careers. But in this DAG, there is no relation- ship between background and earnings, which is itself an assumption. And you are free to call foul on this assumption if you think that back- ground factors affect both schooling and the child\u2019s own productivity, which itself should affect wages. So what if you think that there should be an arrow from B to Y? Then you would draw one and rewrite all the backdoor paths between D and Y.\nNow that we have a DAG, what do we do? I like to list out all direct and indirect paths (i.e., backdoor paths) between D and Y. Once I have all those, I have a better sense of where my problems are. So:\n1. D \u2192 Y (the causal effect of education on earnings) 2. D \u2190 I \u2192 Y (backdoor path 1) 3. D \u2190 PE \u2192 I \u2192 Y (backdoor path 2) 4. D \u2190 B \u2192 PE \u2192 I \u2192 Y (backdoor path 3)\nSo there are four paths between D and Y: one direct causal effect (which arguably is the important one if we want to know the return on\nschooling) and three backdoor paths. And since none of the variables along the backdoor paths is a collider, each of the backdoors paths is open. The problem, though, with open backdoor paths is that they create systematic and independent correlations between D and Y. Put a different way, the presence of open backdoor paths introduces bias when comparing educated and less-educated workers.\nColliding. But what is this collider? It\u2019s an unusual term, one you may have never seen before, so let\u2019s introduce it with another example. I\u2019m going to show you what a collider is graphically using a simple DAG, because it\u2019s an easy thing to see and a slightly more complicated phenomenon to explain. So let\u2019s work with a new DAG. Pay careful attention to the directions of the arrows, which have changed.\nAs before, let\u2019s list all paths from D to Y:\n1. D \u2192 Y (causal effect of D on Y) 2. D \u2192 X \u2190 Y (backdoor path 1)\nJust like last time, there are two ways to get from D to Y. You can get from D to Y using the direct (causal) path, D \u2192 Y. Or you can use the backdoor path, D \u2192 X \u2190 Y. But something is different about this backdoor path; do you see it? This time the X has two arrows point- ing to it, not away from it. When two variables cause a third variable along some path, we call that third variable a \u201ccollider.\u201d Put differently, X is a collider along this backdoor path because D and the causal effects of Y collide at X. But so what? What makes a collider so spe- cial? Colliders are special in part because when they appear along a backdoor path, that backdoor path is closed simply because of their presence. Colliders, when they are left alone, always close a speci\ufb01c backdoor path.\nBackdoor criterion. We care about open backdoor paths because they they create systematic, noncausal correlations between the causal\nvariable of interest and the outcome you are trying to study. In regres- sion terms, open backdoor paths introduce omitted variable bias, and for all you know, the bias is so bad that it \ufb02ips the sign entirely. Our goal, then, is to close these backdoor paths. And if we can close all of the otherwise open backdoor paths, then we can isolate the causal effect of D on Y using one of the research designs and identi\ufb01cation strategies discussed in this book. So how do we close a backdoor path?\nThere are two ways to close a backdoor path. First, if you have a confounder that has created an open backdoor path, then you can close that path by conditioning on the confounder. Conditioning requires holding the variable \ufb01xed using something like subclassi\ufb01- cation, matching, regression, or another method. It is equivalent to \u201ccontrolling for\u201d the variable in a regression. The second way to close a backdoor path is the appearance of a collider along that backdoor path. Since colliders always close backdoor paths, and conditioning on a collider always opens a backdoor path, choosing to ignore the colliders is part of your overall strategy to estimate the causal effect itself.", "start_char_idx": 11331, "end_char_idx": 15477, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9a42df33-55b6-41a0-936a-ab2f5d38e7ab": {"__data__": {"id_": "9a42df33-55b6-41a0-936a-ab2f5d38e7ab", "embedding": null, "metadata": {"page number": "113 - 135"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "955ddc19-f5ce-411b-b542-1a22027b6054", "node_type": "4", "metadata": {"page number": "113 - 135"}, "hash": "4e5a8b8dbb9310db94862aedc9b794c9d940cbf7b391dc117f873999721fd335", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a3f93cad-494e-4d42-a10a-033da91d5748", "node_type": "1", "metadata": {"page number": "113 - 135"}, "hash": "5e895851e429f3e06c8089e4cdd9652785c7f397697ec5232b95db512e8c54f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5b94589-66b6-4b37-b50d-7137fecda024", "node_type": "1", "metadata": {}, "hash": "020223465b6032db52b5269f6dfed649c37b41dafd9f1f4baa825079462809d3", "class_name": "RelatedNodeInfo"}}, "text": "So how do we close a backdoor path?\nThere are two ways to close a backdoor path. First, if you have a confounder that has created an open backdoor path, then you can close that path by conditioning on the confounder. Conditioning requires holding the variable \ufb01xed using something like subclassi\ufb01- cation, matching, regression, or another method. It is equivalent to \u201ccontrolling for\u201d the variable in a regression. The second way to close a backdoor path is the appearance of a collider along that backdoor path. Since colliders always close backdoor paths, and conditioning on a collider always opens a backdoor path, choosing to ignore the colliders is part of your overall strategy to estimate the causal effect itself. By not conditioning on a collider, you will have closed that back- door path and that takes you closer to your larger ambition to isolate some causal effect.\nWhen all backdoor paths have been closed, we say that you have come up with a research design that satis\ufb01es the backdoor criterion. And if you have satis\ufb01ed the backdoor criterion, then you have in effect isolated some causal effect. But let\u2019s formalize this: a set of variables X satis\ufb01es the backdoor criterion in a DAG if and only if X blocks every path between confounders that contain an arrow from D to Y. Let\u2019s review our original DAG involving parental education, background and earnings.\nThe minimally su\ufb03cient conditioning strategy necessary to achieve the backdoor criterion is the control for I, because I appeared\nas a noncollider along every backdoor path (see earlier). literally be no simpler than to run the following regression:\nBy simply conditioning on I, your estimated (cid:12)\u03b4 takes on a causal interpretation.4\nBut maybe in hearing this story, and studying it for yourself by reviewing the literature and the economic theory surrounding it, you are skeptical of this DAG. Maybe this DAG has really bothered you from the moment you saw me produce it because you are skeptical that B has no relationship to Y except through D or PE. That skepticism leads you to believe that there should be a direct connection from B to Y, not merely one mediated through own education.\nNote that including this new backdoor path has created a prob- lem because our conditioning strategy no longer satis\ufb01es the backdoor criterion. Even controlling for I, there still exist spurious correlations between D and Y due to the D \u2190 B \u2192 Y backdoor path. Without more information about the nature of B \u2192 Y and B \u2192 D, we cannot say much more about the partial correlation between D and Y. We just are not legally allowed to interpret (cid:12)\u03b4 from our regression as the causal effect of D on Y.\nMore examples of collider bias. The issue of conditioning on a collider is important, so how do we know if we have that problem or not? No data set comes with a \ufb02ag saying \u201ccollider\u201d and \u201cconfounder.\u201d Rather, the only way to know whether you have satis\ufb01ed the backdoor criterion is with a DAG, and a DAG requires a model. It requires in-depth knowl- edge of the data-generating process for the variables in your DAG,\n4 Subsequent chapters discuss other estimators, such as matching.\nbut it also requires ruling out pathways. And the only way to rule out pathways is through logic and models. There is no way to avoid it\u2014all empirical work requires theory to guide it. Otherwise, how do you know if you\u2019ve conditioned on a collider or a noncollider? Put differently, you cannot identify treatment effects without making assumptions.\nIn our earlier DAG with collider bias, we conditioned on some vari- able X that was a collider\u2014speci\ufb01cally, it was a descendent of D and Y. But that is just one example of a collider. Oftentimes, colliders enter into the system in very subtle ways. Let\u2019s consider the following sce- nario: Again, let D and Y be child schooling and child future earnings. But this time we introduce three new variables\u2014U1, which is father\u2019s unobserved genetic ability; U2, which is mother\u2019s unobserved genetic ability; and I, which is joint family income. Assume that I is observed but that Ui is unobserved for both parents.\nNotice in this DAG that there are several backdoor paths from D to\nY. They are as follows:\nNotice, the \ufb01rst two are open-backdoor paths, and as such, they can- not be closed, because U1 andU 2 are not observed. But what if we controlled for I anyway?", "start_char_idx": 14755, "end_char_idx": 19121, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b5b94589-66b6-4b37-b50d-7137fecda024": {"__data__": {"id_": "b5b94589-66b6-4b37-b50d-7137fecda024", "embedding": null, "metadata": {"page number": "113 - 135"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "955ddc19-f5ce-411b-b542-1a22027b6054", "node_type": "4", "metadata": {"page number": "113 - 135"}, "hash": "4e5a8b8dbb9310db94862aedc9b794c9d940cbf7b391dc117f873999721fd335", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9a42df33-55b6-41a0-936a-ab2f5d38e7ab", "node_type": "1", "metadata": {"page number": "113 - 135"}, "hash": "444c38d9b48307bb80a81c7b54b856141545bad399c8d3d7977a619c6e89b7ce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1935f891-ec9c-4857-8676-c136c62a55bf", "node_type": "1", "metadata": {}, "hash": "c4685602895801dfe7ff91095d419d1a1adfde8e6381219286b6852bd57087ae", "class_name": "RelatedNodeInfo"}}, "text": "Oftentimes, colliders enter into the system in very subtle ways. Let\u2019s consider the following sce- nario: Again, let D and Y be child schooling and child future earnings. But this time we introduce three new variables\u2014U1, which is father\u2019s unobserved genetic ability; U2, which is mother\u2019s unobserved genetic ability; and I, which is joint family income. Assume that I is observed but that Ui is unobserved for both parents.\nNotice in this DAG that there are several backdoor paths from D to\nY. They are as follows:\nNotice, the \ufb01rst two are open-backdoor paths, and as such, they can- not be closed, because U1 andU 2 are not observed. But what if we controlled for I anyway? Controlling for I only makes matters worse, because it opens the third and fourth backdoor paths, as I was a col- lider along both of them. It does not appear that any conditioning strategy could meet the backdoor criterion in this DAG. And any strat- egy controlling for I would actually make matters worse. Collider bias\nis a di\ufb03cult concept to understand at \ufb01rst, so I\u2019ve included a couple of examples to help you sort through it.\nDiscrimination and collider bias. Let\u2019s examine a real-world example around the problem of gender discrimination in labor-markets. It is common to hear that once occupation or other characteristics of a job are conditioned on, the wage disparity between genders disappears or gets smaller. For instance, critics once claimed that Google systemat- ically underpaid its female employees. But Google responded that its data showed that when you take \u201clocation, tenure, job role, level and performance\u201d into consideration, women\u2019s pay is basically identical to that of men. In other words, controlling for characteristics of the job, women received the same pay.\nBut what if one of the ways gender discrimination creates gender disparities in earnings is through occupational sorting? If discrimi- nation happens via the occupational match, then na\u00efve contrasts of wages by gender controlling for occupation characteristics will likely understate the presence of discrimination in the marketplace. Let me illustrate this with a DAG based on a simple occupational sorting model with unobserved heterogeneity.\nNotice that there is in fact no effect of female gender on earn- ings; women are assumed to have productivity identical to that of men. Thus, if we could control for discrimination, we\u2019d get a coe\ufb03cient of zero as in this example because women are, initially, just as productive as men.5\n5 Productivity could diverge, though,\nif women systematically sort into lower-\nquality occupations in which human capital accumulates over time at a lower rate.\nBut in this example, we aren\u2019t interested in estimating the effect of being female on earnings; we are interested in estimating the effect of discrimination itself. Now you can see several noticeable paths between discrimination and earnings. They are as follows:\nThe \ufb01rst path is not a backdoor path; rather, it is a path whereby discrimination is mediated by occupation before discrimination has an effect on earnings. This would imply that women are discriminated against, which in turn affects which jobs they hold, and as a result of holding marginally worse jobs, women are paid less. The second path relates to that channel but is slightly more complicated. In this path, unobserved ability affects both which jobs people get and their earnings.\nSo let\u2019s say we regress Y onto D, our discrimination variable. This yields the total effect of discrimination as the weighted sum of both the direct effect of discrimination on earnings and the mediated effect of discrimination on earnings through occupational sorting. But say that we want to control for occupation because we want to compare men and women in similar jobs. Well, controlling for occupation in the regression closes down the mediation channel, but it then opens up the second channel. Why? Because D \u2192 O \u2190 A \u2192 Y has a collider O. So when we control for occupation, we open up this second path. It had been closed because colliders close backdoor paths, but since we conditioned on it, we actually opened it instead. This is the rea- son we cannot merely control for occupation. Such a control ironically introduces new patterns of bias.6\n6 Angrist and Pischke [2009] talk about this problem in a different way using lan-\nguage called \u201cbad controls.\u201d Bad controls are not merely conditioning on outcomes.", "start_char_idx": 18446, "end_char_idx": 22882, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1935f891-ec9c-4857-8676-c136c62a55bf": {"__data__": {"id_": "1935f891-ec9c-4857-8676-c136c62a55bf", "embedding": null, "metadata": {"page number": "113 - 135"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "955ddc19-f5ce-411b-b542-1a22027b6054", "node_type": "4", "metadata": {"page number": "113 - 135"}, "hash": "4e5a8b8dbb9310db94862aedc9b794c9d940cbf7b391dc117f873999721fd335", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b5b94589-66b6-4b37-b50d-7137fecda024", "node_type": "1", "metadata": {"page number": "113 - 135"}, "hash": "890d3d41b26e6d6107888589ba6835c54fc7efd231f0c6b24693c547f0bcb7b1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "788f5d9b-7c3f-4118-9b18-299ff807945c", "node_type": "1", "metadata": {}, "hash": "4c86b52baef129ffde78d65356d0b22e542321f6a533e3791fda21421f414075", "class_name": "RelatedNodeInfo"}}, "text": "But say that we want to control for occupation because we want to compare men and women in similar jobs. Well, controlling for occupation in the regression closes down the mediation channel, but it then opens up the second channel. Why? Because D \u2192 O \u2190 A \u2192 Y has a collider O. So when we control for occupation, we open up this second path. It had been closed because colliders close backdoor paths, but since we conditioned on it, we actually opened it instead. This is the rea- son we cannot merely control for occupation. Such a control ironically introduces new patterns of bias.6\n6 Angrist and Pischke [2009] talk about this problem in a different way using lan-\nguage called \u201cbad controls.\u201d Bad controls are not merely conditioning on outcomes.\nRather, they are any situation in which the outcome had been a collider linking the treatment to the outcome of interest, like D \u2192 O \u2190 A \u2192 Y.\nWhat is needed is to control for occupation and ability, but since\nability is unobserved, we cannot do that, and therefore we do not pos-\nsess an identi\ufb01cation strategy that satis\ufb01es the backdoor criterion. Let\u2019s now look at code to illustrate this DAG.7\n1 clear all 2 set obs 10000 3 4 * Half of the population is female. 5 generate female = runiform()>=0.5 6 7 * Innate ability is independent of gender. 8 generate ability = rnormal() 9 10 * All women experience discrimination. 11 generate discrimination = female 12 13 * Data generating processes 14 generate occupation = (1) + (2)*ability + (0)*female + (-2)*discrimination +\n15 generate wage = (1) + (-1)*discrimination + (1)*occupation + 2*ability + rnormal() 16 17 * Regressions 18 regress wage female 19 regress wage female occupation 20 regress wage female occupation ability 21 22\n7 Erin Hengel is a professor of economics at the University of Liverpool. She and I\nwere talking about this on Twitter one day, and she and I wrote down the code describing\nthis problem. Her code was better, so I asked if I could reproduce it here, and she said\nyes. Erin\u2019s work partly focuses on gender discrimination. You can see some of that work on her website at http://www.erinhengel.com.\nfemale = ifelse(runif(10000)>=0.5,1,0), ability = rnorm(10000), discrimination = female, occupation = 1 + 2*ability + 0*female - 2*discrimination + rnorm(10000), wage = 1 - 1*discrimination + 1*occupation + 2*ability + rnorm(10000)\nThis simulation hard-codes the data-generating process repre- sented by the previous DAG. Notice that ability is a random draw from the standard normal distribution. Therefore it is independent of female preferences. And then we have our last two generated vari- ables: the heterogeneous occupations and their corresponding wages. Occupations are increasing in unobserved ability but decreasing in discrimination. Wages are decreasing in discrimination but increas- ing in higher-quality jobs and higher ability. Thus, we know that dis- crimination exists in this simulation because we are hard-coding it that way with the negative coe\ufb03cients both the occupation and wage processes.\nThe regression coe\ufb03cients from the three regressions at the end of the code are presented in Table 9. First note that when we simply regress wages onto gender, we get a large negative effect, which is the combination of the direct effect of discrimination on earnings and the\nTable 9. Regressions illustrating confounding bias with simulated gender disparity.\nBiased unconditional\nBiased biased\nUnbiased conditional\nindirect effect via occupation. But if we run the regression that Google and others recommend wherein we control for occupation, the sign on gender changes. It becomes positive! We know this is wrong because we hard-coded the effect of gender to be \u22121! The problem is that occu- pation is a collider. It is caused by ability and discrimination. If we control for occupation, we open up a backdoor path between discrim- ination and earnings that is spurious and so strong that it perverts the entire relationship. So only when we control for occupation and ability can we isolate the direct causal effect of gender on wages.\nSample selection and collider bias. Bad controls are not the only kind of collider bias to be afraid of, though.", "start_char_idx": 22132, "end_char_idx": 26329, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "788f5d9b-7c3f-4118-9b18-299ff807945c": {"__data__": {"id_": "788f5d9b-7c3f-4118-9b18-299ff807945c", "embedding": null, "metadata": {"page number": "113 - 135"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "955ddc19-f5ce-411b-b542-1a22027b6054", "node_type": "4", "metadata": {"page number": "113 - 135"}, "hash": "4e5a8b8dbb9310db94862aedc9b794c9d940cbf7b391dc117f873999721fd335", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1935f891-ec9c-4857-8676-c136c62a55bf", "node_type": "1", "metadata": {"page number": "113 - 135"}, "hash": "073abd621047316ace80755692f1e3c79b7f1766bd258a32dc7ae1c007a19a97", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5b4862c9-e4e7-4859-92a5-2eee96de8b97", "node_type": "1", "metadata": {}, "hash": "2f12beca5ee92d64075350e1715525a4ad5ca30c7479bffb917740a302787401", "class_name": "RelatedNodeInfo"}}, "text": "Regressions illustrating confounding bias with simulated gender disparity.\nBiased unconditional\nBiased biased\nUnbiased conditional\nindirect effect via occupation. But if we run the regression that Google and others recommend wherein we control for occupation, the sign on gender changes. It becomes positive! We know this is wrong because we hard-coded the effect of gender to be \u22121! The problem is that occu- pation is a collider. It is caused by ability and discrimination. If we control for occupation, we open up a backdoor path between discrim- ination and earnings that is spurious and so strong that it perverts the entire relationship. So only when we control for occupation and ability can we isolate the direct causal effect of gender on wages.\nSample selection and collider bias. Bad controls are not the only kind of collider bias to be afraid of, though. Collider bias can also be baked directly into the sample if the sample itself was a collider. That\u2019s no doubt a strange concept to imagine, so I have a funny illustration to clarify what I mean.\nA 2009 CNN blog post reported that Megan Fox, who starred in the movie Transformers, was voted the worst and most attractive actress of 2009 in some survey about movie stars [Piazza, 2009]. The implication could be taken to be that talent and beauty are negatively\ncorrelated. But are they? And why might they be? What if they are inde- pendent of each other in reality but negatively correlated in a sample of movie stars because of collider bias? Is that even possible?8\nTo illustrate, we will generate some data based on the follow-\ning DAG:\nLet\u2019s illustrate this with a simple program.\n1 clear all 2 set seed 3444 3 4 * 2500 independent draws from standard normal distribution 5 set obs 2500 6 generate beauty=rnormal() 7 generate talent=rnormal() 8 9 * Creating the collider variable (star) 10 gen score=(beauty+talent) 11 egen c85=pctile(score), p(85) 12 gen star=(score>=c85) 13 label variable star \"Movie star\" 14 15 * Conditioning on the top 15\\% 16 twoway (scatter beauty talent, mcolor(black) msize(small) msymbol(smx)),\n8 I wish I had thought of this example, but alas the sociologist Gabriel Rossman\ngets full credit.\nlm(beauty ~ talent, .) %>% ggplot(aes(x = talent, y = beauty)) + geom_point(size = 0.5, shape=23) + xlim(-4, 4) + ylim(-4, 4)\nfilter(star == 1) %>% lm(beauty ~ talent, .) %>% ggplot(aes(x = talent, y = beauty)) + geom_point(size = 0.5, shape=23) + xlim(-4, 4) + ylim(-4, 4)\nfilter(star == 0) %>% lm(beauty ~ talent, .) %>% ggplot(aes(x = talent, y = beauty)) + geom_point(size = 0.5, shape=23) + xlim(-4, 4) + ylim(-4, 4)\nFigure 12 shows the output from this simulation. The bottom left panel shows the scatter plot between talent and beauty. Notice that the two variables are independent, random draws from the standard nor- mal distribution, creating an oblong data cloud. But because \u201cmovie star\u201d is in the top 85th percentile of the distribution of a linear com- bination of talent and beauty, the sample consists of people whose combined score is in the top right portion of the joint distribution. This frontier has a negative slope and is in the upper right portion of the data\nFigure 12. Aspiring actors and actresses.\nNote: Top left: Non-star sample scatter plot of beauty (vertical axis) and tal- ent (horizontal axis). Top right: Star sample scatter plot of beauty and talent.\nBottom left: Entire (stars and non-stars combined) sample scatter plot of\ncloud, creating a negative correlation between the observations in the movie-star sample. Likewise, the collider bias has created a negative correlation between talent and beauty in the non-movie-star sample as well. Yet we know that there is in fact no relationship between the two variables. This kind of sample selection creates spurious correlations. A random sample of the full population would be su\ufb03cient to show that there is no relationship between the two variables, but splitting the sample into movie stars only, we introduce spurious correlations between the two variables of interest.\nCollider bias and police use of force.", "start_char_idx": 25462, "end_char_idx": 29555, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5b4862c9-e4e7-4859-92a5-2eee96de8b97": {"__data__": {"id_": "5b4862c9-e4e7-4859-92a5-2eee96de8b97", "embedding": null, "metadata": {"page number": "113 - 135"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "955ddc19-f5ce-411b-b542-1a22027b6054", "node_type": "4", "metadata": {"page number": "113 - 135"}, "hash": "4e5a8b8dbb9310db94862aedc9b794c9d940cbf7b391dc117f873999721fd335", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "788f5d9b-7c3f-4118-9b18-299ff807945c", "node_type": "1", "metadata": {"page number": "113 - 135"}, "hash": "52674f3949c8017557832d9b8709f0c9847923721f39481a8f6884113fd63777", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "106a1df9-dcf7-43cd-ae1a-386da9e48ac4", "node_type": "1", "metadata": {}, "hash": "d39898c8599b154ce4fd5b95b695e65bc6391c745e188a41daa74b10fc14c5fe", "class_name": "RelatedNodeInfo"}}, "text": "Aspiring actors and actresses.\nNote: Top left: Non-star sample scatter plot of beauty (vertical axis) and tal- ent (horizontal axis). Top right: Star sample scatter plot of beauty and talent.\nBottom left: Entire (stars and non-stars combined) sample scatter plot of\ncloud, creating a negative correlation between the observations in the movie-star sample. Likewise, the collider bias has created a negative correlation between talent and beauty in the non-movie-star sample as well. Yet we know that there is in fact no relationship between the two variables. This kind of sample selection creates spurious correlations. A random sample of the full population would be su\ufb03cient to show that there is no relationship between the two variables, but splitting the sample into movie stars only, we introduce spurious correlations between the two variables of interest.\nCollider bias and police use of force. We\u2019ve known about the prob- lems of nonrandom sample selection for decades [Heckman, 1979]. But DAGs may still be useful for helping spot what might be otherwise subtle cases of conditioning on colliders [Elwert and Winship, 2014].\nAnd given the ubiquitous rise in researcher access to large adminis- trative databases, it\u2019s also likely that some sort of theoretically guided reasoning will be needed to help us determine whether the databases we have are themselves rife with collider bias. A contemporary debate could help illustrate what I mean.\nPublic concern about police o\ufb03cers systematically discriminat- ing against minorities has reached a breaking point and led to the emergence of the Black Lives Matter movement. \u201cVigilante justice\u201d episodes such as George Zimmerman\u2019s killing of teenage Trayvon Mar- tin, as well as police killings of Michael Brown, Eric Garner, and count- less others, served as catalysts to bring awareness to the perception that African Americans face enhanced risks for shootings. Fryer [2019] attempted to ascertain the degree to which there was racial bias in the use of force by police. This is perhaps one of the most important questions in policing as of this book\u2019s publication.\nThere are several critical empirical challenges in studying racial biases in police use of force, though. The main problem is that all data on police-citizen interactions are conditional on an interaction having already occurred. The data themselves were generated as a function of earlier police-citizen interactions. In this sense, we can say that the data itself are endogenous. Fryer [2019] collected several databases that he hoped would help us better understand these patterns. Two were public-use data sets\u2014the New York City Stop and Frisk database and the Police-Public Contact Survey. The former was from the New York Police Department and contained data on police stops and ques- tioning of pedestrians; if the police wanted to, they could frisk them for weapons or contraband. The latter was a survey of civilians describing interactions with the police, including the use of force.\nBut two of the data sets were administrative. The \ufb01rst was a com- pilation of event summaries from more than a dozen large cities and large counties across the United States from all incidents in which an o\ufb03cer discharged a weapon at a civilian. The second was a ran- dom sample of police-civilian interactions from the Houston Police Department. The accumulation of these databases was by all evidence a gigantic empirical task. For instance, Fryer [2019] notes that the Houston data was based on arrest narratives that ranged from two to one hundred pages in length. From these arrest narratives, a team of\nresearchers collected almost three hundred variables relevant to the police use of force on the incident. This is the world in which we now live, though. Administrative databases can be accessed more eas- ily than ever, and they are helping break open the black box of many opaque social processes.\nA few facts are important to note. First, using the stop-and-frisk data, Fryer \ufb01nds that blacks and Hispanics were more than 50 per- cent more likely to have an interaction with the police in the raw data. The racial difference survives conditioning on 125 baseline character- istics, encounter characteristics, civilian behavior, precinct, and year \ufb01xed effects. In his full model, blacks are 21 percent more likely than whites to be involved in an interaction with police in which a weapon is drawn (which is statistically signi\ufb01cant). These racial differences show up in the Police-Public Contact Survey as well, only here the racial dif- ferences are considerably larger. So the \ufb01rst thing to note is that the actual stop itself appears to be larger for minorities, which I will come back to momentarily.\nThings become surprising when Fryer moves to his rich admin- istrative data sources.", "start_char_idx": 28652, "end_char_idx": 33471, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "106a1df9-dcf7-43cd-ae1a-386da9e48ac4": {"__data__": {"id_": "106a1df9-dcf7-43cd-ae1a-386da9e48ac4", "embedding": null, "metadata": {"page number": "113 - 135"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "955ddc19-f5ce-411b-b542-1a22027b6054", "node_type": "4", "metadata": {"page number": "113 - 135"}, "hash": "4e5a8b8dbb9310db94862aedc9b794c9d940cbf7b391dc117f873999721fd335", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5b4862c9-e4e7-4859-92a5-2eee96de8b97", "node_type": "1", "metadata": {"page number": "113 - 135"}, "hash": "d18d949b0d4f944d6839d0167f91c745267a043873d71634ec0924a26e47275d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e24931e6-26d5-4850-bfc2-b49bb797df92", "node_type": "1", "metadata": {}, "hash": "52fa863a0395bdd1ac8aac2c01cbf537b592c81dbe94d228d865bd658d820cf3", "class_name": "RelatedNodeInfo"}}, "text": "A few facts are important to note. First, using the stop-and-frisk data, Fryer \ufb01nds that blacks and Hispanics were more than 50 per- cent more likely to have an interaction with the police in the raw data. The racial difference survives conditioning on 125 baseline character- istics, encounter characteristics, civilian behavior, precinct, and year \ufb01xed effects. In his full model, blacks are 21 percent more likely than whites to be involved in an interaction with police in which a weapon is drawn (which is statistically signi\ufb01cant). These racial differences show up in the Police-Public Contact Survey as well, only here the racial dif- ferences are considerably larger. So the \ufb01rst thing to note is that the actual stop itself appears to be larger for minorities, which I will come back to momentarily.\nThings become surprising when Fryer moves to his rich admin- istrative data sources. He \ufb01nds that conditional on a police inter- action, there are no racial differences in o\ufb03cer-involved shootings. In fact, controlling for suspect demographics, o\ufb03cer demographics, encounter characteristics, suspect weapon, and year \ufb01xed effects, blacks are 27 percent less likely to be shot at by police than are non- black non-Hispanics. The coe\ufb03cient is not signi\ufb01cant, and it shows up across alternative speci\ufb01cations and cuts of the data. Fryer is sim- ply unable with these data to \ufb01nd evidence for racial discrimination in o\ufb03cer-involved shootings.\nOne of the main strengths of Fryer\u2019s study are the shoe leather he used to accumulate the needed data sources. Without data, one can- not study the question of whether police shoot minorities more than they shoot whites. And the extensive coding of information from the narratives is also a strength, for it afforded Fryer the ability to con- trol for observable confounders. But the study is not without issues that could cause a skeptic to take issue. Perhaps the police depart- ments most willing to cooperate with a study of this kind are the ones with the least racial bias, for instance. In other words, maybe these\nare not the departments with the racial bias to begin with.9 Or per- haps a more sinister explanation exists, such as records being unre- liable because administrators scrub out the data on racially motivated shootings before handing them over to Fryer altogether.\nBut I would like to discuss a more innocent possibility, one that requires no conspiracy theories and yet is so basic a problem that it is in fact more worrisome. Perhaps the administrative datasource is endogenous because of conditioning on a collider. If so, then the administrative data itself may have the racial bias baked into it from the start. Let me explain with a DAG.\nFryer showed that minorities were more likely to be stopped using both the stop-and-frisk data and the Police-Public Contact Survey. So we know already that the D \u2192 M pathway exists. In fact, it was a very robust correlation across multiple studies. Minorities are more likely to have an encounter with the police. Fryer\u2019s study introduces extensive controls about the nature of the interaction, time of day, and hundreds of factors that I\u2019ve captured with X. Controlling for X allows Fryer to shut this backdoor path.\nBut notice M\u2014the stop itself. All the administrative data is con- ditional on a stop. Fryer [2019] acknowledges this from the outset: \u201cUnless otherwise noted, all results are conditional on an interaction. Understanding potential selection into police data sets due to bias in who police interacts with is a di\ufb03cult endeavor\u201d (3). Yet what this DAG\n9 I am not sympathetic to this claim. The administrative data comes from large\ncounties racial bias has been reported.\nshows is that if police stop people who they believe are suspicious and use force against people they \ufb01nd suspicious, then conditioning on the stop is equivalent to conditioning on a collider. It opens up the D \u2192 M \u2190 U \u2192 Y mediated path, which introduces spurious patterns into the data that, depending on the signs of these causal associations, may distort any true relationship between police and racial differences in shootings.\nDean Knox, Will Lowe, and Jonathan Mummolo are a talented team of political scientists who study policing, among other things. They produced a study that revisited Fryer\u2019s question and in my opinion both yielded new clues as to the role of racial bias in police use of force and the challenges of using administrative data sources to do so. I consider Knox et al. [2020] one of the more methodologically helpful studies for understanding this problem and attempting to solve it.", "start_char_idx": 32578, "end_char_idx": 37190, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e24931e6-26d5-4850-bfc2-b49bb797df92": {"__data__": {"id_": "e24931e6-26d5-4850-bfc2-b49bb797df92", "embedding": null, "metadata": {"page number": "113 - 135"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "955ddc19-f5ce-411b-b542-1a22027b6054", "node_type": "4", "metadata": {"page number": "113 - 135"}, "hash": "4e5a8b8dbb9310db94862aedc9b794c9d940cbf7b391dc117f873999721fd335", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "106a1df9-dcf7-43cd-ae1a-386da9e48ac4", "node_type": "1", "metadata": {"page number": "113 - 135"}, "hash": "51a5fd4547f797090b3e4c459f75b5b230b321271db0508f9d9366b4baf5b07a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "90421679-c6e1-45dc-97a2-4375725556a2", "node_type": "1", "metadata": {}, "hash": "1ef80e55daf4ff21194903f42f9ef694cef9b049d141a39841effc4280dcc5ee", "class_name": "RelatedNodeInfo"}}, "text": "The administrative data comes from large\ncounties racial bias has been reported.\nshows is that if police stop people who they believe are suspicious and use force against people they \ufb01nd suspicious, then conditioning on the stop is equivalent to conditioning on a collider. It opens up the D \u2192 M \u2190 U \u2192 Y mediated path, which introduces spurious patterns into the data that, depending on the signs of these causal associations, may distort any true relationship between police and racial differences in shootings.\nDean Knox, Will Lowe, and Jonathan Mummolo are a talented team of political scientists who study policing, among other things. They produced a study that revisited Fryer\u2019s question and in my opinion both yielded new clues as to the role of racial bias in police use of force and the challenges of using administrative data sources to do so. I consider Knox et al. [2020] one of the more methodologically helpful studies for understanding this problem and attempting to solve it. The study should be widely read by every applied researcher whose day job involves working with proprietary administrative data sets, because this DAG may in fact be a more general problem. After all, admin- istrative data sources are already select samples, and depending on the study question, they may constitute a collider problem of the sort described in this DAG. The authors develop a bias correction procedure that places bounds on the severity of the selection problems. When using this bounding approach, they \ufb01nd that even lower-bound esti- mates of the incidence of police violence against civilians is as much as \ufb01ve times higher than a traditional approach that ignores the sample selection problem altogether.\nIt is incorrect to say that sample selection problems were unknown without DAGs. We\u2019ve known about them and have had some limited solutions to them since at least Heckman [1979]. What I have tried to show here is more general. An atheoretical approach to empiricism will simply fail. Not even \u201cbig data\u201d will solve it. Causal inference is not solved with more data, as I argue in the next chapter. Causal infer- ence requires knowledge about the behavioral processes that struc- ture equilibria in the world. Without them, one cannot hope to devise a credible identi\ufb01cation strategy. Not even data is a substitute for deep institutional knowledge about the phenomenon you\u2019re studying. That, strangely enough, even includes the behavioral processes that gen- erated the samples you\u2019re using in the \ufb01rst place. You simply must\ntake seriously the behavioral theory that is behind the phenomenon you\u2019re studying if you hope to obtain believable estimates of causal effects. And DAGs are a helpful tool for wrapping your head around and expressing those problems.\nConclusion. In conclusion, DAGs are powerful tools.10 They are helpful at both clarifying the relationships between variables and guiding you in a research design that has a shot at identifying a causal effect. The two concepts we discussed in this chapter\u2014the backdoor criterion and collider bias\u2014are but two things I wanted to bring to your attention. And since DAGs are themselves based on counterfactual forms of reason- ing, they \ufb01t well with the potential outcomes model that I discuss in the next chapter.\n10 There is far more to DAGs than I have covered here. If you are interested in\nlearning more about them, then I encourage you to carefully read Pearl [2009], which is", "start_char_idx": 36199, "end_char_idx": 39655, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "90421679-c6e1-45dc-97a2-4375725556a2": {"__data__": {"id_": "90421679-c6e1-45dc-97a2-4375725556a2", "embedding": null, "metadata": {"page number": "136 - 140", "chapter": "Potential Outcomes Causal Model"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a3e020a5-0f41-4b6c-b676-b0179dc36958", "node_type": "4", "metadata": {"page number": "136 - 140", "chapter": "Potential Outcomes Causal Model"}, "hash": "eb4a9dd26cb4cb1fa9f36469ee910ed3ac843355ac8d0964ff33a9d61309c2d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e24931e6-26d5-4850-bfc2-b49bb797df92", "node_type": "1", "metadata": {"page number": "113 - 135"}, "hash": "cd49b9a3b10e602a9e67a4038b4777ed2bee44e953a373e7a743c2519cf76ce8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fe066021-8699-4c1c-a112-8d73aa445280", "node_type": "1", "metadata": {}, "hash": "b29e623660f48fa46aee5ae4551aaf8699b21dbe6023c73e86f6f97a9bb0644a", "class_name": "RelatedNodeInfo"}}, "text": "Potential Outcomes Causal Model:\nIt\u2019s like the more money we come across, the more problems we see.\nPractical questions about causation have been a preoccupation of economists for several centuries. Adam Smith wrote about the causes of the wealth of nations [Smith, 2003]. Karl Marx was interested in the transition of society from capitalism to socialism [Needleman and Needleman, 1969]. In the twentieth century the Cowles Commission sought to better understand identifying causal parameters [Heckman and Vytlacil, 2007].1 Economists have been wrestling with both the big ideas around causality and the development of useful empirical tools from day one.\nWe can see the development of the modern concepts of causality in the writings of several philosophers. Hume [1993] described causa- tion as a sequence of temporal events in which, had the \ufb01rst event not occurred, subsequent ones would not either. For example, he said:\n1 This brief history will focus on the development of the potential outcomes model.\nSee Morgan [1991] for a more comprehensive history of econometric ideas.\nWe may de\ufb01ne a cause to be an object, followed by another, and where all the objects similar to the \ufb01rst are followed by objects sim- ilar to the second. Or in other words where, if the \ufb01rst object had not been, the second never had existed.\nMill [2010] devised \ufb01ve methods for inferring causation. Those methods were (1) the method of agreement, (2) the method of differ- ence, (3) the joint method, (4) the method of concomitant variation, and (5) the method of residues. The second method, the method of difference, is most similar to the idea of causation as a comparison among counterfactuals. For instance, he wrote:\nIf a person eats of a particular dish, and dies in consequence, that is, would not have died if he had not eaten it, people would be apt to say that eating of that dish was the source of his death. [399]\nStatistical inference. A major jump in our understanding of causation occurs coincident with the development of modern statistics. Probabil- ity theory and statistics revolutionized science in the nineteenth cen- tury, beginning with the \ufb01eld of astronomy. Giuseppe Piazzi, an early nineteenth-century astronomer, discovered the dwarf planet Ceres, located between Jupiter and Mars, in 1801. Piazzi observed it 24 times before it was lost again. Carl Friedrich Gauss proposed a method that could successfully predict Ceres\u2019s next location using data on its prior in location. His method minimized the sum of the squared errors; other words, the ordinary least squares method we discussed earlier. He discovered OLS at age 18 and published his derivation of OLS in 1809 at age 24 [Gauss, 1809].2 Other scientists who contributed to our understanding of OLS include Pierre-Simon LaPlace and Adrien-Marie Legendre.\nThe statistician G. Udny Yule made early use of regression anal- ysis in the social sciences. Yule [1899] was interested in the causes of poverty in England. Poor people depended on either poorhouses or the local authorities for \ufb01nancial support, and Yule wanted to know if\n2 Around age 20, I \ufb01nally beat Tomb Raider 2 on the Sony PlayStation. So yeah, I\ncan totally relate to Gauss\u2019s accomplishments at such a young age.\npublic assistance increased the number of paupers, which is a causal question. Yule used least squares regression to estimate the partial correlation between public assistance and poverty. His data was drawn from the English censuses of 1871 and 1881, and I have made his data available at my website for Stata or the Mixtape library for R users. Here\u2019s an example of the regression one might run using these data:\nLet\u2019s run this regression using the data.\n1 library(tidyverse) 2 library(haven) 3 4 read_data <- function(df) 5 { 6 7 8 9 10 } 11 12 yule <- read_data(\"yule.dta\") %>% 13 14 summary(yule)\ndf, sep = \"\")\nEach row in this data set is a particular location in England (e.g., Chelsea, Strand). So, since there are 32 rows, that means the data set contains 32 English locations. Each of the variables is expressed as\nTable 10. Estimated association between pauperism growth rates and public\nan annual growth rate.", "start_char_idx": 0, "end_char_idx": 4166, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fe066021-8699-4c1c-a112-8d73aa445280": {"__data__": {"id_": "fe066021-8699-4c1c-a112-8d73aa445280", "embedding": null, "metadata": {"page number": "136 - 140", "chapter": "Potential Outcomes Causal Model"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a3e020a5-0f41-4b6c-b676-b0179dc36958", "node_type": "4", "metadata": {"page number": "136 - 140", "chapter": "Potential Outcomes Causal Model"}, "hash": "eb4a9dd26cb4cb1fa9f36469ee910ed3ac843355ac8d0964ff33a9d61309c2d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "90421679-c6e1-45dc-97a2-4375725556a2", "node_type": "1", "metadata": {"page number": "136 - 140", "chapter": "Potential Outcomes Causal Model"}, "hash": "55f235de4ca4488fbf158abf0e44809d23248a04cd34831fe8f7ca67631d1b21", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "edfaf9aa-0b12-4c71-9aab-8764f8f98138", "node_type": "1", "metadata": {}, "hash": "a4c8b31c28c67b28deb4a2075ad59d4ba4c4d75c6226a947e3cea5e77ff1d151", "class_name": "RelatedNodeInfo"}}, "text": "Here\u2019s an example of the regression one might run using these data:\nLet\u2019s run this regression using the data.\n1 library(tidyverse) 2 library(haven) 3 4 read_data <- function(df) 5 { 6 7 8 9 10 } 11 12 yule <- read_data(\"yule.dta\") %>% 13 14 summary(yule)\ndf, sep = \"\")\nEach row in this data set is a particular location in England (e.g., Chelsea, Strand). So, since there are 32 rows, that means the data set contains 32 English locations. Each of the variables is expressed as\nTable 10. Estimated association between pauperism growth rates and public\nan annual growth rate. As a result, each regression coe\ufb03cient has elasticity interpretations, with one caveat\u2014technically, as I explained at the beginning of the book, elasticities are actually causal objects, not simply correlations between two variables. And it\u2019s unlikely that the conditions needed to interpret these as causal relationships are met in Yule\u2019s data. Nevertheless, let\u2019s run the regression and look at the results, which I report in Table 10.\nIn words, a 10-percentage-point change in the out-relief growth rate is associated with a 7.5-percentage-point increase in the pauperism growth rate, or an elasticity of 0.75. Yule used his regression to crank out the correlation between out-relief and pauperism, from which he concluded that public assistance increased pauper growth rates.\nBut what might be wrong with this reasoning? How convinced are you that all backdoor paths between pauperism and out-relief are blocked once you control for two covariates in a cross-sectional database for all of England? Could there be unobserved determinants of both poverty and public assistance? After all, he does not control for any economic factors, which surely affect both poverty and the amount of resources allocated to out-relief. Likewise, he may have the causality backwards\u2014perhaps increased poverty causes communities to increase relief, and not merely the other way around. The earliest adopters of some new methodology or technique are often the ones who get the most criticism, despite being pioneers of the methods themselves. It\u2019s trivially easy to beat up on a researcher from one hun- dred years ago, working at a time when the alternative to regression\nwas ideological make-believe. Plus he isn\u2019t here to reply. I merely want to note that the na\u00efve use of regression to estimate correlations as a way of making causal claims that inform important policy questions has been the norm for a very long time, and it likely isn\u2019t going away any time soon.", "start_char_idx": 3592, "end_char_idx": 6120, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "edfaf9aa-0b12-4c71-9aab-8764f8f98138": {"__data__": {"id_": "edfaf9aa-0b12-4c71-9aab-8764f8f98138", "embedding": null, "metadata": {"page number": "140 - 166", "chapter": "Potential Outcomes Causal Model"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "39a1e882-b5dd-4076-870d-98cfa39f2345", "node_type": "4", "metadata": {"page number": "140 - 166", "chapter": "Potential Outcomes Causal Model"}, "hash": "e38f1081f1286b325b814f08b024ef5550bc90d055dc94227410569ae7180129", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fe066021-8699-4c1c-a112-8d73aa445280", "node_type": "1", "metadata": {"page number": "136 - 140", "chapter": "Potential Outcomes Causal Model"}, "hash": "3c39fc6cc4cf5374241cd869d35868cf308a3c669137f875d855f1c5baf227b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "56408816-91e0-4bf6-a309-dcbf6ee05392", "node_type": "1", "metadata": {}, "hash": "3fc6523f95c3c350c558b20dd50d7f5f9ea086ba9481d3eff9a1d5dc1c3e962b", "class_name": "RelatedNodeInfo"}}, "text": "Physical Randomization:\nThe notion of physical randomization as the foundation of causal inference was in the air in the nineteenth and twentieth centuries, but it was not until Fisher [1935] that it crystallized. The \ufb01rst historically recognized randomized experiment had occurred \ufb01fty years earlier in psychology [Peirce and Jastrow, 1885]. But interestingly, in that exper- iment, the reason for randomization was not as the basis for causal inference. Rather, the researchers proposed randomization as a way of fooling subjects in their experiments. Peirce and Jastrow [1885] used several treatments, and they used physical randomization so that participants couldn\u2019t guess what would happen next. Unless I\u2019m mis- taken, recommending physical randomization of treatments to units as a basis for causal inference is based on Splawa-Neyman [1923] and Fisher [1925]. More speci\ufb01cally, Splawa-Neyman [1923] developed the powerful potential outcomes notation (which we will discuss soon), and while he proposed randomization, it was not taken to be literally necessary until Fisher [1925]. Fisher [1925] proposed the explicit use of randomization in experimental design for causal inference.3\nPhysical randomization was largely the domain of agricultural experiments until the mid-1950s, when it began to be used in medical trials. Among the \ufb01rst major randomized experiments in medicine\u2014in fact, ever attempted\u2014were the Salk polio vaccine \ufb01eld trials. In 1954, the Public Health Service set out to determine whether the Salk vac- cine prevented polio. Children in the study were assigned at random to receive the vaccine or a placebo.4 Also, the doctors making the\n3 For more on the transition from Splawa-Neyman [1923] to Fisher [1925], see Rubin\n4 In the placebo, children were injected with a saline solution.\nwas ideological make-believe. Plus he isn\u2019t here to reply. I merely want to note that the na\u00efve use of regression to estimate correlations as a way of making causal claims that inform important policy questions has been the norm for a very long time, and it likely isn\u2019t going away any time soon.\nThe notion of physical randomization as the foundation of causal inference was in the air in the nineteenth and twentieth centuries, but it was not until Fisher [1935] that it crystallized. The \ufb01rst historically recognized randomized experiment had occurred \ufb01fty years earlier in psychology [Peirce and Jastrow, 1885]. But interestingly, in that exper- iment, the reason for randomization was not as the basis for causal inference. Rather, the researchers proposed randomization as a way of fooling subjects in their experiments. Peirce and Jastrow [1885] used several treatments, and they used physical randomization so that participants couldn\u2019t guess what would happen next. Unless I\u2019m mis- taken, recommending physical randomization of treatments to units as a basis for causal inference is based on Splawa-Neyman [1923] and Fisher [1925]. More speci\ufb01cally, Splawa-Neyman [1923] developed the powerful potential outcomes notation (which we will discuss soon), and while he proposed randomization, it was not taken to be literally necessary until Fisher [1925]. Fisher [1925] proposed the explicit use of randomization in experimental design for causal inference.3\nPhysical randomization was largely the domain of agricultural experiments until the mid-1950s, when it began to be used in medical trials. Among the \ufb01rst major randomized experiments in medicine\u2014in fact, ever attempted\u2014were the Salk polio vaccine \ufb01eld trials. In 1954, the Public Health Service set out to determine whether the Salk vac- cine prevented polio. Children in the study were assigned at random to receive the vaccine or a placebo.4 Also, the doctors making the\n3 For more on the transition from Splawa-Neyman [1923] to Fisher [1925], see Rubin\n4 In the placebo, children were injected with a saline solution.\ndiagnoses of polio did not know whether the child had received the vac- cine or the placebo. The polio vaccine trial was called a double-blind, randomized controlled trial because neither the patient nor the admin- istrator of the vaccine knew whether the treatment was a placebo or a vaccine. It was necessary for the \ufb01eld trial to be very large because the rate at which polio occurred in the population was 50 per 100,000. The treatment group, which contained 200,745 individuals, saw 33 polio cases.", "start_char_idx": 0, "end_char_idx": 4402, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "56408816-91e0-4bf6-a309-dcbf6ee05392": {"__data__": {"id_": "56408816-91e0-4bf6-a309-dcbf6ee05392", "embedding": null, "metadata": {"page number": "140 - 166", "chapter": "Potential Outcomes Causal Model"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "39a1e882-b5dd-4076-870d-98cfa39f2345", "node_type": "4", "metadata": {"page number": "140 - 166", "chapter": "Potential Outcomes Causal Model"}, "hash": "e38f1081f1286b325b814f08b024ef5550bc90d055dc94227410569ae7180129", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "edfaf9aa-0b12-4c71-9aab-8764f8f98138", "node_type": "1", "metadata": {"page number": "140 - 166", "chapter": "Potential Outcomes Causal Model"}, "hash": "2d9f2df841560521da2d161bcfc3a2391a4e23c8a618a184f7b8d71dd3e07c12", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "165ae155-f6cb-4585-a40c-38f2851203b2", "node_type": "1", "metadata": {}, "hash": "a1a404ef488cf4f63eeeac1ed3d361e4064be1c5241857cd0f9059485b8d44bd", "class_name": "RelatedNodeInfo"}}, "text": "Children in the study were assigned at random to receive the vaccine or a placebo.4 Also, the doctors making the\n3 For more on the transition from Splawa-Neyman [1923] to Fisher [1925], see Rubin\n4 In the placebo, children were injected with a saline solution.\ndiagnoses of polio did not know whether the child had received the vac- cine or the placebo. The polio vaccine trial was called a double-blind, randomized controlled trial because neither the patient nor the admin- istrator of the vaccine knew whether the treatment was a placebo or a vaccine. It was necessary for the \ufb01eld trial to be very large because the rate at which polio occurred in the population was 50 per 100,000. The treatment group, which contained 200,745 individuals, saw 33 polio cases. The control group had 201,229 individuals and saw 115 cases. The probability of seeing such a big difference in rates of polio because of chance alone is about one in a billion. The only plausible explana- tion, it was argued, was that the polio vaccine caused a reduction in the risk of polio.\nA similar large-scale randomized experiment occurred in eco- nomics in the 1970s. Between 1971 and 1982, the RAND Corporation conducted a large-scale randomized experiment studying the causal effect of health-care insurance on health-care utilization. For the study, Rand recruited 7,700 individuals younger than age 65. The experiment was somewhat complicated, with multiple treatment arms. Partici- pants were randomly assigned to one of \ufb01ve health insurance plans: free care, three plans with varying levels of cost sharing, and an HMO plan. Participants with cost sharing made fewer physician visits and had fewer hospitalizations than those with free care. Other declines in health-care utilization, such as fewer dental visits, were also found among the cost-sharing treatment groups. Overall, participants in the cost-sharing plans tended to spend less on health because they used fewer services. The reduced use of services occurred mainly because participants in the cost-sharing treatment groups were opting not to initiate care.5\nBut the use of randomized experiments has exploded since that health-care experiment. There have been multiple Nobel Prizes given to those who use them: Vernon Smith for his pioneering of the lab- oratory experiments in 2002, and more recently, Abhijit Bannerjee, Esther Du\ufb02o, and Michael Kremer in 2019 for their leveraging of \ufb01eld\n5 More information about this fascinating experiment can be found in Newhouse\nexperiments at the service of alleviating global poverty.6 The exper- imental design has become a hallmark in applied microeconomics, political science, sociology, psychology, and more. But why is it viewed as important? Why is randomization such a key element of this design for isolating causal effects? To understand this, we need to learn more about the powerful notation that Splawa-Neyman [1923] developed, called \u201cpotential outcomes.\u201d\nPotential outcomes. While the potential outcomes notation goes back to Splawa-Neyman [1923], it got a big lift in the broader social sciences with Rubin [1974].7 As of this book\u2019s writing, potential outcomes is more or less the lingua franca for thinking about and expressing causal statements, and we probably owe Rubin [1974] for that as much as anyone.\nIn the potential outcomes tradition [Rubin, 1974; Splawa-Neyman, 1923], a causal effect is de\ufb01ned as a comparison between two states of the world. Let me illustrate with a simple example. In the \ufb01rst state of the world (sometimes called the \u201cactual\u201d state of the world), a man takes aspirin for his headache and one hour later reports the severity of his headache. In the second state of the world (sometimes called the \u201ccounterfactual\u201d state of the world), that same man takes nothing for his headache and one hour later reports the severity of his headache. What was the causal effect of the aspirin? According to the poten- tial outcomes tradition, the causal effect of the aspirin is the differ- ence in the severity of his headache between two states of the world: one where he took the aspirin (the actual state of the world) and one where he never took the aspirin (the counterfactual state of the world). The difference in headache severity between these two states of the world, measured at what is otherwise the same point in time, is the causal effect of aspirin on his headache. Sounds easy!\n6 If I were a betting man\u2014and I am\u2014then I would bet we see at least one more\nexperimental prize given out.", "start_char_idx": 3638, "end_char_idx": 8158, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "165ae155-f6cb-4585-a40c-38f2851203b2": {"__data__": {"id_": "165ae155-f6cb-4585-a40c-38f2851203b2", "embedding": null, "metadata": {"page number": "140 - 166", "chapter": "Potential Outcomes Causal Model"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "39a1e882-b5dd-4076-870d-98cfa39f2345", "node_type": "4", "metadata": {"page number": "140 - 166", "chapter": "Potential Outcomes Causal Model"}, "hash": "e38f1081f1286b325b814f08b024ef5550bc90d055dc94227410569ae7180129", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "56408816-91e0-4bf6-a309-dcbf6ee05392", "node_type": "1", "metadata": {"page number": "140 - 166", "chapter": "Potential Outcomes Causal Model"}, "hash": "d9e238c0b208334ca9cb43fa48c16ab3a69d4988ac5a8ead83ce7b1dad9dd3f4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c632b4c-15e8-4e1a-a2c8-40bcb3d6f102", "node_type": "1", "metadata": {}, "hash": "e80bcad6b325c75921e33349c87696bb053bdc70b69166e6807e4bb500862c83", "class_name": "RelatedNodeInfo"}}, "text": "In the second state of the world (sometimes called the \u201ccounterfactual\u201d state of the world), that same man takes nothing for his headache and one hour later reports the severity of his headache. What was the causal effect of the aspirin? According to the poten- tial outcomes tradition, the causal effect of the aspirin is the differ- ence in the severity of his headache between two states of the world: one where he took the aspirin (the actual state of the world) and one where he never took the aspirin (the counterfactual state of the world). The difference in headache severity between these two states of the world, measured at what is otherwise the same point in time, is the causal effect of aspirin on his headache. Sounds easy!\n6 If I were a betting man\u2014and I am\u2014then I would bet we see at least one more\nexperimental prize given out. The most likely candidate being John List, for his work\nusing \ufb01eld experiments.\nfactuals at the same time as Rubin\u2019s early work with the great metaphysical philosopher David Lewis [Lewis, 1973]. This stuff was apparently in the air, which makes tracing the\nTo even ask questions like this (let alone attempt to answer them) is to engage in storytelling. Humans have always been interested in stories exploring counterfactuals. What if Bruce Wayne\u2019s parents had never been murdered? What if that waitress had won the lottery? What if your friend from high school had never taken that \ufb01rst drink? What if in The Matrix Neo had taken the blue pill? These are fun hypotheticals to entertain, but they are still ultimately storytelling. We need Doctor Strange to give us the Time Stone to answer questions like these.\nYou can probably see where this is going. The potential outcomes notation expresses causality in terms of counterfactuals, and since counterfactuals do not exist, con\ufb01dence about causal effects must to some degree be unanswerable. To wonder how life would be different had one single event been different is to indulge in counterfactual rea- soning, and counterfactuals are not realized in history because they are hypothetical states of the world. Therefore, if the answer requires data on those counterfactuals, then the question cannot be answered. History is a sequence of observable, factual events, one after another. We don\u2019t know what would have happened had one event changed because we are missing data on the counterfactual outcome.8 Poten- tial outcomes exist ex ante as a set of possibilities, but once a decision is made, all but one outcome disappears.9\nTo make this concrete, let\u2019s introduce some notation and more speci\ufb01c concepts. For simplicity, we will assume a binary variable that takes on a value of 1 if a particular unit i receives the treatment and a 0 if it does not.10 Each unit will have two potential outcomes, but only\n8 Counterfactual reasoning can be helpful, but it can also be harmful, particularly\nwhen it is the source of regret. There is likely a counterfactual version of the sunk-cost\nfallacy wherein, since we cannot know with certainty what would\u2019ve happened had we\nmade a different decision, we must accept a certain amount of basic uncertainty just to\nmove on and get over it. Ultimately, no one can say that an alternative decision would\u2019ve\nhad a better outcome. You cannot know, and that can be di\ufb03cult sometimes. It has been\nand will continue to be, for me at least.\n9 As best I can tell, the philosopher I mentioned earlier, David Lewis, believed that\npotential outcomes were actually separate worlds\u2014just as real as our world. That means\nthat, according to Lewis, there is a very real, yet inaccessible, world in which Kanye released Yandhi instead of Jesus Is King, I \ufb01nd extremely frustrating.\n10 A couple of things. First, this analysis can be extended to more than two poten-\ntial outcomes, but as a lot of this book focuses on program evaluation, I am sticking\none observed outcome. Potential outcomes are de\ufb01ned as Y1 if unit i i received the treatment and as Y0 if the unit did not. Notice that both i potential outcomes have the same i subscript\u2014this indicates two sep- arate states of the world for the exact same person in our example at the exact same moment in time. We\u2019ll call the state of the world where no treatment occurred the control state. Each unit i has exactly two potential outcomes: a potential outcome under a state of the world where the treatment occurred (Y1) and a potential outcome where the treatment did not occur (Y0).", "start_char_idx": 7313, "end_char_idx": 11773, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8c632b4c-15e8-4e1a-a2c8-40bcb3d6f102": {"__data__": {"id_": "8c632b4c-15e8-4e1a-a2c8-40bcb3d6f102", "embedding": null, "metadata": {"page number": "140 - 166", "chapter": "Potential Outcomes Causal Model"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "39a1e882-b5dd-4076-870d-98cfa39f2345", "node_type": "4", "metadata": {"page number": "140 - 166", "chapter": "Potential Outcomes Causal Model"}, "hash": "e38f1081f1286b325b814f08b024ef5550bc90d055dc94227410569ae7180129", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "165ae155-f6cb-4585-a40c-38f2851203b2", "node_type": "1", "metadata": {"page number": "140 - 166", "chapter": "Potential Outcomes Causal Model"}, "hash": "6d9a0bb7e310695a0e1123a74c3d31ef5b9a1bceeed66768cd4fe156047c469d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bbfbff1d-d29c-4036-bc98-4670e198a78d", "node_type": "1", "metadata": {}, "hash": "fc12d4114fe966570af8d31bce388ad5965df973cb76d74e481ea3071615fe5c", "class_name": "RelatedNodeInfo"}}, "text": "10 A couple of things. First, this analysis can be extended to more than two poten-\ntial outcomes, but as a lot of this book focuses on program evaluation, I am sticking\none observed outcome. Potential outcomes are de\ufb01ned as Y1 if unit i i received the treatment and as Y0 if the unit did not. Notice that both i potential outcomes have the same i subscript\u2014this indicates two sep- arate states of the world for the exact same person in our example at the exact same moment in time. We\u2019ll call the state of the world where no treatment occurred the control state. Each unit i has exactly two potential outcomes: a potential outcome under a state of the world where the treatment occurred (Y1) and a potential outcome where the treatment did not occur (Y0).\nObservable or \u201cactual\u201d outcomes, Yi, are distinct from potential outcomes. First, notice that actual outcomes do not have a super- script. That is because they are not potential outcomes\u2014they are the realized, actual, historical, empirical\u2014however you want to say it\u2014outcomes that unit i experienced. Whereas potential outcomes are hypothetical random variables that differ across the population, observable outcomes are factual random variables. How we get from potential outcomes to actual outcomes is a major philosophical move, but like any good economist, I\u2019m going to make it seem simpler than it is with an equation. A unit\u2019s observable outcome is a function of its potential outcomes determined according to the switching equation:\nwhere Di equals 1 if the unit received the treatment and 0 if it did not. Notice the logic of the equation. When Di = 1, then Yi = Y1 i because the second term zeroes out. And when Di = 0, the \ufb01rst term zeroes out and therefore Yi = Y0 i . Using this notation, we de\ufb01ne the unit-speci\ufb01c treatment effect, or causal effect, as the difference between the two states of the world:\nImmediately we are confronted with a problem. If a treatment effect requires knowing two states of the world, Y1 i , but by the switching equation we observe only one, then we cannot calculate the treatment effect. Herein lies the fundamental problem of causal\nwith just two. Second, the treatment here is any particular intervention that can be\nmanipulated, such as the taking of aspirin or not. In the potential outcomes tradition,\nmanipulation is central to the concept of causality.\ninference\u2014certainty around causal effects requires access to data that is and always will be missing.\nAverage treatment effects. From this simple de\ufb01nition of a treatment effect come three different parameters that are often of interest to researchers. They are all population means. The \ufb01rst is called the average treatment effect:\nNotice, as with our de\ufb01nition of individual-level treatment effects, that the average treatment effect requires both potential outcomes for each i unit. Since we only know one of these by the switching equation, the average treatment effect, or the ATE, is inherently unknowable. Thus, the ATE, like the individual treatment effect, is not a quantity that can be calculated. But it can be estimated.\nThe second parameter of interest is the average treatment effect for the treatment group. That\u2019s a mouthful, but let me explain. There exist two groups of people in this discussion we\u2019ve been having: a treat- ment group and a control group. The average treatment effect for the treatment group, or ATT for short, is simply that population mean treat- ment effect for the group of units that had been assigned the treatment in the \ufb01rst place according to the switching equation. Insofar as \u03b4i dif- fers across the population, the ATT will likely differ from the ATE. In observational data involving human beings, it almost always will be different from the ATE, and that\u2019s because individuals will be endoge- nously sorting into some treatment based on the gains they expect from it. Like the ATE, the ATT is unknowable, because like the ATE, it also requires two observations per treatment unit i. Formally we write the ATT as:\nThe \ufb01nal parameter of interest is called the average treatment effect for the control group, or untreated group. It\u2019s shorthand is ATU, which stands for average treatment effect for the untreated. And like ATT, the ATU is simply the population mean treatment effect for those units who sorted into the control group.11 Given heterogeneous treat- ment effects, it\u2019s probably the case that the ATT (cid:9)= ATU, especially in an observational setting. The formula for the ATU is as follows:\nDepending on the research question, one, or all three, of these parameters is interesting.", "start_char_idx": 11017, "end_char_idx": 15619, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bbfbff1d-d29c-4036-bc98-4670e198a78d": {"__data__": {"id_": "bbfbff1d-d29c-4036-bc98-4670e198a78d", "embedding": null, "metadata": {"page number": "140 - 166", "chapter": "Potential Outcomes Causal Model"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "39a1e882-b5dd-4076-870d-98cfa39f2345", "node_type": "4", "metadata": {"page number": "140 - 166", "chapter": "Potential Outcomes Causal Model"}, "hash": "e38f1081f1286b325b814f08b024ef5550bc90d055dc94227410569ae7180129", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c632b4c-15e8-4e1a-a2c8-40bcb3d6f102", "node_type": "1", "metadata": {"page number": "140 - 166", "chapter": "Potential Outcomes Causal Model"}, "hash": "5b7da65e72186fb64a228492021ccc031f9015cd7d1ea10b1390d88e5d75c0a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bfc12990-1ab9-4538-b340-a3935271b8d5", "node_type": "1", "metadata": {}, "hash": "934dc547d348a7ef43acf4a1001d12a409f3f663361a8157cbaaed3ec6f88f0a", "class_name": "RelatedNodeInfo"}}, "text": "Like the ATE, the ATT is unknowable, because like the ATE, it also requires two observations per treatment unit i. Formally we write the ATT as:\nThe \ufb01nal parameter of interest is called the average treatment effect for the control group, or untreated group. It\u2019s shorthand is ATU, which stands for average treatment effect for the untreated. And like ATT, the ATU is simply the population mean treatment effect for those units who sorted into the control group.11 Given heterogeneous treat- ment effects, it\u2019s probably the case that the ATT (cid:9)= ATU, especially in an observational setting. The formula for the ATU is as follows:\nDepending on the research question, one, or all three, of these parameters is interesting. But the two most common ones of interest are the ATE and the ATT.\nSimple difference in means decomposition. This discussion has been somewhat abstract, so let\u2019s be concrete. Let\u2019s assume there are ten patients i who have cancer, and two medical procedures or treatments. There is a surgery intervention, Di = 1, and there is a chemotherapy intervention, Di = 0. Each patient has the following two potential out- comes where a potential outcome is de\ufb01ned as post-treatment life span in years: a potential outcome in a world where they received surgery and a potential outcome where they had instead received chemo. We use the notation Y1 and Y0, respectively, for these two states of the world.\nWe can calculate the average treatment effect if we have this matrix of data, because the average treatment effect is simply the mean difference between columns 2 and 3. That is, E[Y1] =5.6, and E[Y0] =5, which means that ATE = 0.6. In words, the average treat- ment effect of surgery across these speci\ufb01c patients is 0.6 additional years (compared to chemo).\nBut that is just the average. Notice, though: not everyone bene\ufb01ts from surgery. Patient 7, for instance, lives only one additional year post- surgery versus ten additional years post-chemo. But the ATE is simply the average over these heterogeneous treatment effects.\n11 This can happen because of preferences, but it also can happen because of\nconstraints. Utility maximization, remember, is a constrained optimization process, and therefore value and obstacles both play a role in sorting.\nTable 11. Potential outcomes for ten patients receiving surgery Y1 or chemo Y0.\nlet\u2019s assume that there exists the per- fect doctor who knows each person\u2019s potential outcomes and chooses whichever treatment that maximizes a person\u2019s post-treatment life span.12 In other words, the doctor chooses to put a patient in surgery or chemotherapy depending on whichever treatment has the longer post- treatment life span. Once he makes that treatment assignment, the doctor observes their post-treatment actual outcome according to the switching equation mentioned earlier.\nTo maintain this \ufb01ction,\nTable 12 shows only the observed outcome for treatment and con- trol group. Table 12 differs from Table 11, which shows each unit\u2019s potential outcomes. Once treatment has been assigned, we can cal- culate the average treatment effect for the surgery group (ATT) versus the chemo group (ATU). The ATT equals 4.4, and the ATU equals \u22123.2. That means that the average post-surgery life span for the surgery group is 4.4 additional years, whereas the average post-surgery life span for the chemotherapy group is 3.2 fewer years.13\n12 Think of the \u201cperfect doctor\u201d as like a Harry Potter\u2013style Sorting Hat. I \ufb01rst\nlearned of this \u201cperfect doctor\u201d illustration from Rubin himself.\n13 The reason that the ATU is negative is because the treatment here is the surgery,\nwhich did not perform as well as chemotherapy-untreated units. But you could just as easily interpret this as 3.2 additional years of life if they had received chemo instead of surgery.\nTable 12. Post-treatment observed life spans in years for surgery D = 1 versus chemotherapy D = 0.\nNow the ATE is 0.6, which is just a weighted average between the ATT and the ATU.14 So we know that the overall effect of surgery is positive, although the effect for some is negative. There exist hetero- geneous treatment effects, in other words, but the net effect is posi- tive. What if we were to simply compare the average post-surgery life span for the two groups?", "start_char_idx": 14895, "end_char_idx": 19174, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bfc12990-1ab9-4538-b340-a3935271b8d5": {"__data__": {"id_": "bfc12990-1ab9-4538-b340-a3935271b8d5", "embedding": null, "metadata": {"page number": "140 - 166", "chapter": "Potential Outcomes Causal Model"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "39a1e882-b5dd-4076-870d-98cfa39f2345", "node_type": "4", "metadata": {"page number": "140 - 166", "chapter": "Potential Outcomes Causal Model"}, "hash": "e38f1081f1286b325b814f08b024ef5550bc90d055dc94227410569ae7180129", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bbfbff1d-d29c-4036-bc98-4670e198a78d", "node_type": "1", "metadata": {"page number": "140 - 166", "chapter": "Potential Outcomes Causal Model"}, "hash": "015cb38e0c9aa378960b5e7e42697c21be730a1159c3e6b86efa86a08de42c67", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "65264c56-b76e-4031-902c-82487cb4cd35", "node_type": "1", "metadata": {}, "hash": "47dcaf8e66cc5400a1d379388eb94ac9b7b0830cacb09340470ccc8ae63355d1", "class_name": "RelatedNodeInfo"}}, "text": "I \ufb01rst\nlearned of this \u201cperfect doctor\u201d illustration from Rubin himself.\n13 The reason that the ATU is negative is because the treatment here is the surgery,\nwhich did not perform as well as chemotherapy-untreated units. But you could just as easily interpret this as 3.2 additional years of life if they had received chemo instead of surgery.\nTable 12. Post-treatment observed life spans in years for surgery D = 1 versus chemotherapy D = 0.\nNow the ATE is 0.6, which is just a weighted average between the ATT and the ATU.14 So we know that the overall effect of surgery is positive, although the effect for some is negative. There exist hetero- geneous treatment effects, in other words, but the net effect is posi- tive. What if we were to simply compare the average post-surgery life span for the two groups? This simplistic estimator is called the simple difference in means, and it is an estimate of the ATE equal to\nwhich can be estimated using samples of data:\nyi | di = 1\nyi | di = 0\nwhich in this situation is equal to 7 \u2212 7.4 = \u22120.4. That means that the treatment group lives 0.4 fewer years post-surgery than the chemo group when the perfect doctor assigned each unit to its best treat- ment. While the statistic is true, notice how misleading it is. This statistic without proper quali\ufb01cation could easily be used to claim that, on average, surgery is harmful, when we know that\u2019s not true.\nIt\u2019s biased because the individuals units were optimally sorting into\ntheir best treatment option, creating fundamental differences between\ntreatment and control group that are a direct function of the poten-\ntial outcomes themselves. To make this as clear as I can make it, we\nwill decompose the simple difference in means into three parts. Those\nthree parts are listed below:\nTo understand where these parts on the right-hand side originate, we\nneed to start over and decompose the parameter of interest, ATE,\ninto its basic building blocks. ATE is equal to the weighted sum of\nwhere \u03c0 is the share of patients who received surgery and 1 \u2212 \u03c0 is the share of patients who received chemotherapy. Because the conditional\nexpectation notation is a little cumbersome, let\u2019s exchange each term\non the left side, ATE, and right side with some letters. This will make\nNow that we have made these substitutions, let\u2019s rearrange the letters by rede\ufb01ning ATE as a weighted average of all conditional expectations\n0 = e \u2212 \u03c0a \u2212 b + \u03c0b + \u03c0c + d \u2212 \u03c0d \u2212 a + a \u2212 c + c \u2212 d + d\na \u2212 d = e \u2212 \u03c0a \u2212 b + \u03c0b + \u03c0c + d \u2212 \u03c0d + a \u2212 c + c \u2212 d\na \u2212 d = e + (c \u2212 d) + a \u2212 \u03c0a \u2212 b + \u03c0b \u2212 c + \u03c0c + d \u2212 \u03c0d\nNow, substituting our de\ufb01nitions, we get the following:\nAnd the decomposition ends. Now the fun part\u2014let\u2019s think about what we just made! The left side can be estimated with a sample of data, as both of those potential outcomes become actual outcomes under the switching equation. That\u2019s just the simple difference in mean out- comes. It\u2019s the right side that is more interesting because it tells us what the simple difference in mean outcomes is by de\ufb01nition. Let\u2019s put some labels to it.\nyi | di = 1\nLet\u2019s discuss each of these in turn. The left side is the simple differ- ence in mean outcomes, and we already know it is equal to \u22120.4. Since this is a decomposition, it must be the case that the right side also equals \u22120.4.\nThe \ufb01rst term is the average treatment effect, which is the parame- ter of interest, and we know that it is equal to 0.6. Thus, the remaining two terms must be the source of the bias that is causing the simple difference in means to be negative.\nThe second term is called the selection bias, which merits some unpacking. In this case, the selection bias is the inherent difference between the two groups if both received chemo. Usually, though, it\u2019s just a description of the differences between the two groups if there had never been a treatment in the \ufb01rst place. There are in other words two groups: a surgery group and a chemo group. How do their potential outcomes under control differ?", "start_char_idx": 18361, "end_char_idx": 22357, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "65264c56-b76e-4031-902c-82487cb4cd35": {"__data__": {"id_": "65264c56-b76e-4031-902c-82487cb4cd35", "embedding": null, "metadata": {"page number": "140 - 166", "chapter": "Potential Outcomes Causal Model"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "39a1e882-b5dd-4076-870d-98cfa39f2345", "node_type": "4", "metadata": {"page number": "140 - 166", "chapter": "Potential Outcomes Causal Model"}, "hash": "e38f1081f1286b325b814f08b024ef5550bc90d055dc94227410569ae7180129", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bfc12990-1ab9-4538-b340-a3935271b8d5", "node_type": "1", "metadata": {"page number": "140 - 166", "chapter": "Potential Outcomes Causal Model"}, "hash": "f66e7a786ce86eb662cdd1109dce9f7c9841d3b38e4e59f382ba6553e6062089", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "816ae97f-aa5a-4c2b-a7c9-083b296c106e", "node_type": "1", "metadata": {}, "hash": "af70cccf0648c6f7bc03a0e9ff536f447ec523e6396b5747560a21ed4e3ff439", "class_name": "RelatedNodeInfo"}}, "text": "Since this is a decomposition, it must be the case that the right side also equals \u22120.4.\nThe \ufb01rst term is the average treatment effect, which is the parame- ter of interest, and we know that it is equal to 0.6. Thus, the remaining two terms must be the source of the bias that is causing the simple difference in means to be negative.\nThe second term is called the selection bias, which merits some unpacking. In this case, the selection bias is the inherent difference between the two groups if both received chemo. Usually, though, it\u2019s just a description of the differences between the two groups if there had never been a treatment in the \ufb01rst place. There are in other words two groups: a surgery group and a chemo group. How do their potential outcomes under control differ? Notice that the \ufb01rst is a counterfactual, whereas the second is an observed outcome according to the switch- ing equation. We can calculate this difference here because we have the complete potential outcomes in Table 11. That difference is equal to \u22124.8.\nThe third term is a lesser-known form of bias, but it\u2019s interesting. Plus, if the focus is the ATE, then it is always present.15 The heteroge- neous treatment effect bias is simply the different returns to surgery for the two groups multiplied by the share of the population that is in the chemotherapy group at all. This \ufb01nal term is 0.5 \u00d7 (4.4 \u2212 (\u22123.2)) or 3.8. Note in case it\u2019s not obvious that the reason \u03c0 = 0.5 is because 5 out of 10 units are in the chemotherapy group.\nNow that we have all three parameters on the right side, we can\nsee why the simple difference in mean outcomes is equal to \u22120.4.\nWhat I \ufb01nd interesting\u2014hopeful even\u2014in this decomposition is that it shows that a contrast between treatment and control group techni- cally \u201ccontains\u201d the parameter of interest. I placed \u201ccontains\u201d in quotes\n15 Note that Angrist and Pischke [2009] have a slightly different decomposition where the SDO = ATT + selection bias, but that is because their parameter of interest is the ATT, and therefore the third term doesn\u2019t appear.\nbecause while it is clearly visible in the decomposition, the simple dif- ference in outcomes is ultimately not laid out as the sum of three parts. Rather, the simple difference in outcomes is nothing more than a num- ber. The number is the sum of the three parts, but we cannot calculate each individual part because we do not have data on the underlying counterfactual outcomes needed to make the calculations. The prob- lem is that that parameter of interest has been masked by two forms of bias, the selection bias and the heterogeneous treatment effect bias. If we knew those, we could just subtract them out, but ordinarily we don\u2019t know them. We develop strategies to negate these biases, but we can- not directly calculate them any more than we can directly calculate the ATE, as these biases depend on unobservable counterfactuals.\nThe problem isn\u2019t caused by assuming heterogeneity either. We can make the strong assumption that treatment effects are constant, \u03b4i = \u03b4 \u2200i, which will cause ATU = ATT and make SDO = ATE + selection bias. But we\u2019d still have that nasty selection bias screwing things up. One could argue that the entire enterprise of causal inference is about developing a reasonable strategy for negating the role that selection bias is playing in estimated causal effects.\nIndependence assumption. Let\u2019s start with the most credible situa- tion for using SDO to estimate ATE: when the treatment itself (e.g., surgery) has been assigned to patients independent of their potential outcomes. But what does this word \u201cindependence\u201d mean anyway? Well, notationally, it means:\nWhat this means is that surgery was assigned to an individual for rea- sons that had nothing to do with the gains to surgery.16 Now in our example, we already know that this is violated because the perfect doctor speci\ufb01cally chose surgery or chemo based on potential out- comes. Speci\ufb01cally, a patient received surgery if Y1 > Y0 and chemo if Y1 < Y0. Thus, in our case, the perfect doctor ensured that D depended on Y1 and Y0. All forms of human-based sorting\u2014probably as a rule\n16 Why do I say \u201cgains\u201d? Because the gain to surgery is Y1 i independent of gains, we are saying it\u2019s independent of Y1 and Y0.\ni .", "start_char_idx": 21577, "end_char_idx": 25861, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "816ae97f-aa5a-4c2b-a7c9-083b296c106e": {"__data__": {"id_": "816ae97f-aa5a-4c2b-a7c9-083b296c106e", "embedding": null, "metadata": {"page number": "140 - 166", "chapter": "Potential Outcomes Causal Model"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "39a1e882-b5dd-4076-870d-98cfa39f2345", "node_type": "4", "metadata": {"page number": "140 - 166", "chapter": "Potential Outcomes Causal Model"}, "hash": "e38f1081f1286b325b814f08b024ef5550bc90d055dc94227410569ae7180129", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "65264c56-b76e-4031-902c-82487cb4cd35", "node_type": "1", "metadata": {"page number": "140 - 166", "chapter": "Potential Outcomes Causal Model"}, "hash": "b061d5b8df21a585184e9fec849f04b712f3229c397c4b3fc284c410ca86ee4a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "588b4611-c557-4839-938c-045601ea7e35", "node_type": "1", "metadata": {}, "hash": "f68d5525f225d52399c9d0df2667e58630996efb745f434f860e12855e7e159b", "class_name": "RelatedNodeInfo"}}, "text": "But what does this word \u201cindependence\u201d mean anyway? Well, notationally, it means:\nWhat this means is that surgery was assigned to an individual for rea- sons that had nothing to do with the gains to surgery.16 Now in our example, we already know that this is violated because the perfect doctor speci\ufb01cally chose surgery or chemo based on potential out- comes. Speci\ufb01cally, a patient received surgery if Y1 > Y0 and chemo if Y1 < Y0. Thus, in our case, the perfect doctor ensured that D depended on Y1 and Y0. All forms of human-based sorting\u2014probably as a rule\n16 Why do I say \u201cgains\u201d? Because the gain to surgery is Y1 i independent of gains, we are saying it\u2019s independent of Y1 and Y0.\ni . Thus, if we say it\u2019s\nto be honest\u2014violate independence, which is the main reason na\u00efve observational comparisons are almost always incapable of recovering causal effects.17\nBut what if he hadn\u2019t done that? What if he had chosen surgery in such a way that did not depend on Y1 or Y0? How does one choose surgery independent of the expected gains of the surgery? For instance, maybe he alphabetized them by last name, and the \ufb01rst \ufb01ve received surgery and the last \ufb01ve received chemotherapy. Or maybe he used the second hand on his watch to assign surgery to them: if it was between 1 and 30 seconds, he gave them surgery, and if it was between 31 and 60 seconds, he gave them chemotherapy.18 In other words, let\u2019s say that he chose some method for assigning treatment that did not depend on the values of potential outcomes under either state of the world. What would that mean in this context? Well, it would mean:\nIn other words, it would mean that the mean potential outcome for Y1 or Y0 is the same (in the population) for either the surgery group or the chemotherapy group. This kind of randomization of the treatment assignment would eliminate both the selection bias and the heteroge- neous treatment effect bias. Let\u2019s take it in order. The selection bias zeroes out as follows:\nAnd thus the SDO no longer suffers from selection bias. How does randomization affect heterogeneity treatment bias from the third line?\n17 This is actually where economics is helpful in my opinion. Economics empha-\nsizes that observed values are equilibria based on agents engaging in constrained opti-\nmization and that all but guarantees that independence is violated in observational data.\nRarely are human beings making important life choices by \ufb02ipping coins.\n18 In Craig [2006], a poker-playing banker used the second hand on his watch as a\nrandom number generator to randomly bluff when he had a weak hand.\nRewrite the third row bias after 1 \u2212 \u03c0:\nIf treatment is independent of potential outcomes, then:\nyi | di = 1\nyi | di = 0\nWhat\u2019s necessary in this situation is simply (a) data on observable out- comes, (b) data on treatment assignment, and (c) (Y1, Y0) \u22a5\u22a5 D. We call (c) the independence assumption. To illustrate that this would lead to the SDO, we use the following Monte Carlo simulation.", "start_char_idx": 25168, "end_char_idx": 28152, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "588b4611-c557-4839-938c-045601ea7e35": {"__data__": {"id_": "588b4611-c557-4839-938c-045601ea7e35", "embedding": null, "metadata": {"page number": "140 - 166", "chapter": "Potential Outcomes Causal Model"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "39a1e882-b5dd-4076-870d-98cfa39f2345", "node_type": "4", "metadata": {"page number": "140 - 166", "chapter": "Potential Outcomes Causal Model"}, "hash": "e38f1081f1286b325b814f08b024ef5550bc90d055dc94227410569ae7180129", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "816ae97f-aa5a-4c2b-a7c9-083b296c106e", "node_type": "1", "metadata": {"page number": "140 - 166", "chapter": "Potential Outcomes Causal Model"}, "hash": "d54f68c10f7e2ed14d89e24be389037225cdaf8715cb976eea403cf61b47287a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6de4c20e-a4c0-4969-8bc6-6af20801f02e", "node_type": "1", "metadata": {}, "hash": "867fe9d0dafc125995315bb3f56e5bb88859c29c0c36ac133e3bbb20408b69e6", "class_name": "RelatedNodeInfo"}}, "text": "Economics empha-\nsizes that observed values are equilibria based on agents engaging in constrained opti-\nmization and that all but guarantees that independence is violated in observational data.\nRarely are human beings making important life choices by \ufb02ipping coins.\n18 In Craig [2006], a poker-playing banker used the second hand on his watch as a\nrandom number generator to randomly bluff when he had a weak hand.\nRewrite the third row bias after 1 \u2212 \u03c0:\nIf treatment is independent of potential outcomes, then:\nyi | di = 1\nyi | di = 0\nWhat\u2019s necessary in this situation is simply (a) data on observable out- comes, (b) data on treatment assignment, and (c) (Y1, Y0) \u22a5\u22a5 D. We call (c) the independence assumption. To illustrate that this would lead to the SDO, we use the following Monte Carlo simulation. Note that ATE in this example is equal to 0.6.\nversion 14.2 syntax [, obs(integer 1) mu(real 0) sigma(real 1) ] clear drop _all set obs 10 gen y1 = 7 in 1 replace y1 = 5 in 2\n(continued)\nSTATA (continued)\nreplace y1 = 5 in 3 replace y1 = 7 in 4 replace y1 = 4 in 5 replace y1 = 10 in 6 replace y1 = 1 in 7 replace y1 = 5 in 8 replace y1 = 3 in 9 replace y1 = 9 in 10\ngen y0 = 1 in 1 replace y0 = 6 in 2 replace y0 = 1 in 3 replace y0 = 8 in 4 replace y0 = 2 in 5 replace y0 = 1 in 6 replace y0 = 10 in 7 replace y0 = 6 in 8 replace y0 = 7 in 9 replace y0 = 8 in 10 drawnorm random sort random\nd=1 in 1/5 gen replace d=0 in 6/10 gen y=d*y1 + (1-d)*y0 egen sy1 = mean(y) if d==1 egen sy0 = mean(y) if d==0 collapse (mean) sy1 sy0 gen sdo = sy1 - sy0 keep sdo summarize sdo gen mean = r(mean) end\nThis Monte Carlo runs 10,000 times, each time calculating the average SDO under independence\u2014which is ensured by the random number sorting that occurs. In my running of this program, the ATE is 0.6, and the SDO is on average equal to 0.59088.19\nBefore we move on from the SDO, let\u2019s just emphasize something that is often lost on students \ufb01rst learning the independence concept and notation. Independence does not imply that E[Y1 | D = 1] \u2212E [Y0 | D = 0] =0. Nor does it imply that E[Y1 | D = 1] \u2212E [Y0 | D = 1] =0. Rather, it implies\n19 Because it\u2019s not seeded, when you run it, your answer will be close but slightly\nin a large population.20 That is, independence implies that the two groups of units, surgery and chemo, have the same potential outcome on average in the population.\nHow realistic is independence in observational data? Economics\u2014 maybe more than any other science\u2014tells us that independence is unlikely to hold observationally. Economic actors are always attempt- ing to achieve some optima. For instance, parents are putting kids in what they perceive to be the best school for them, and that is based on potential outcomes. In other words, people are choosing their interven- tions, and most likely their decisions are related to the potential out- comes, which makes simple comparisons improper. Rational choice is always pushing against the independence assumption, and there- fore simple comparison in means will not approximate the true causal effect. We need unit randomization for simple comparisons to help us understand the causal effects at play.\nSUTVA. Rubin argues that there are a bundle of assumptions behind this kind of calculation, and he calls these assumptions the stable unit treatment value assumption, or SUTVA for short. That\u2019s a mouthful, but here\u2019s what it means: our potential outcomes framework places limits on us for calculating treatment effects. When those limits do not cred- ibly hold in the data, we have to come up with a new solution.", "start_char_idx": 27346, "end_char_idx": 30937, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6de4c20e-a4c0-4969-8bc6-6af20801f02e": {"__data__": {"id_": "6de4c20e-a4c0-4969-8bc6-6af20801f02e", "embedding": null, "metadata": {"page number": "140 - 166", "chapter": "Potential Outcomes Causal Model"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "39a1e882-b5dd-4076-870d-98cfa39f2345", "node_type": "4", "metadata": {"page number": "140 - 166", "chapter": "Potential Outcomes Causal Model"}, "hash": "e38f1081f1286b325b814f08b024ef5550bc90d055dc94227410569ae7180129", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "588b4611-c557-4839-938c-045601ea7e35", "node_type": "1", "metadata": {"page number": "140 - 166", "chapter": "Potential Outcomes Causal Model"}, "hash": "b1847e3bce372f16862c56c8ad6fc499ae59d4b9b5a7d33a9f040493e8ca8e40", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62a91a12-8022-4894-bc99-a0863296d34c", "node_type": "1", "metadata": {}, "hash": "6abac30d02582f2af4169e5ac2360c805c94442a584aa948f9abeab15b2db395", "class_name": "RelatedNodeInfo"}}, "text": "In other words, people are choosing their interven- tions, and most likely their decisions are related to the potential out- comes, which makes simple comparisons improper. Rational choice is always pushing against the independence assumption, and there- fore simple comparison in means will not approximate the true causal effect. We need unit randomization for simple comparisons to help us understand the causal effects at play.\nSUTVA. Rubin argues that there are a bundle of assumptions behind this kind of calculation, and he calls these assumptions the stable unit treatment value assumption, or SUTVA for short. That\u2019s a mouthful, but here\u2019s what it means: our potential outcomes framework places limits on us for calculating treatment effects. When those limits do not cred- ibly hold in the data, we have to come up with a new solution. And those limitations are that each unit receives the same sized dose, no spillovers (\u201cexternalities\u201d) to other units\u2019 potential outcomes when a unit is exposed to some treatment, and no general equilibrium effects. First, this implies that the treatment is received in homogeneous doses to all units. It\u2019s easy to imagine violations of this, though\u2014for instance, if some doctors are better surgeons than others. In which case, we just need to be careful what we are and are not de\ufb01ning as the treatment.\nSecond, this implies that there are no externalities, because by de\ufb01nition, an externality spills over to other untreated units. In other words, if unit 1 receives the treatment, and there is some externality,\n20 Here\u2019s a simple way to remember what equality we get with independence. The\nterm before the vertical bar is the same, but the term after the vertical bar is different. So independence guarantees that in the population Y1 is the same on average, for each group.\nthen unit 2 will have a different Y0 value than if unit 1 had not received the treatment. We are assuming away this kind of spillover. When there are such spillovers, though, such as when we are working with social network data, we will need to use models that can explicitly account for such SUTVA violations, such as that of Goldsmith-Pinkham and Imbens [2013].\nRelated to this problem of spillovers is the issue of general equi- librium. Let\u2019s say we are estimating the causal effect of returns to schooling. The increase in college education would in general equi- librium cause a change in relative wages that is different from what happens under partial equilibrium. This kind of scaling-up issue is of common concern when one considers extrapolating from the experi- mental design to the large-scale implementation of an intervention in some population.\nReplicating \u201cdemand for learning HIV status.\u201d Rebecca Thornton is a proli\ufb01c, creative development economist. Her research has spanned a number of topics in development and has evaluated critically impor- tant questions regarding optimal HIV policy, demand for learning, cir- cumcision, education, and more. Some of these papers have become major accomplishments. Meticulous and careful, she has become a leading expert on HIV in sub-Saharan Africa. I\u2019d like to discuss an ambi- tious project she undertook as a grad student in rural Malawi concern- ing whether cash incentives caused people to learn their HIV status and the cascading effect of that learning on subsequent risky sexual behavior [Thornton, 2008].\nThornton\u2019s study emerges in a policy context where people believed that HIV testing could be used to \ufb01ght the epidemic. The idea was simple: if people learned their HIV status, then maybe learn- ing they were infected would cause them to take precautions, thus slowing the rate of infection. For instance, they might seek medi- cal treatment, thus prolonging their life and the quality of life they enjoyed. But upon learning their HIV status, maybe \ufb01nding out they were HIV-positive would cause them to decrease high-risk behavior. If so, then increased testing could create frictions throughout the sex- ual network itself that would slow an epidemic. So commonsense was this policy that the assumptions on which it rested were not chal- lenged until Thornton [2008] did an ingenious \ufb01eld experiment in rural\nMalawi. Her results were, like many studies, a mixture of good news and bad.\nAttempting to understand the demand for HIV status, or the effect is generally impossible without of HIV status on health behaviors, an experiment. Insofar as individuals are optimally choosing to learn about their type or engaging in health behaviors, then it is unlikely that knowledge about HIV status is independent of potential outcomes.", "start_char_idx": 30092, "end_char_idx": 34721, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "62a91a12-8022-4894-bc99-a0863296d34c": {"__data__": {"id_": "62a91a12-8022-4894-bc99-a0863296d34c", "embedding": null, "metadata": {"page number": "140 - 166", "chapter": "Potential Outcomes Causal Model"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "39a1e882-b5dd-4076-870d-98cfa39f2345", "node_type": "4", "metadata": {"page number": "140 - 166", "chapter": "Potential Outcomes Causal Model"}, "hash": "e38f1081f1286b325b814f08b024ef5550bc90d055dc94227410569ae7180129", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6de4c20e-a4c0-4969-8bc6-6af20801f02e", "node_type": "1", "metadata": {"page number": "140 - 166", "chapter": "Potential Outcomes Causal Model"}, "hash": "cb6dec948eb3c00a6f18f2ff377e46480ea0ce9f28358bd1728d82f6d52edf3b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a264822-5315-4642-be39-1ba8e6061c0a", "node_type": "1", "metadata": {}, "hash": "b717bf907ff60fd34bcf768401e00916fd9784322559349c1dacc0f1e963b8f7", "class_name": "RelatedNodeInfo"}}, "text": "For instance, they might seek medi- cal treatment, thus prolonging their life and the quality of life they enjoyed. But upon learning their HIV status, maybe \ufb01nding out they were HIV-positive would cause them to decrease high-risk behavior. If so, then increased testing could create frictions throughout the sex- ual network itself that would slow an epidemic. So commonsense was this policy that the assumptions on which it rested were not chal- lenged until Thornton [2008] did an ingenious \ufb01eld experiment in rural\nMalawi. Her results were, like many studies, a mixture of good news and bad.\nAttempting to understand the demand for HIV status, or the effect is generally impossible without of HIV status on health behaviors, an experiment. Insofar as individuals are optimally choosing to learn about their type or engaging in health behaviors, then it is unlikely that knowledge about HIV status is independent of potential outcomes. Almost certainly, it is those very potential outcomes that shape the decisions both to acquire that information and to engage in risky behaviors of any sort. Thus, a \ufb01eld experiment would be needed if we were to test the underlying assumptions behind this commonsense policy to use testing to \ufb01ght the epidemic.\nHow did she do this, though? Respondents in rural Malawi were offered a free door-to-door HIV test and randomly assigned no voucher or vouchers ranging from $1\u2013$3. These vouchers were redeemable once they visited a nearby voluntary counseling and testing center (VCT). The most encouraging news was that monetary incentives were highly effective in causing people to seek the results of tests. On aver- age, respondents who received any cash-value voucher were two times as likely to go to the VCT center to get their test results compared to those individuals who received no compensation. How big was this incentive? Well, the average incentive in her experiment was worth about a day\u2019s wage. But she found positive status-seeking behavior even for the smallest incentive, which was worth only one-tenth a day\u2019s wage. Thornton showed that even small monetary nudges could be used to encourage people to learn their HIV type, which has obvious policy implications.\nThe second part of the experiment threw cold water on any opti- mism from her \ufb01rst results. Several months after the cash incentives were given to respondents, Thornton followed up and interviewed them about their subsequent health behaviors. Respondents were also given the opportunity to purchase condoms. Using her randomized assign- ment of incentives for learning HIV status, she was able to isolate the causal effect of learning itself on condom purchase her proxy for engaging in risky sex. She \ufb01nds that conditional on learning one\u2019s HIV status from the randomized incentives, HIV-positive individuals did increase their condom usage over those HIV-positive individuals who\nhad not learned their results but only in the form of buying two additional\ncondoms. This study suggested that some kinds of outreach, such as\ndoor-to-door testing, may cause people to learn their type\u2014particularly\nwhen bundled with incentives\u2014but simply having been incentivized to\nlearn one\u2019s HIV status may not itself lead HIV-positive individuals to\nreduce any engagement in high-risk sexual behaviors, such as having\nThorton\u2019s experiment was more complex than I am able to repre-\nsent here, and also, I focus now on only the cash-transfer aspect of the\nexperiment, in the form of vouchers. but I am going to focus purely on\nher incentive results. But before I do so, let\u2019s take a look at what she\nfound. Table 13 shows her \ufb01ndings.\nSince her project uses randomized assignment of cash transfers\nfor identifying causal effect on learning, she mechanically creates a\ntreatment assignment that is independent of the potential outcomes\nunder consideration. We know this even though we cannot directly\ntest it (i.e., potential outcomes are unseen) because we know how\nthe science works. Randomization, in other words, by design assigns\ntreatment independent of potential outcomes. And as a result, sim-\nple differences in means are su\ufb03cient for getting basic estimates of\nBut Thornton is going to estimate a linear regression model with\ncontrols instead of using a simple difference in means for a few rea-\nsons. One, doing so allows her to include a variety of controls that\ncan reduce the residual variance and thus improve the precision of her\nestimates. This has value because in improving precision, she is able\nto rule out a broader range of treatment effects that are technically\ncontained by her con\ufb01dence intervals. Although probably in this case,\nthat\u2019s not terribly important given, as we will see, that her standard\nerrors are miniscule.\nBut the inclusion of controls has other value.", "start_char_idx": 33783, "end_char_idx": 38578, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2a264822-5315-4642-be39-1ba8e6061c0a": {"__data__": {"id_": "2a264822-5315-4642-be39-1ba8e6061c0a", "embedding": null, "metadata": {"page number": "140 - 166", "chapter": "Potential Outcomes Causal Model"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "39a1e882-b5dd-4076-870d-98cfa39f2345", "node_type": "4", "metadata": {"page number": "140 - 166", "chapter": "Potential Outcomes Causal Model"}, "hash": "e38f1081f1286b325b814f08b024ef5550bc90d055dc94227410569ae7180129", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62a91a12-8022-4894-bc99-a0863296d34c", "node_type": "1", "metadata": {"page number": "140 - 166", "chapter": "Potential Outcomes Causal Model"}, "hash": "065faeff29327440c6c0f1ef7f6d7fde5e1a393b8679df8dcfce7af0d80d1db9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0b521db6-b74d-474a-b18b-e4539facdd44", "node_type": "1", "metadata": {}, "hash": "d6a0db9b1777a06567020bf97406d559436b9563099ca36eeb7907cc0c979e07", "class_name": "RelatedNodeInfo"}}, "text": "Randomization, in other words, by design assigns\ntreatment independent of potential outcomes. And as a result, sim-\nple differences in means are su\ufb03cient for getting basic estimates of\nBut Thornton is going to estimate a linear regression model with\ncontrols instead of using a simple difference in means for a few rea-\nsons. One, doing so allows her to include a variety of controls that\ncan reduce the residual variance and thus improve the precision of her\nestimates. This has value because in improving precision, she is able\nto rule out a broader range of treatment effects that are technically\ncontained by her con\ufb01dence intervals. Although probably in this case,\nthat\u2019s not terribly important given, as we will see, that her standard\nerrors are miniscule.\nBut the inclusion of controls has other value. For instance,\nassignment was conditional on observables, or if the assignment was\ndone at different times, then including these controls (such as dis-\ntrict \ufb01xed effects) is technically needed to isolate the causal effects\nTable 13. Impact of monetary incentives and distance on learning HIV results\nNote: Columns 1\u20135 represent OLS coe\ufb03cients; robust standard errors clustered by vil-\nlage (for 119 villages) with district \ufb01xed effects in parentheses. All speci\ufb01cations also\ninclude a term for age-squared. \u201cAny incentive\u201d is an indicator if the respondent received any nonzero monetary incentive. \u201cHIV\u201d is an indicator of being HIV-positive. \u201cSimu-\nlated average distance\u201d is an average distance of respondents\u2019 households to simulated\nrandomized locations of HIV results centers. Distance is measured as a straight-line\nspherical distance from a respondent\u2019s home to a randomly assigned VCT center from\ngeospatial coordinates and is measured in kilometers. *** Signi\ufb01cantly different from\nzero at 99 percent con\ufb01dence level. ** Signi\ufb01cantly different from zero at 95 percent\nthemselves. And \ufb01nally, regression generates nice standard errors, and maybe for that alone, we should give it a chance.21\nSo what did Thornton \ufb01nd? She uses least squares as her primary model, represented in columns 1\u20135. The effect sizes that she \ufb01nds\n21 She also chose to cluster those standard errors by village for 119 villages. In\ndoing so, she addresses the over-rejection problem that we saw earlier when discussing\nclustering in the probability and regression chapter.\ncould be described as gigantic. Because only 34 percent of the con- trol group participants went to a center to learn their HIV status, it is impressive that receiving any money caused a 43-percentage-point increase in learning one\u2019s HIV status. Monetary incentives\u2014even very small ones\u2014are enough to push many people over the hump to go collect health data.\nColumns 2\u20135 are also interesting, but I won\u2019t belabor them here. In short, column 2 includes a control for the amount of the incentive, which ranged from US$0 to US$3. This allows us to estimate the linear impact of each additional dollar on learning, which is relatively steep. Columns 3\u20135 include a quadratic and as a result we see that while each additional dollar increases learning, it does so only at a decreasing rate. Columns 4 and 5 include controls for distance to the VCT cen- ter, and as with other studies, distance itself is a barrier to some types of health care [Lindo et al., 2019].\nThornton also produces a simple graphic of her results, showing box plots with mean and con\ufb01dence intervals for the treatment and control group. As we will continually see throughout the book, the best papers estimating causal effects will always summarize their main results in smart and effective pictures, and this study is no exception. As this \ufb01gure shows, the effects were huge.\nFigure 13. Visual representation of cash transfers on learning HIV test results [Thornton,\nWhile learning one\u2019s own HIV status is important, particularly if it leads to medical care, the gains to policies that nudge learning are particularly higher if they lead to changes in high-risk sexual behav- ior among HIV-positive individuals. In fact, given the multiplier effects associated with introducing frictions into the sexual network via risk- mitigating behavior (particularly if it disrupts concurrent partnerships), such efforts may be so bene\ufb01cial that they justify many types of programs that otherwise may not be cost-effective.\nThornton examines in her follow-up survey where she asked all individuals, regardless of whether they learned their HIV status, the effect of a cash transfer on condom purchases.", "start_char_idx": 37769, "end_char_idx": 42292, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0b521db6-b74d-474a-b18b-e4539facdd44": {"__data__": {"id_": "0b521db6-b74d-474a-b18b-e4539facdd44", "embedding": null, "metadata": {"page number": "140 - 166", "chapter": "Potential Outcomes Causal Model"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "39a1e882-b5dd-4076-870d-98cfa39f2345", "node_type": "4", "metadata": {"page number": "140 - 166", "chapter": "Potential Outcomes Causal Model"}, "hash": "e38f1081f1286b325b814f08b024ef5550bc90d055dc94227410569ae7180129", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a264822-5315-4642-be39-1ba8e6061c0a", "node_type": "1", "metadata": {"page number": "140 - 166", "chapter": "Potential Outcomes Causal Model"}, "hash": "c21fbd6c1420c39ad7c205537bb49b14a3db906f0ed63331006d578ad5527b86", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "90eaf067-33f6-4902-b983-c897fefb509a", "node_type": "1", "metadata": {}, "hash": "7e3676fd973dd1823bd84c52dbfa16d3d87a0481069a14106f536afc5b7ad778", "class_name": "RelatedNodeInfo"}}, "text": "As this \ufb01gure shows, the effects were huge.\nFigure 13. Visual representation of cash transfers on learning HIV test results [Thornton,\nWhile learning one\u2019s own HIV status is important, particularly if it leads to medical care, the gains to policies that nudge learning are particularly higher if they lead to changes in high-risk sexual behav- ior among HIV-positive individuals. In fact, given the multiplier effects associated with introducing frictions into the sexual network via risk- mitigating behavior (particularly if it disrupts concurrent partnerships), such efforts may be so bene\ufb01cial that they justify many types of programs that otherwise may not be cost-effective.\nThornton examines in her follow-up survey where she asked all individuals, regardless of whether they learned their HIV status, the effect of a cash transfer on condom purchases. Let\u2019s \ufb01rst see her main results in Figure 14.\nIt is initially encouraging to see that the effects on condom pur- chases are large for the HIV-positive individuals who, as a result of the incentive, got their test results. Those who bought any con- doms increases from a baseline that\u2019s a little over 30 percent to a whopping 80 percent with any incentive. But where things get discouraging is when we examine how many additional condoms this actually entailed. In columns 3 and 4 of Table 14, we see the problem.\npositive individuals [Thornton, 2008].\nTable 14. Reactions to learning HIV results among sexually active at baseline\nNumber of condoms bought IV OLS\nNote: Sample includes individuals who tested for HIV and have demographic data.\nNow Thornton wisely approaches the question in two ways for the sake of the reader and for the sake of accuracy. She wants to know the effect of getting results, but the results only matter (1) for those who got their status and (2) for those who were HIV-positive. The effects shouldn\u2019t matter if they were HIV-negative. And ultimately that is what she \ufb01nds, but how is she going to answer the \ufb01rst? Here she examines the effect for those who got their results and who were HIV-positive using an interaction. And that\u2019s column 1: individuals who got their HIV status and who learned they were HIV positive were 41% more likely to buy condoms several months later. This result shrinks, though, once she utilizes the randomization of the incentives in an instrumental variables framework, which we will discuss later in the book. The coef- \ufb01cient is almost cut in half and her con\ufb01dence intervals are so large that we can\u2019t be sure the effects are nonexistent.\nBut let\u2019s say that the reason she failed to \ufb01nd an effect on any pur- chasing behavior is because the sample size is just small enough that to pick up the effect with IV is just asking too much of the data. What if we used something that had a little more information, like number of\ncondoms bought? And that\u2019s where things get pessimistic. Yes, Thorn- ton does \ufb01nd evidence that the HIV-positive individuals were buying more condoms, but when see how many, we learn that it is only around 2 more condoms at the follow-up visit (columns 3\u20134). And the effect on sex itself (not shown) was negative, small (4% reduction), and not precise enough to say either way anyway.\nIn conclusion, Thornton\u2019s study is one of those studies we regularly come across in causal inference, a mixture of positive and negative. It\u2019s positive in that nudging people with small incentives leads them to collecting information about their own HIV status. But our enthu- siasm is muted when we learn the effect on actual risk behaviors is not very large\u2014a mere two additional condoms bought several months later for the HIV-positive individuals is likely not going to generate large positive externalities unless it falls on the highest-risk HIV-positive individuals.", "start_char_idx": 41433, "end_char_idx": 45237, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "90eaf067-33f6-4902-b983-c897fefb509a": {"__data__": {"id_": "90eaf067-33f6-4902-b983-c897fefb509a", "embedding": null, "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "81eef3b9-e34d-4b00-8a19-890225e31bcc", "node_type": "4", "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "hash": "8e8d6335ab9eedc454f6ee72e8291ccc11b17c3d8d6cbafa6d4014b4034606b2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0b521db6-b74d-474a-b18b-e4539facdd44", "node_type": "1", "metadata": {"page number": "140 - 166", "chapter": "Potential Outcomes Causal Model"}, "hash": "825481c4974c9652803b0dadaf057759b3be4dc13415baaeea84ab4c12a798bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "96ae19bb-0997-4812-9dd3-540861e336a7", "node_type": "1", "metadata": {}, "hash": "f84e80aace74a98589de738b61b2faf565ba8a822f4f80549efa720917ccefc3", "class_name": "RelatedNodeInfo"}}, "text": "Randomization Inference:\nAthey and Imbens [2017a], in their chapter on randomized experi- ments, note that \u201cin randomization-based inference, uncertainty in esti- mates arises naturally from the random assignment of the treatments, rather than from hypothesized sampling from a large population\u201d (73). Athey and Imbens are part of a growing trend of economists using randomization-based methods for inferring the probability that an esti- mated coe\ufb03cient is not simply a result of change. This growing trend uses randomization-based methods to construct exact p-values that re\ufb02ect the likelihood that chance could\u2019ve produced the estimate.\nWhy has randomization inference become so population now? Why not twenty years ago or more? It\u2019s not clear why randomization- based inference has become so popular in recent years, but a few pos- sibilities could explain the trend. It may be the rise in the randomized controlled trials within economics, the availability of large-scale admin- istrative databases that are not samples of some larger population but rather represent \u201call the data,\u201d or it may be that computational power\ncondoms bought? And that\u2019s where things get pessimistic. Yes, Thorn- ton does \ufb01nd evidence that the HIV-positive individuals were buying more condoms, but when see how many, we learn that it is only around 2 more condoms at the follow-up visit (columns 3\u20134). And the effect on sex itself (not shown) was negative, small (4% reduction), and not precise enough to say either way anyway.\nIn conclusion, Thornton\u2019s study is one of those studies we regularly come across in causal inference, a mixture of positive and negative. It\u2019s positive in that nudging people with small incentives leads them to collecting information about their own HIV status. But our enthu- siasm is muted when we learn the effect on actual risk behaviors is not very large\u2014a mere two additional condoms bought several months later for the HIV-positive individuals is likely not going to generate large positive externalities unless it falls on the highest-risk HIV-positive individuals.\nAthey and Imbens [2017a], in their chapter on randomized experi- ments, note that \u201cin randomization-based inference, uncertainty in esti- mates arises naturally from the random assignment of the treatments, rather than from hypothesized sampling from a large population\u201d (73). Athey and Imbens are part of a growing trend of economists using randomization-based methods for inferring the probability that an esti- mated coe\ufb03cient is not simply a result of change. This growing trend uses randomization-based methods to construct exact p-values that re\ufb02ect the likelihood that chance could\u2019ve produced the estimate.\nWhy has randomization inference become so population now? Why not twenty years ago or more? It\u2019s not clear why randomization- based inference has become so popular in recent years, but a few pos- sibilities could explain the trend. It may be the rise in the randomized controlled trials within economics, the availability of large-scale admin- istrative databases that are not samples of some larger population but rather represent \u201call the data,\u201d or it may be that computational power\nhas improved so much that randomization inference has become triv- ially simple to implement when working with thousands of observa- tions. But whatever the reason, randomization inference has become a very common way to talk about the uncertainty around one\u2019s estimates. There are at least three reasons we might conduct randomization inference. First, it may be because we aren\u2019t working with samples, and since standard errors are often justi\ufb01ed on the grounds that they re\ufb02ect sampling uncertainty, traditional methods may not be as meaningful. The core uncertainty within a causal study is not based on sampling uncertainty, but rather on the fact that we do not know the counterfac- tual [Abadie et al., 2020, 2010]. Second, it may be that we are uncom- fortable appealing to the large sample properties of an estimator in a particular setting, such as when we are working with a small number of treatment units. In such situations, maybe assuming the number of units increases to in\ufb01nity stretches credibility [Buchmueller et al., 2011]. This can be particularly problematic in practice. Young [2019] shows that in \ufb01nite samples, it is common for some observations to experience concentrated leverage. Leverage causes standard errors and estimates to become volatile and can lead to overrejection. Ran- domization inference can be more robust to such outliers. Finally, there seems to be some aesthetic preference for these types of placebo- based inference, as many people \ufb01nd them intuitive.", "start_char_idx": 0, "end_char_idx": 4686, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "96ae19bb-0997-4812-9dd3-540861e336a7": {"__data__": {"id_": "96ae19bb-0997-4812-9dd3-540861e336a7", "embedding": null, "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "81eef3b9-e34d-4b00-8a19-890225e31bcc", "node_type": "4", "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "hash": "8e8d6335ab9eedc454f6ee72e8291ccc11b17c3d8d6cbafa6d4014b4034606b2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "90eaf067-33f6-4902-b983-c897fefb509a", "node_type": "1", "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "hash": "d486dc222a573e976ddb7d1519ca75f24d2cd56ecd652d40bf8c849218e42922", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5b543a75-b019-4279-9d72-786a979d53d7", "node_type": "1", "metadata": {}, "hash": "61d6252a6301efe881cab80ebdc34c5fcd7728d147d18cab503cce8d20421844", "class_name": "RelatedNodeInfo"}}, "text": "Second, it may be that we are uncom- fortable appealing to the large sample properties of an estimator in a particular setting, such as when we are working with a small number of treatment units. In such situations, maybe assuming the number of units increases to in\ufb01nity stretches credibility [Buchmueller et al., 2011]. This can be particularly problematic in practice. Young [2019] shows that in \ufb01nite samples, it is common for some observations to experience concentrated leverage. Leverage causes standard errors and estimates to become volatile and can lead to overrejection. Ran- domization inference can be more robust to such outliers. Finally, there seems to be some aesthetic preference for these types of placebo- based inference, as many people \ufb01nd them intuitive. While this is not a su\ufb03cient reason to adopt a methodological procedure, it is nonethe- less very common to hear someone say that they used randomization inference because it makes sense. I \ufb01gured it was worth mentioning since you\u2019ll likely run into comments like that as well. But before we dig into it, let\u2019s discuss its history, which dates back to Ronald Fisher in the early twentieth century.\nLady tasting tea. Fisher [1935] described a thought experiment in which a woman claims she can discern whether milk or tea was poured \ufb01rst into a cup of tea. While he does not give her name, we now know that the woman in the thought experiment was Muriel Bristol and that the thought experiment in fact did happen.22 Muriel Bristol was a PhD scientist back in the days when women rarely were able to become PhD scientists. One day during afternoon tea, Muriel claimed that\n22 Apparently, Bristol correctly guessed all four cups of tea.\nshe could tell whether the milk was added to the cup before or after the tea. Incredulous, Fisher hastily devised an experiment to test her self-proclaimed talent.\nThe hypothesis, properly stated, is that, given a cup of tea with milk, a woman can discern whether milk or tea was \ufb01rst added to the cup. To test her claim, eight cups of tea were prepared; in four the milk was added \ufb01rst, and in four the tea was added \ufb01rst. How many cups does she have to correctly identify to convince us of her uncanny ability?\nFisher [1935] proposed a kind of permutation-based inference\u2014a method we now call the Fisher\u2019s exact test. The woman possesses the ability probabilistically, not with certainty, if the likelihood of her guessing all four correctly was su\ufb03ciently low. There are 8\u00d77\u00d76\u00d75 = 1, 680 ways to choose a \ufb01rst cup, a second cup, a third cup, and a fourth cup, in order. There are 4 \u00d7 3 \u00d7 2 \u00d7 1 = 24 ways to order four cups. So = 70. Note, the woman performs the experiment by selecting four cups. The\nthe number of ways to choose four cups out of eight is\nprobability that she would correctly identify all four cups is is p = 0.014.\nMaybe you would be more convinced of this method if you could see a simulation, though. So let\u2019s conduct a simple combination exercise. You can with the following code.\n1 clear 2 capture log close 3 4 * Create the data. 4 cups with tea, 4 cups with milk.", "start_char_idx": 3909, "end_char_idx": 7011, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5b543a75-b019-4279-9d72-786a979d53d7": {"__data__": {"id_": "5b543a75-b019-4279-9d72-786a979d53d7", "embedding": null, "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "81eef3b9-e34d-4b00-8a19-890225e31bcc", "node_type": "4", "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "hash": "8e8d6335ab9eedc454f6ee72e8291ccc11b17c3d8d6cbafa6d4014b4034606b2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "96ae19bb-0997-4812-9dd3-540861e336a7", "node_type": "1", "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "hash": "5afca38c7723ca46464c6d53462ba097598005ca4547cd163faa21471024e968", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c4161d9b-ee14-4926-9310-548c93d4db5d", "node_type": "1", "metadata": {}, "hash": "21f14903626c9209d5490b648c2d3d1527293d09a2a8101a8e27726b8200f0e9", "class_name": "RelatedNodeInfo"}}, "text": "There are 8\u00d77\u00d76\u00d75 = 1, 680 ways to choose a \ufb01rst cup, a second cup, a third cup, and a fourth cup, in order. There are 4 \u00d7 3 \u00d7 2 \u00d7 1 = 24 ways to order four cups. So = 70. Note, the woman performs the experiment by selecting four cups. The\nthe number of ways to choose four cups out of eight is\nprobability that she would correctly identify all four cups is is p = 0.014.\nMaybe you would be more convinced of this method if you could see a simulation, though. So let\u2019s conduct a simple combination exercise. You can with the following code.\n1 clear 2 capture log close 3 4 * Create the data. 4 cups with tea, 4 cups with milk. 5 6 set obs 8 7 gen cup = _n 8\n(continued)\nSTATA (continued)\n9 * Assume she guesses the first cup (1), then the second cup (2), and so forth 10 gen guess = 1 in 1 11 replace guess = 2 in 2 12 replace guess = 3 in 3 13 replace guess = 4 in 4 14 replace guess = 0 in 5 15 replace guess = 0 in 6 16 replace guess = 0 in 7 17 replace guess = 0 in 8 18 label variable guess \"1: she guesses tea before milk then stops\" 19 20 tempfile correct 21 save \"`correct'\", replace 22 23 * ssc install percom 24 combin cup, k(4) 25 gen permutation = _n 26 tempfile combo 27 save \"'combo'\", replace 28 29 destring cup*, replace 30 cross using 'correct' 31 sort permutation cup 32 33 gen 34 replace correct = 1 if cup_1 == 1 & cup_2 == 2 & cup_3 == 3 & cup_4 == 4 35 36 * Calculation p-value 37 count if correct==1 38 local correct `r(N)' 39 count 40 local total `r(N)' 41 di 'correct'/`total' 42 gen pvalue = (`correct')/(`total') 43 su pvalue 44 45 * pvalue equals 0.014 46 47 capture log close 48 exit\ncorrect = 0\n1 library(tidyverse) 2 library(utils) 3 4 correct <- tibble( 5 6 7 ) 8 9 combo <- correct %$% as_tibble(t(combn(cup, 4))) %>% 10 11 12 13 mutate(permutation = 1:70) %>% 14 crossing(., correct) %>% 15 arrange(permutation, cup) %>% 16 mutate(correct = case_when(cup_1 == 1 & cup_2 == 2 & 17 18 19 sum(combo$correct == 1) 20 p_value <- sum(combo$correct == 1)/nrow(combo)\nNotice, we get the same answer either way\u20140.014. So, let\u2019s return to Dr. Bristol. Either she has no ability to discriminate the order in which the tea and milk were poured, and therefore chose the correct four cups by random chance, or she (like she said) has the ability to discriminate the order in which ingredients were poured into a drink. Since choosing correctly is highly unlikely (1 chance in 70), it is reasonable to believe she has the talent that she claimed all along that she had.\nSo what exactly have we done? Well, what we have done is provide an exact probability value that the observed phenomenon was merely the product of chance. You can never let the fundamental problem of causal inference get away from you: we never know a causal effect. We only estimate it. And then we rely on other procedures to give us reasons to believe the number we calculated is probably a causal effect. Randomization inference, like all inference, is epistemological scaffolding for a particular kind of belief\u2014speci\ufb01cally, the likelihood\nthat chance created this observed value through a particular kind of procedure.\nBut this example, while it motivated Fisher to develop this method, is not an experimental design wherein causal effects are estimated. So now I\u2019d like to move beyond it. Here, I hope, the randomization infer- ence procedure will become a more interesting and powerful tool for making credible causal statements.\nMethodology of Fisher\u2019s sharp null.", "start_char_idx": 6385, "end_char_idx": 9847, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c4161d9b-ee14-4926-9310-548c93d4db5d": {"__data__": {"id_": "c4161d9b-ee14-4926-9310-548c93d4db5d", "embedding": null, "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "81eef3b9-e34d-4b00-8a19-890225e31bcc", "node_type": "4", "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "hash": "8e8d6335ab9eedc454f6ee72e8291ccc11b17c3d8d6cbafa6d4014b4034606b2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5b543a75-b019-4279-9d72-786a979d53d7", "node_type": "1", "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "hash": "8fa1d1bb61e1a7716442fb588b66634d2fcdcf63708d6108b246ee7bfc1747e2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "afb18742-996d-4b30-9677-dfbbd7f1e195", "node_type": "1", "metadata": {}, "hash": "5f9438687137bc79d1958b9558d220cdf8c973a8c5dfa93c03f0e06c88086c5d", "class_name": "RelatedNodeInfo"}}, "text": "Well, what we have done is provide an exact probability value that the observed phenomenon was merely the product of chance. You can never let the fundamental problem of causal inference get away from you: we never know a causal effect. We only estimate it. And then we rely on other procedures to give us reasons to believe the number we calculated is probably a causal effect. Randomization inference, like all inference, is epistemological scaffolding for a particular kind of belief\u2014speci\ufb01cally, the likelihood\nthat chance created this observed value through a particular kind of procedure.\nBut this example, while it motivated Fisher to develop this method, is not an experimental design wherein causal effects are estimated. So now I\u2019d like to move beyond it. Here, I hope, the randomization infer- ence procedure will become a more interesting and powerful tool for making credible causal statements.\nMethodology of Fisher\u2019s sharp null. Let\u2019s discuss more of what we mean by randomization inference in a context that is easier to understand\u2014 a literal experiment or quasi-experiment. We will conclude with code that illustrates how we might implement it. The main advantage of randomization inference is that it allows us to make probability calcu- lations revealing whether the data are likely a draw from a truly random distribution or not.\nThe methodology can\u2019t be understood without \ufb01rst understanding the concept of Fisher\u2019s sharp null. Fisher\u2019s sharp null is a claim we make wherein no unit in our data, when treated, had a causal effect. While that is a subtle concept and maybe not readily clear, it will be much clearer once we work through some examples. The value of Fisher\u2019s sharp null is that it allows us to make an \u201cexact\u201d inference that does not depend on hypothesized distributions (e.g., Gaussian) or large sample approximations. In this sense, it is nonparametric.23\nSome, when \ufb01rst confronted with the concept of randomization inference, think, \u201cOh, this sounds like bootstrapping,\u201d but the two are in fact completely different. Bootstrapped p-values are random draws from the sample that are then used to conduct inference. This means that bootstrapping is primarily about uncertainty in the observations used in the sample itself. But randomization inference p-values are not about uncertainty in the sample; rather, they are based on uncertainty over which units within a sample are assigned to the treatment itself. let\u2019s break it down into a few methodological steps. You could say that there are six steps to randomization inference: (1) the choice of the sharp null,\nTo help you understand randomization inference,\n23 I simply mean that the inference does not depend on asymptotics or a type of\nSteps to a p value. Fisher and Neyman debated about this \ufb01rst step. Fisher\u2019s \u201csharp\u201d null was the assertion that every single unit had a treat- ment effect of zero, which leads to an easy statement that the ATE is also zero. Neyman, on the other hand, started at the other direction and asserted that there was no average treatment effect, not that each unit had a zero treatment effect. This is an important distinction. To see this, assume that your treatment effect is a 5, but my treatment effect is \u22125. Then the ATE = 0 which was Neyman\u2019s idea. But Fisher\u2019s idea was to say that my treatment effect was zero, and your treatment effect was zero. This is what \u201csharp\u201d means\u2014it means literally that no single unit has a treatment effect. Let\u2019s express this using potential outcomes notation, which can help clarify what I mean.\nNow, it may not be obvious how this is going to help us, but consider this\u2014since we know all observed values, if there is no treatment effect, then we also know each unit\u2019s counterfactual. Let me illustrate my point using the example in Table 15.\nTable 15. Example of made-up data for eight people with missing counterfactuals.\nIf you look closely at Table 15, you will see that for each unit, we only observe one potential outcome. But under the sharp null, we can infer the other missing counterfactual. We only have information on observed outcomes based on the switching equation. So if a unit is treated, we know its Y1 but not its Y0.\nThe second step is the construction of what is called a \u201ctest statis- tic.\u201d What is this? A test statistic t(D, Y) is simply a known, scalar quantity calculated from the treatment assignments and the observed outcomes.", "start_char_idx": 8904, "end_char_idx": 13322, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "afb18742-996d-4b30-9677-dfbbd7f1e195": {"__data__": {"id_": "afb18742-996d-4b30-9677-dfbbd7f1e195", "embedding": null, "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "81eef3b9-e34d-4b00-8a19-890225e31bcc", "node_type": "4", "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "hash": "8e8d6335ab9eedc454f6ee72e8291ccc11b17c3d8d6cbafa6d4014b4034606b2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c4161d9b-ee14-4926-9310-548c93d4db5d", "node_type": "1", "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "hash": "2da864ba47fceaa94bfa6c49ec54f2bda7c2fc3225d77634afb9318cceea5051", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "44c4979b-985c-44f2-b655-e7837bb6ce69", "node_type": "1", "metadata": {}, "hash": "9d5792f59d31dab698dc2b24f8ea2fdd69e60585d6f562d8159538bf0a899375", "class_name": "RelatedNodeInfo"}}, "text": "Let me illustrate my point using the example in Table 15.\nTable 15. Example of made-up data for eight people with missing counterfactuals.\nIf you look closely at Table 15, you will see that for each unit, we only observe one potential outcome. But under the sharp null, we can infer the other missing counterfactual. We only have information on observed outcomes based on the switching equation. So if a unit is treated, we know its Y1 but not its Y0.\nThe second step is the construction of what is called a \u201ctest statis- tic.\u201d What is this? A test statistic t(D, Y) is simply a known, scalar quantity calculated from the treatment assignments and the observed outcomes. It is often simply nothing more than a measurement of the relationship between the Y values by D. In the rest of this section, we will build out a variety of ways that people construct test statistics, but we will start with a fairly straightforward measurement\u2014the simple difference in mean outcome.\nTest statistics ultimately help us distinguish between the sharp null itself and some other hypothesis. And if you want a test statistic with high statistical power, then you need the test statistic to take on \u201cextreme\u201d values (i.e., large in absolute values) when the null is false, and you need for these large values to be unlikely when the null is true.24 As we said, there are a number of ways to estimate a test statistic, and we will be discussing several of them, but let\u2019s start with the simple difference in mean outcomes. The average values for the treatment group are 34/4, the average values for the control group are 30/4, and the difference between these two averages is 1. So given this particular treatment assignment in our sample\u2014the true assignment, mind you\u2014 there is a corresponding test statistic (the simple difference in mean outcomes) that is equal to 1.\nNow, what is implied by Fisher\u2019s sharp null is one of the more inter- esting parts of this method. While historically we do not know each unit\u2019s counterfactual, under the sharp null we do know each unit\u2019s coun- terfactual. How is that possible? Because if none of the units has nonzero treatment effects, then it must be that each counterfactual\n24 It\u2019s kind of interesting what precisely the engine of this method is\u2014it\u2019s actually\nnot designed to pick up small treatment effects because often those small values will\nbe swamped by the randomization process. There\u2019s no philosophical reason to believe,\nthough, that average treatment effects have to be relatively \u201clarge.\u201d It\u2019s just that random- ization inference does require that so as to distinguish the true effect from that of the sharp null.\naccording to Fisher\u2019s sharp null hypothesis.\nNote: Under the sharp null, we can infer the missing counterfactual, which I have represented with bold face.\nis equal to its observed outcome. This means that we can \ufb01ll in those missing counterfactuals with the observed values (Table 16).\nWith these missing counterfactuals replaced by the correspond- ing observed outcome, there\u2019s no treatment effect at the unit level and therefore a zero ATE. So why did we \ufb01nd earlier a simple difference in mean outcomes of 1 if in fact there was no average treatment effect? Simple\u2014it was just noise, pure and simple. It was simply a re\ufb02ec- tion of some arbitrary treatment assignment under Fisher\u2019s sharp null, and through random chance it just so happens that this assignment generated a test statistic of 1.\nSo, let\u2019s summarize. We have a particular treatment assignment and a corresponding test statistic. If we assume Fisher\u2019s sharp null, that test statistic is simply a draw from some random process. And if that\u2019s true, then we can shu\ufb04e the treatment assignment, calculate a new test statistic and ultimately compare this \u201cfake\u201d test statistic with the real one.\nThe key insight of randomization inference is that under the sharp null, the treatment assignment ultimately does not matter. It explicitly assumes as we go from one assignment to another that the counter- factuals aren\u2019t changing\u2014they are always just equal to the observed outcomes. So the randomization distribution is simply a set of all pos- sible test statistics for each possible treatment assignment vector.\nThe third and fourth steps extend this idea by literally shu\ufb04ing the treatment assignment and calculating the unique test statistic for each assignment. And as you do this repeatedly (step 5), in the limit you will eventually cycle through all possible combinations that will yield a distribution of test statistics under the sharp null.\nOnce you have the entire distribution of test statistics, you can cal- culate the exact p-value. How?", "start_char_idx": 12652, "end_char_idx": 17314, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "44c4979b-985c-44f2-b655-e7837bb6ce69": {"__data__": {"id_": "44c4979b-985c-44f2-b655-e7837bb6ce69", "embedding": null, "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "81eef3b9-e34d-4b00-8a19-890225e31bcc", "node_type": "4", "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "hash": "8e8d6335ab9eedc454f6ee72e8291ccc11b17c3d8d6cbafa6d4014b4034606b2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "afb18742-996d-4b30-9677-dfbbd7f1e195", "node_type": "1", "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "hash": "ba495850d706cfe46207c695b7c6a8c8dd9d539f54cf0f8d28b74755b0257e72", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "63f4da4a-0e49-4ec9-a37d-a358a3932413", "node_type": "1", "metadata": {}, "hash": "69cbdf5eaece0c618ad591624ced8a0697947118cc9c9c60e4e00698e6e86449", "class_name": "RelatedNodeInfo"}}, "text": "The key insight of randomization inference is that under the sharp null, the treatment assignment ultimately does not matter. It explicitly assumes as we go from one assignment to another that the counter- factuals aren\u2019t changing\u2014they are always just equal to the observed outcomes. So the randomization distribution is simply a set of all pos- sible test statistics for each possible treatment assignment vector.\nThe third and fourth steps extend this idea by literally shu\ufb04ing the treatment assignment and calculating the unique test statistic for each assignment. And as you do this repeatedly (step 5), in the limit you will eventually cycle through all possible combinations that will yield a distribution of test statistics under the sharp null.\nOnce you have the entire distribution of test statistics, you can cal- culate the exact p-value. How? Simple\u2014you rank these test statistics, \ufb01t the true effect into that ranking, count the number of fake test statis- tics that dominate the real one, and divide that number by all possible combinations. Formally, that would be this:\nAgain, we see what is meant by \u201cexact.\u201d These p-values are exact, not approximations. And with a rejection threshold of \u03b1\u2014for instance, 0.05\u2014then a randomization inference test will falsely reject the sharp null less than 100 \u00d7 \u03b1 percent of the time.\nExample. I think this has been kind of abstract, and when things are abstract, it\u2019s easy to be confused, so let\u2019s work through an example with some new data. Imagine that you work for a homeless shelter with a cognitive behavioral therapy (CBT) program for treating mental illness and substance abuse. You have enough funding to enlist four people into the study, but you have eight residents. Therefore, there are four in treatment and four in control. After concluding the CBT, residents are interviewed to determine the severity of their mental illness symp- toms. The therapist records their mental health on a scale of 0 to 20. With the following information, we can both \ufb01ll in missing counterfac- tuals so as to satisfy Fisher\u2019s sharp null and calculate a corresponding test statistic based on this treatment assignment. Our test statistic will be the absolute value of the simple difference in mean outcomes for simplicity. The test statistic for this particular treatment assignment is simply |34/4 \u2212 30/4| = 8.5 \u2212 7.5 = 1, using the data in Table 17.\nNow we move to the randomization stage. Let\u2019s shu\ufb04e the treat- ment assignment and calculate the new test statistic for that new treatment vector. Table 18 shows this permutation. But \ufb01rst, one thing. We are going to keep the number of treatment units \ufb01xed throughout this example. But if treatment assignment had followed some random\nTable 18. First permutation holding the number of treatment units \ufb01xed.\nprocess, like the Bernoulli, then the number of treatment units would be random and the randomized treatment assignment would be larger than what we are doing here. Which is right? Neither is right in itself. Holding treatment units \ufb01xed is ultimately a re\ufb02ection of whether it had been \ufb01xed in the original treatment assignment. That means that you need to know your data and the process by which units were assigned to treatment to know how to conduct the randomization inference.\nWith this shu\ufb04ing of the treatment assignment, we can calculate a new test statistic, which is |36/4 \u2212 28/4| =9 \u2212 7 = 2. Now before we move on, look at this test statistic: that test statistic of 2 is \u201cfake\u201d because it is not the true treatment assignment. But under the null, the treatment assignment, was already meaningless, since there were no nonzero treatment effects anyway. The point is that even when null\nTable 19. Second permutation holding the number of treatment units \ufb01xed.\nof no effect holds, it can and usually will yield a nonzero effect for no other reason than \ufb01nite sample properties.\nLet\u2019s write that number 2 down and do another permutation, by which I mean, let\u2019s shu\ufb04e the treatment assignment again. Table 19 shows this second permutation, again holding the number of treat- ment units \ufb01xed at four in treatment and four in control.\nThe test statistic associated with this treatment assignment is |36/4 \u2212 27/4| = 9 \u2212 6.75 = 2.25. Again, 2.25 is a draw from a random treatment assignment where each unit has no treatment effect.", "start_char_idx": 16460, "end_char_idx": 20800, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "63f4da4a-0e49-4ec9-a37d-a358a3932413": {"__data__": {"id_": "63f4da4a-0e49-4ec9-a37d-a358a3932413", "embedding": null, "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "81eef3b9-e34d-4b00-8a19-890225e31bcc", "node_type": "4", "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "hash": "8e8d6335ab9eedc454f6ee72e8291ccc11b17c3d8d6cbafa6d4014b4034606b2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "44c4979b-985c-44f2-b655-e7837bb6ce69", "node_type": "1", "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "hash": "6fed0a59b40840870bc948c027caa41a068cbade7e9bbed78e1ddfd96ed5ed53", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ecbfb9b-4afe-4c7a-915e-57c481930f05", "node_type": "1", "metadata": {}, "hash": "0043ec7d8625509d0d81508b1745116a24f2e6c875c8fe003c5dbe9f9f271407", "class_name": "RelatedNodeInfo"}}, "text": "But under the null, the treatment assignment, was already meaningless, since there were no nonzero treatment effects anyway. The point is that even when null\nTable 19. Second permutation holding the number of treatment units \ufb01xed.\nof no effect holds, it can and usually will yield a nonzero effect for no other reason than \ufb01nite sample properties.\nLet\u2019s write that number 2 down and do another permutation, by which I mean, let\u2019s shu\ufb04e the treatment assignment again. Table 19 shows this second permutation, again holding the number of treat- ment units \ufb01xed at four in treatment and four in control.\nThe test statistic associated with this treatment assignment is |36/4 \u2212 27/4| = 9 \u2212 6.75 = 2.25. Again, 2.25 is a draw from a random treatment assignment where each unit has no treatment effect.\nEach time you randomize the treatment assignment, you calculate a test statistic, store that test statistic somewhere, and then go onto the next combination. You repeat this over and over until you have exhausted all possible treatment assignments. Let\u2019s look at the \ufb01rst iterations of this in Table 20.\nThe \ufb01nal step is the calculation of the exact p-value. To do this, we have a couple of options. We can either use software to do it, which is a \ufb01ne way to do it, or we can manually do it ourselves. And for pedagogical reasons, I am partial to doing this manually. So let\u2019s go.\n1 use https://github.com/scunning1975/mixtape/raw/master/ri.dta, clear 2 3 tempfile ri 4 gen id = _n 5 save \"`ri'\", replace 6 7 * Create combinations 8 * ssc install percom 9 combin id, k(4) 10 gen permutation = _n 11 tempfile combo 12 save \"`combo'\", replace 13 14 forvalue i =1/4 { 15 16 } 17 18 19 destring treated*, replace 20 cross using `ri' 21 sort permutation name 22 replace d = 1 if id == treated1 | id == treated2 | id == treated3 | id == treated4 23 replace d = 0 if ~(id == treated1 | id == treated2 | id == treated3 | id == treated4) 24 25 * Calculate true effect using absolute value of SDO 26 egen 27 egen 28 29 collapse (mean) te1 te0, by(permutation) 30 gen 31 keep 32 33 sort ate 34 gen rank = _n 35 su rank if permutation==1 36 gen pvalue = (`r(mean)'/70) 37 list pvalue if permutation==1 38 * pvalue equals 0.6\nren id_`i' treated`i'\nte1 = mean(y) if d==1, by(permutation) te0 = mean(y) if d==0, by(permutation)\n1 library(tidyverse) 2 library(magrittr) 3 library(haven) 4 5 read_data <- function(df) 6 { 7 8 9 10 11 } 12 13 ri <- read_data(\"ri.dta\") %>% 14 mutate(id = c(1:8)) 15 16 treated <- c(1:4) 17 18 combo <- ri %$% as_tibble(t(combn(id, 4))) %>% 19 20 21 22 mutate(permutation = 1:70) %>% 23 24 25 mutate(d = case_when(id == treated1 | id == treated2 | 26 27 28 29 te1 <- combo %>% 30 31 32 33 34 te0 <- combo %>% 35 36 37 38\ndf, sep = \"\")\n(continued)\nR (continued)\n39 n <- nrow(inner_join(te1, te0, by = \"permutation\")) 40 41 p_value <- inner_join(te1, te0, by = \"permutation\") %>% 42 mutate(ate = te1 - te0) %>% 43 44 45 mutate(rank = 1:nrow(.)) %>% 46 filter(permutation == 1) %>% 47 pull(rank)/n\nselect(permutation, ate) %>% arrange(ate) %>%\nThis program was fairly straightforward because the number of possible combinations was so small. Out of eight observations, then four choose eight equals 70.", "start_char_idx": 20005, "end_char_idx": 23214, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ecbfb9b-4afe-4c7a-915e-57c481930f05": {"__data__": {"id_": "6ecbfb9b-4afe-4c7a-915e-57c481930f05", "embedding": null, "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "81eef3b9-e34d-4b00-8a19-890225e31bcc", "node_type": "4", "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "hash": "8e8d6335ab9eedc454f6ee72e8291ccc11b17c3d8d6cbafa6d4014b4034606b2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "63f4da4a-0e49-4ec9-a37d-a358a3932413", "node_type": "1", "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "hash": "329c336a7bcae7699c4e7c8c741cd2a4be177338730211ae0168acd4c682fa78", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e434a416-d6d3-4936-a7c4-47bf549262e2", "node_type": "1", "metadata": {}, "hash": "7d652e4b5a5909d310516c0d3ac8a7fbf40fd98cce75ebaea0bbec802ac25210", "class_name": "RelatedNodeInfo"}}, "text": "%>% 46 filter(permutation == 1) %>% 47 pull(rank)/n\nselect(permutation, ate) %>% arrange(ate) %>%\nThis program was fairly straightforward because the number of possible combinations was so small. Out of eight observations, then four choose eight equals 70. We just had to manipulate the data to get to that point, but once we did, the actual calculation was straighfor- ward. So we can see that the estimated ATE cannot reject the null in the placebo distribution.\nBut often the data sets we work with will be much larger than eight observations. In those situations, we cannot use this method, as the sheer volume of combination grows very fast as n increases. We will hold off for now reviewing this inference method when n is too large until we\u2019ve had a chance to cover more ground.\nOther test statistics. Recall that the second step in this methodology was selection of the test statistic.25 We chose the simple difference in mean outcomes (or the absolute value of such), which is \ufb01ne when effects are additive and there are few outliers in the data. But outliers create problems for that test statistic because of the variation that gets introduced in the randomization distribution. So other alternative test statistics become more attractive.\nOne transformation that handles outliers and skewness more gen- erally is the log transformation. Imbens and Rubin [2015] de\ufb01ne this as the average difference on a log scale by treatment status, or\n25 For more in-depth discussion of the following issues, I highly recommend the\nTable 21. Illustrating ranks using the example data.\nThis makes sense when the raw data is skewed, which happens for positive values like earnings and in instances when treatment effects are multiplicative rather than additive.\nAnother test statistic seen is the absolute value in the difference in quantiles. This also protects against outliers and is represented as\nWe could look at the median, the 25th quantile, the 75th quantile, or anything along the unit interval.\nThe issue of outliers also leads us to consider a test statistic that uses ranks rather than differences. This again is useful when there are large numbers of outliers, when outcomes are continuous or data sets are small. Rank statistics transform outcomes to ranks and then conduct analysis on the ranks themselves. The basic idea is to rank the outcomes and then compare the average rank of the treated and control groups. Let\u2019s illustrate this with an example \ufb01rst (Table 21).\nAs before, we only observe one half of the potential outcomes given the switching equation which assigns potential outcomes to actual outcomes. But under Fisher\u2019s sharp null, we can impute the\nmissing counterfactual so as to ensure no treatment effect. To cal- culate ranks, we simply count the number of units with higher values of Y, including the unit in question. And in instances of ties, we simply take the average over all tied units.\nFor instance, consider Andy. Andy has a value of 10. Andy is as large as himself (1); larger than Ben (2), Daniel (3), Edith (4), Frank (5), and George (6); and tied with Hank (7). Since he is tied with Hank, we average the two, which brings his rank to 6.5. Now consider Ben. Ben has a value of 5. He is as large as himself (1), larger than Daniel (2), and tied with Edith (3). Therefore, we average Edith and himself to get 0.5, bringing us to a rank of 2.\nIt is common, though, to normalize the ranks to have mean 0, which\nis done according to the following formula:\nThis gives us the \ufb01nal column, which we will now use to calculate the test statistic. Let\u2019s use the absolute value of the simple difference in mean outcomes on the normalized rank, which here is\nTo calculate the exact p-value, we would simply conduct the same ran- domization process as earlier, only instead of calculating the simple difference in mean outcomes, we would calculate the absolute value of the simpler difference in mean rank.\nBut all of these test statistics we\u2019ve been discussing have been differences in the outcomes by treatment status. We considered sim- ple differences in averages, simple differences in log averages, differ- ences in quantiles, and differences in ranks. Imbens and Rubin [2015] note that there are shortcomings that come from focusing solely on a few features of the data (e.g., skewness), as it can cause us to miss dif- ferences in other aspects. This speci\ufb01cally can be problematic if the variance in potential outcomes for the treatment group differs from that of the control group.", "start_char_idx": 22958, "end_char_idx": 27477, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e434a416-d6d3-4936-a7c4-47bf549262e2": {"__data__": {"id_": "e434a416-d6d3-4936-a7c4-47bf549262e2", "embedding": null, "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "81eef3b9-e34d-4b00-8a19-890225e31bcc", "node_type": "4", "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "hash": "8e8d6335ab9eedc454f6ee72e8291ccc11b17c3d8d6cbafa6d4014b4034606b2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ecbfb9b-4afe-4c7a-915e-57c481930f05", "node_type": "1", "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "hash": "0a7ef525d3f782edf56ee9f19470c11aef0ed7d87620b75908ef7b18f0610172", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "733067d5-ebd3-4a19-952f-bf27c4b036d2", "node_type": "1", "metadata": {}, "hash": "6c2a67a046d9395f3b580f02042072270a1c83bbc6cad40a227a6dcf62311d7d", "class_name": "RelatedNodeInfo"}}, "text": "Let\u2019s use the absolute value of the simple difference in mean outcomes on the normalized rank, which here is\nTo calculate the exact p-value, we would simply conduct the same ran- domization process as earlier, only instead of calculating the simple difference in mean outcomes, we would calculate the absolute value of the simpler difference in mean rank.\nBut all of these test statistics we\u2019ve been discussing have been differences in the outcomes by treatment status. We considered sim- ple differences in averages, simple differences in log averages, differ- ences in quantiles, and differences in ranks. Imbens and Rubin [2015] note that there are shortcomings that come from focusing solely on a few features of the data (e.g., skewness), as it can cause us to miss dif- ferences in other aspects. This speci\ufb01cally can be problematic if the variance in potential outcomes for the treatment group differs from that of the control group. Focusing only on the simple average dif- ferences we discussed may not generate p-values that are \u201cextreme\u201d enough to reject the null even when the null in fact does not hold. So\nwe may be interested in a test statistic that can detect differences in distributions between the treatment and control units. One such test statistic is the Kolmogorov-Smirnov test statistic (see \ufb01gure 15).\nLet\u2019s \ufb01rst de\ufb01ne the empirical cumulative distribution function\nIf two distributions are the same, then their empirical CDF is the same. But note, empirical CDFs are functions, and test statistics are scalars. So how will we take differences between two functions and turn that into a single scalar quantity? Easy\u2014we will use the maximum difference between the two empirical CDFs. Visually, it will literally be the greatest vertical distance between the two empirical CDFs. That vertical distance will be our test statistic. Formally it is:\nFigure 15. Visualization of distributions by treatment status using a kernel density.\n(continued)\nSTATA (continued)\n37 1 38 1 39 1 40 1 41 1 42 1 43 end 44 45 twoway (kdensity y if d==1) (kdensity y if d==0, lcolor(blue) lwidth(medium)\n(continued)\nR (continued)\n26 kdensity_d0 <- tibble(x = kdensity_d0$x, y = kdensity_d0$y, d = 0) 27 kdensity_d1 <- tibble(x = kdensity_d1$x, y = kdensity_d1$y, d = 1) 28 29 kdensity <- full_join(kdensity_d1, kdensity_d0) 30 kdensity$d <- as_factor(kdensity$d) 31 32 ggplot(kdensity)+ 33 34 35 36\ngeom_point(size = 0.3, aes(x,y, color = d))+ xlim(-7, 8)+ labs(title = \"Kolmogorov-Smirnov Test\")+ scale_color_discrete(labels = c(\"Control\", \"Treatment\"))\nAnd to calculate the p-value, you repeat what we did in earlier examples. Speci\ufb01cally, drop the treatment variable, re-sort the data, reassign new (\ufb01xed) treatment values, calculate TKS, save the coe\ufb03- cient, and repeat a thousand or more times until you have a distribution that you can use to calculate an empirical p-value.\nRandomization inference with large n. What did we do when the number of observations is very large? For instance, Thornton\u2019s total sample was 2,901 participants. Of those, 2,222 received any incentive at all. Wolfram Alpha is an easy to use online calculator for more complicated calculations and easy to use interface. If you go to the website and type \u201c2901 choose 2222\u201d you get the following truncated number of combinations:\nGood luck calculating those combinations. So clearly, exact p- values using all of the combinations won\u2019t work. So instead, we are going estimate approximate p-values. To do that, we will need to ran- domly assign the treatment, estimate a test statistic satisfying the\nsharp null for that sample, repeating that thousands of times, and then calculate the p-value associated with this treatment assignment based on its ranked position in the distribution.", "start_char_idx": 26537, "end_char_idx": 30305, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "733067d5-ebd3-4a19-952f-bf27c4b036d2": {"__data__": {"id_": "733067d5-ebd3-4a19-952f-bf27c4b036d2", "embedding": null, "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "81eef3b9-e34d-4b00-8a19-890225e31bcc", "node_type": "4", "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "hash": "8e8d6335ab9eedc454f6ee72e8291ccc11b17c3d8d6cbafa6d4014b4034606b2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e434a416-d6d3-4936-a7c4-47bf549262e2", "node_type": "1", "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "hash": "4e2b5e6072fe9978b65cd478b226af5e2342b24ca4d02a2fe56cad38b3b9f3c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "40a56963-2e6f-4f30-8808-e31221e3364b", "node_type": "1", "metadata": {}, "hash": "6f9c197add6090fc45debc9ba5f616a2277a839c006f251f6aa9ec890c06d2de", "class_name": "RelatedNodeInfo"}}, "text": "Randomization inference with large n. What did we do when the number of observations is very large? For instance, Thornton\u2019s total sample was 2,901 participants. Of those, 2,222 received any incentive at all. Wolfram Alpha is an easy to use online calculator for more complicated calculations and easy to use interface. If you go to the website and type \u201c2901 choose 2222\u201d you get the following truncated number of combinations:\nGood luck calculating those combinations. So clearly, exact p- values using all of the combinations won\u2019t work. So instead, we are going estimate approximate p-values. To do that, we will need to ran- domly assign the treatment, estimate a test statistic satisfying the\nsharp null for that sample, repeating that thousands of times, and then calculate the p-value associated with this treatment assignment based on its ranked position in the distribution.\n2 3 tempfile hiv 4 save \"`hiv'\", replace 5 6 * Calculate true effect using absolute value of SDO 7 egen te1 = mean(got) if any==1 8 egen te0 = mean(got) if any==0 9 10 collapse (mean) te1 te0 11 gen ate = te1 - te0 12 keep ate 13 gen iteration = 1 14 15 tempfile permute1 16 save \"`permute1'\", replace 17 18 * Create a hundred datasets 19 20 forvalues i = 2/1000 { 21 22 use \"`hiv'\", replace 23 24 drop any 25 set seed `i' 26 gen random_`i' = runiform() 27 sort random_`i' 28 gen one=_n 29 drop random* 30 sort one 31 32 gen 33 replace any = 1 in 1/2222 34\n(continued)\nSTATA (continued)\n35 * Calculate test statistic using absolute value of SDO 36 egen te1 = mean(got) if any==1 37 egen te0 = mean(got) if any==0 38 39 collapse (mean) te1 te0 40 gen ate = te1 - te0 41 keep ate 42 43 gen 44 tempfile permute`i' 45 save \"`permute`i''\", replace 46 47 } 48 49 use \"`permute1'\", replace 50 forvalues i = 2/1000 { 51 52 } 53 54 tempfile final 55 save \"`final'\", replace 56 57 * Calculate exact p-value 58 gsort -ate 59 gen rank = _n 60 su rank if iteration==1 61 gen pvalue = (`r(mean)'/1000) 62 list if iteration==1 63\nappend using \"`permute`i''\"\n1 library(tidyverse) 2 library(haven) 3 4 read_data <- function(df) 5 { 6 7\ndf, sep = \"\")\n(continued)\nR (continued)\n8 9 10 } 11 12 hiv <- read_data(\"thornton_hiv.dta\") 13 14 15 # creating the permutations 16 17 tb <- NULL 18 19 permuteHIV <- function(df, random = TRUE){ 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 } 43 44 permuteHIV(hiv, random = FALSE) 45\nte1 <- tb %>% filter(any == 1) %>% pull(got) %>% mean(na.rm = TRUE)\nte0 <- tb %>% filter(any == 0) %>% pull(got) %>% mean(na.rm = TRUE)\n(continued)\nR (continued)\n52 53 ) 54 55 #calculating the p-value 56 57 permutation <- permutation %>% 58 59 mutate(rank = seq(iterations)) 60 61 p_value <- permutation %>% 62 63\narrange(-ate) %>%\nQuite impressive. Table 22 shows Thornton\u2019s experiment under Fisher\u2019s sharp null with between 100 and 1,000 repeated draws yields highly signi\ufb01cant p-values. In fact, it is always the highest-ranked ATE in a one-tailed test.\nSo what I have done here is obtain an approximation of the p-value associated with our test statistic and the sharp null hypothesis. In prac- tice, if the number of draws is large, the p-value based on this random sample will be fairly accurate [Imbens and Rubin, 2015].", "start_char_idx": 29421, "end_char_idx": 32659, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "40a56963-2e6f-4f30-8808-e31221e3364b": {"__data__": {"id_": "40a56963-2e6f-4f30-8808-e31221e3364b", "embedding": null, "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "81eef3b9-e34d-4b00-8a19-890225e31bcc", "node_type": "4", "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "hash": "8e8d6335ab9eedc454f6ee72e8291ccc11b17c3d8d6cbafa6d4014b4034606b2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "733067d5-ebd3-4a19-952f-bf27c4b036d2", "node_type": "1", "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "hash": "5619ba0d7f520f7a08c9de536952f809e2fab2ec78a9689b54ec33e6279c014e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "248147f2-b673-4b06-8a61-65686e1305ab", "node_type": "1", "metadata": {}, "hash": "72bb053e3be66c0200f8f8220940492fc44c924832b8fef9a59b825024790025", "class_name": "RelatedNodeInfo"}}, "text": "Table 22 shows Thornton\u2019s experiment under Fisher\u2019s sharp null with between 100 and 1,000 repeated draws yields highly signi\ufb01cant p-values. In fact, it is always the highest-ranked ATE in a one-tailed test.\nSo what I have done here is obtain an approximation of the p-value associated with our test statistic and the sharp null hypothesis. In prac- tice, if the number of draws is large, the p-value based on this random sample will be fairly accurate [Imbens and Rubin, 2015]. I wanted to illustrate this randomization method because in reality this is exactly\nTable 22. Estimated p-value using different number of trials.\nwhat you will be doing most of the time since the number of com- binations with any reasonably sized data set will be computationally prohibitive.\nNow, in some ways, this randomization exercise didn\u2019t reveal a whole lot, and that\u2019s probably because Thornton\u2019s original \ufb01ndings were just so precise to begin with (0.4 with a standard error of 0.02). We could throw atom bombs at this result and it won\u2019t go anywhere. But the purpose here is primarily to show its robustness under differ- ent ways of generating those precious p-values, as well as provide you with a map for programming this yourself and for having an arguably separate intuitive way of thinking about signi\ufb01cance itself.\nLeverage. Before we conclude, I\u2019d like to go back to something I said earlier regarding leverage. A recent provocative study by Young [2019] has woken us up to challenges we may face when using traditional inference for estimating the uncertainty of some point estimate, such as robust standard errors. He \ufb01nds practical problems with our tradi- tional forms of inference, which while previously known, had not been made as salient as they were made by his study. The problem that he highlights is one of concentrated leverage. Leverage is a measure of the degree to which a single observation on the right-hand-side vari- able takes on extreme values and is in\ufb02uential in estimating the slope of the regression line. A concentration of leverage in even a few obser- vations can make coe\ufb03cients and standard errors extremely volatile and even bias robust standard errors towards zero, leading to higher rejection rates.\nTo illustrate this problem, Young [2019] went through a simple exercise. He collected over \ufb01fty experimental (lab and \ufb01eld) articles from the American Economic Association\u2019s \ufb02agship journals: Ameri- can Economic Review, American Economic Journal: Applied, and Amer- ican Economic Journal: Economic Policy. He then reanalyzed these papers, using the authors\u2019 models, by dropping one observation or cluster and reestimating the entire model, repeatedly. What he found was shocking:\nWith the removal of just one observation, 35% of 0.01-signi\ufb01cant reported results in the average paper can be rendered insigni\ufb01cant\nat that level. Conversely, 16% of 0.01-insigni\ufb01cant reported results can be found to be signi\ufb01cant at that level. (567)\nFor evidence to be so dependent on just a few observations cre- ates some doubt about the clarity of our work, so what are our alter- natives? The randomization inference method based on Fisher\u2019s sharp null, which will be discussed in this section, can improve upon these problems of leverage, in addition to the aforementioned reasons to consider it. In the typical paper, randomization inference found indi- vidual treatment effects that were 13 to 22 percent fewer signi\ufb01cant results than what the authors\u2019 own analysis had discovered. Random- ization inference, it appears, is somewhat more robust to the presence of leverage in a few observations.\nIn conclusion, we have done a few things in this chapter. We\u2019ve introduced the potential outcomes notation and used it to de\ufb01ne var- ious types of causal effects. We showed that the simple difference in mean outcomes was equal to the sum of the average treatment effect, or the selection bias, and the weighted heterogeneous treat- ment effect bias. Thus the simple difference-in-mean outcomes esti- mator is biased unless those second and third terms zero out. One situation in which they zero out is under independence of the treat- ment, which is when the treatment has been assigned independent of the potential outcomes. When does independence occur? The most commonly confronted situation is under physical randomization of the treatment to the units. Because physical randomization assigns the treatment for reasons that are independent of the potential outcomes, the selection bias zeroes out, as does the heterogeneous treatment effect bias.", "start_char_idx": 32182, "end_char_idx": 36748, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "248147f2-b673-4b06-8a61-65686e1305ab": {"__data__": {"id_": "248147f2-b673-4b06-8a61-65686e1305ab", "embedding": null, "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "81eef3b9-e34d-4b00-8a19-890225e31bcc", "node_type": "4", "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "hash": "8e8d6335ab9eedc454f6ee72e8291ccc11b17c3d8d6cbafa6d4014b4034606b2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "40a56963-2e6f-4f30-8808-e31221e3364b", "node_type": "1", "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "hash": "0a37ce18b176d2c0a1892573d964e7d212d289963b181a0f8a8441ca49ecedd8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "760e4741-bcd4-4cfd-84bf-528fa6a66d64", "node_type": "1", "metadata": {}, "hash": "9537b7308b903dea8dba1be3299ca2d832eb53c31b060ccbd142235ecf9c0457", "class_name": "RelatedNodeInfo"}}, "text": "In conclusion, we have done a few things in this chapter. We\u2019ve introduced the potential outcomes notation and used it to de\ufb01ne var- ious types of causal effects. We showed that the simple difference in mean outcomes was equal to the sum of the average treatment effect, or the selection bias, and the weighted heterogeneous treat- ment effect bias. Thus the simple difference-in-mean outcomes esti- mator is biased unless those second and third terms zero out. One situation in which they zero out is under independence of the treat- ment, which is when the treatment has been assigned independent of the potential outcomes. When does independence occur? The most commonly confronted situation is under physical randomization of the treatment to the units. Because physical randomization assigns the treatment for reasons that are independent of the potential outcomes, the selection bias zeroes out, as does the heterogeneous treatment effect bias. We now move to discuss a second situation where the two terms zero out: conditional independence.\nat that level. Conversely, 16% of 0.01-insigni\ufb01cant reported results can be found to be signi\ufb01cant at that level. (567)\nFor evidence to be so dependent on just a few observations cre- ates some doubt about the clarity of our work, so what are our alter- natives? The randomization inference method based on Fisher\u2019s sharp null, which will be discussed in this section, can improve upon these problems of leverage, in addition to the aforementioned reasons to consider it. In the typical paper, randomization inference found indi- vidual treatment effects that were 13 to 22 percent fewer signi\ufb01cant results than what the authors\u2019 own analysis had discovered. Random- ization inference, it appears, is somewhat more robust to the presence of leverage in a few observations.\nIn conclusion, we have done a few things in this chapter. We\u2019ve introduced the potential outcomes notation and used it to de\ufb01ne var- ious types of causal effects. We showed that the simple difference in mean outcomes was equal to the sum of the average treatment effect, or the selection bias, and the weighted heterogeneous treat- ment effect bias. Thus the simple difference-in-mean outcomes esti- mator is biased unless those second and third terms zero out. One situation in which they zero out is under independence of the treat- ment, which is when the treatment has been assigned independent of the potential outcomes. When does independence occur? The most commonly confronted situation is under physical randomization of the treatment to the units. Because physical randomization assigns the treatment for reasons that are independent of the potential outcomes, the selection bias zeroes out, as does the heterogeneous treatment effect bias. We now move to discuss a second situation where the two terms zero out: conditional independence.\nOne of the main things I wanted to cover in the chapter on directed acylical graphical models was the idea of the backdoor criterion. Speci\ufb01cally, insofar as there exists a conditioning strategy that will sat- isfy the backdoor criterion, then you can use that strategy to identify some causal effect. We now discuss three different kinds of condi- tioning strategies. They are subclassi\ufb01cation, exact matching, and approximate matching.1\nSubclassi\ufb01cation is a method of satisfying the backdoor criterion by weighting differences in means by strata-speci\ufb01c weights. These strata-speci\ufb01c weights will, in turn, adjust the differences in means so that their distribution by strata is the same as that of the counter- factual\u2019s strata. This method implicitly achieves distributional balance between the treatment and control in terms of that known, observable confounder. This method was created by statisticians like Cochran [1968], who tried to analyze the causal effect of smoking on lung can- cer, and while the methods today have moved beyond it, we include\n1 Everything I know about matching I learned from the Northwestern causal infer-\nence workshops in lectures taught by the econometrician Alberto Abadie. I would like to\nacknowledge him as this chapter is heavily indebted to him and those lectures.\nit because some of the techniques implicit in subclassi\ufb01cation are present throughout the rest of the book.\nOne of the concepts threaded through this chapter is the condi- tional independence assumption, or CIA. Sometimes we know that ran- domization occurred only conditional on some observable character- istics. For instance, in Krueger [1999], Tennessee randomly assigned kindergarten students and their teachers to small classrooms, large classrooms, and large classrooms with an aide. But the state did this conditionally\u2014speci\ufb01cally, schools were chosen, and then students were randomized.", "start_char_idx": 35798, "end_char_idx": 40573, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "760e4741-bcd4-4cfd-84bf-528fa6a66d64": {"__data__": {"id_": "760e4741-bcd4-4cfd-84bf-528fa6a66d64", "embedding": null, "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "81eef3b9-e34d-4b00-8a19-890225e31bcc", "node_type": "4", "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "hash": "8e8d6335ab9eedc454f6ee72e8291ccc11b17c3d8d6cbafa6d4014b4034606b2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "248147f2-b673-4b06-8a61-65686e1305ab", "node_type": "1", "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "hash": "5c2650a53c88af5baf07fc209c51182d35b32f212950d52ee4478bf7f24ff80a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b86ab68d-9922-4b5b-9843-e558c00322da", "node_type": "1", "metadata": {}, "hash": "8f8917a39c28c1bb0dc6630eac0e98d427fdc1e9822a799b092fda468e40c4ea", "class_name": "RelatedNodeInfo"}}, "text": "I would like to\nacknowledge him as this chapter is heavily indebted to him and those lectures.\nit because some of the techniques implicit in subclassi\ufb01cation are present throughout the rest of the book.\nOne of the concepts threaded through this chapter is the condi- tional independence assumption, or CIA. Sometimes we know that ran- domization occurred only conditional on some observable character- istics. For instance, in Krueger [1999], Tennessee randomly assigned kindergarten students and their teachers to small classrooms, large classrooms, and large classrooms with an aide. But the state did this conditionally\u2014speci\ufb01cally, schools were chosen, and then students were randomized. Krueger therefore estimated regression models that included a school \ufb01xed effect because he knew that the treatment assignment was only conditionally random.\nThis assumption is written as\nwhere again \u22a5\u22a5 is the notation for statistical independence and X is the variable we are conditioning on. What this means is that the expected values of Y1 and Y0 are equal for treatment and control group for each value of X. Written out, this means:\nLet me link together some concepts. First, insofar as CIA is credible, then CIA means you have found a conditioning strategy that satis\ufb01es the backdoor criterion. Second, when treatment assignment had been conditional on observable variables, it is a situation of selection on observables. The variable X can be thought of as an n \u00d7 k matrix of covariates that satisfy the CIA as a whole.\nSome background. A major public health problem of the mid- to late twentieth century was the problem of rising lung cancer. For instance, the mortality rate per 100,000 from cancer of the lungs in males reached 80\u2013100 per 100,000 by 1980 in Canada, England, and Wales. From 1860 to 1950, the incidence of lung cancer found in cadavers\nduring autopsy grew from 0% to as high as 7%. The rate of lung cancer incidence appeared to be increasing.\nStudies began emerging that suggested smoking was the cause since it was so highly correlated with incidence of lung cancer. For instance, studies found that the relationship between daily smok- ing and lung cancer in males was monotonically increasing in the number of cigarettes a male smoked per day. But some statisticians believed that scientists couldn\u2019t draw a causal conclusion because it was possible that smoking was not independent of potential health outcomes. Speci\ufb01cally, perhaps the people who smoked cigarettes differed from non-smokers in ways that were directly related to the inci- dence of lung cancer. After all, no one is \ufb02ipping coins when deciding to smoke.\nThinking about the simple difference in means decomposition from earlier, we know that contrasting the incidence of lung cancer between smokers and non-smokers will be biased in observational data if the independence assumption does not hold. And because smoking is endogenous\u2014that is, people choose to smoke\u2014it\u2019s entirely possible that smokers differed from the non-smokers in ways that were directly related to the incidence of lung cancer.\nCriticisms at the time came from such prominent statisticians as Joseph Berkson, Jerzy Neyman, and Ronald Fisher. They made several compelling arguments. First, they suggested that the cor- relation was spurious due to a non-random selection of subjects. Functional form complaints were also common. This had to do with people\u2019s use of risk ratios and odds ratios. The association, they argued, was sensitive to those kinds of functional form choices, which is a fair criticism. The arguments were really not so differ- ent from the kinds of arguments you might see today when people are skeptical of a statistical association found in some observational data set.\nProbably most damning, though, was the hypothesis that there existed an unobservable genetic element that both caused people to smoke and independently caused people to develop lung cancer. This confounder meant that smokers and non-smokers differed from one another in ways that were directly related to their potential outcomes,\nand thus independence did not hold. And there was plenty of evidence that the two groups were different. For instance, smokers were more extroverted than non-smokers, and they also differed in age, income, education, and so on.\nThe arguments against the smoking cause mounted. Other criti- cisms included that the magnitudes relating smoking and lung cancer were implausibly large. And again, the ever-present criticism of obser- vational studies: there did not exist any experimental evidence that could incriminate smoking as a cause of lung cancer.2\nThe theory that smoking causes lung cancer is now accepted sci- ence. I wouldn\u2019t be surprised if more people believe in a \ufb02at Earth than that smoking causes lung cancer.", "start_char_idx": 39882, "end_char_idx": 44687, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b86ab68d-9922-4b5b-9843-e558c00322da": {"__data__": {"id_": "b86ab68d-9922-4b5b-9843-e558c00322da", "embedding": null, "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "81eef3b9-e34d-4b00-8a19-890225e31bcc", "node_type": "4", "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "hash": "8e8d6335ab9eedc454f6ee72e8291ccc11b17c3d8d6cbafa6d4014b4034606b2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "760e4741-bcd4-4cfd-84bf-528fa6a66d64", "node_type": "1", "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "hash": "8bd81a3b3e9f82dc4a7b59d6094cd5ec29184732a04ad130b84b4a87df6b5881", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "405f61bc-c0a2-456f-96b7-36932ba297d2", "node_type": "1", "metadata": {}, "hash": "fbb33c36b52d60eacc0341037f63d5ffbeb0d81fdb8aef3443ba0398df680afe", "class_name": "RelatedNodeInfo"}}, "text": "This confounder meant that smokers and non-smokers differed from one another in ways that were directly related to their potential outcomes,\nand thus independence did not hold. And there was plenty of evidence that the two groups were different. For instance, smokers were more extroverted than non-smokers, and they also differed in age, income, education, and so on.\nThe arguments against the smoking cause mounted. Other criti- cisms included that the magnitudes relating smoking and lung cancer were implausibly large. And again, the ever-present criticism of obser- vational studies: there did not exist any experimental evidence that could incriminate smoking as a cause of lung cancer.2\nThe theory that smoking causes lung cancer is now accepted sci- ence. I wouldn\u2019t be surprised if more people believe in a \ufb02at Earth than that smoking causes lung cancer. I can\u2019t think of a more well- known and widely accepted causal theory, in fact. So how did Fisher and others fail to see it? Well, in Fisher\u2019s defense, his arguments were based on sound causal logic. Smoking was endogenous. There was no experimental evidence. The two groups differed considerably on observables. And the decomposition of the simple difference in means shows that contrasts will be biased if there is selection bias. Nonethe- less, Fisher was wrong, and his opponents were right. They just were right for the wrong reasons.\nTo motivate what we\u2019re doing in subclassi\ufb01cation, let\u2019s work with Cochran [1968], which was a study trying to address strange patterns in smoking data by adjusting for a confounder. Cochran lays out mortality rates by country and smoking type (Table 23).\nAs you can see, the highest death rate for Canadians is among the cigar and pipe smokers, which is considerably higher than for non- smokers or for those who smoke cigarettes. Similar patterns show up\n2 But think about the hurdle that the last criticism actually creates. Just imagine\nare assigned to a treatment group (smoker) and control (non-smoker). These people\nmust be dosed with their corresponding treatments long enough for us to observe lung\ncancer develop\u2014so presumably years of heavy smoking. How could anyone ever run\nan experiment like that? Who in their right mind would participate!? Just to describe\nthe idealized experiment is to admit it\u2019s impossible. But how do we answer the causal question without independence (i.e., randomization)?\nSmoking group\nin both countries, though smaller in magnitude than what we see in Canada.\nThis table suggests that pipes and cigars are more dangerous than cigarette smoking, which, to a modern reader, sounds ridiculous. The reason it sounds ridiculous is because cigar and pipe smokers often do not inhale, and therefore there is less tar that accumulates in the lungs than with cigarettes. And insofar as it\u2019s the tar that causes lung cancer, it stands to reason that we should see higher mortality rates among cigarette smokers.\nBut, recall the independence assumption. Do we really believe that:\nIs it the case that factors related to these three states of the world are truly independent to the factors that determine death rates? Well, let\u2019s assume for the sake of argument that these independence assumptions held. What else would be true across these three groups? Well, if the mean potential outcomes are the same for each type of smoking category, then wouldn\u2019t we expect the observable char- acteristics of the smokers themselves to be as well? This connec- tion between the independence assumption and the characteristics of the groups is called balance. If the means of the covariates are the same for each group, then we say those covariates are bal- anced and the two groups are exchangeable with respect to those covariates.\nOne variable that appears to matter is the age of the person. Older people were more likely at this time to smoke cigars and pipes, and\nSmoking group\nwithout stating the obvious, older people were more likely to die. In Table 24 we can see the mean ages of the different groups.\nThe high means for cigar and pipe smokers are probably not terribly surprising. Cigar and pipe smokers are typically older than cigarette smokers, or at least they were in 1968 when Cochran was writing. And since older people die at a higher rate (for reasons other than just smoking cigars), maybe the higher death rate for cigar smokers is because they\u2019re older on average. Furthermore, maybe by the same logic, cigarette smoking has such a low mor- tality rate because cigarette smokers are younger on average. Note, using DAG notation, this simply means that we have the following DAG:\nwhere D is smoking, Y is mortality, and A is age of the smoker.", "start_char_idx": 43824, "end_char_idx": 48511, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "405f61bc-c0a2-456f-96b7-36932ba297d2": {"__data__": {"id_": "405f61bc-c0a2-456f-96b7-36932ba297d2", "embedding": null, "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "81eef3b9-e34d-4b00-8a19-890225e31bcc", "node_type": "4", "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "hash": "8e8d6335ab9eedc454f6ee72e8291ccc11b17c3d8d6cbafa6d4014b4034606b2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b86ab68d-9922-4b5b-9843-e558c00322da", "node_type": "1", "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "hash": "07ccfbbc03323cd19eb649f02e0296c97646dc412bb0ce5ca9d358fe517b6f62", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "21cb16d8-a7e9-4e0d-8a4f-faa13387ea80", "node_type": "1", "metadata": {}, "hash": "f8418955c2834c41b3cba605bdfde980234acf1219424f934d321fcd7447040d", "class_name": "RelatedNodeInfo"}}, "text": "Older people were more likely at this time to smoke cigars and pipes, and\nSmoking group\nwithout stating the obvious, older people were more likely to die. In Table 24 we can see the mean ages of the different groups.\nThe high means for cigar and pipe smokers are probably not terribly surprising. Cigar and pipe smokers are typically older than cigarette smokers, or at least they were in 1968 when Cochran was writing. And since older people die at a higher rate (for reasons other than just smoking cigars), maybe the higher death rate for cigar smokers is because they\u2019re older on average. Furthermore, maybe by the same logic, cigarette smoking has such a low mor- tality rate because cigarette smokers are younger on average. Note, using DAG notation, this simply means that we have the following DAG:\nwhere D is smoking, Y is mortality, and A is age of the smoker. Insofar as CIA is violated, then we have a backdoor path that is open, which also means that we have omitted variable bias. But however we want to describe it, the common thing is that the distribution of age for each group will be different\u2014which is what I mean by covariate imbalance. My \ufb01rst strategy for addressing this problem of covariate imbalance is to condition on age in such a way that the distribution of age is comparable for the treatment and control groups.3\nSo how does subclassi\ufb01cation achieve covariate balance? Our \ufb01rst step is to divide age into strata: say, 20\u201340, 41\u201370, and 71\nidenti\ufb01cation strategy that we will discuss.\nand older. Then we can calculate the mortality rate for some treat- ment group (cigarette smokers) by strata (here, that is age). Next, weight the mortality rate for the treatment group by a strata-speci\ufb01c (or age-speci\ufb01c) weight that corresponds to the control group. This gives us the age-adjusted mortality rate for the treatment group. Let\u2019s explain with an example by looking at Table 25. Assume that age is the only relevant confounder between cigarette smoking and mortality.4\nWhat is the average death rate for pipe smokers without subclassi- \ufb01cation? It is the weighted average of the mortality rate column where\n, and Nt and N are the number of people in each group and the total number of people, respectively. Here that would be\neach weight is equal to\nThat is, the mortality rate of smokers in the population is 29 per 100,000.\nBut notice that the age distribution of cigarette smokers is the exact opposite (by construction) of pipe and cigar smokers. Thus the age distribution is imbalanced. Subclassi\ufb01cation simply adjusts the mortality rate for cigarette smokers so that it has the same age distribution as the comparison group. In other words, we would multi- ply each age-speci\ufb01c mortality rate by the proportion of individuals in that age strata for the comparison group. That would be\n4 A truly hilarious assumption, but this is just illustrative.\nTable 26. Adjusted mortality rates using three age groups [Cochran, 1968].\nSmoking group\nThat is, when we adjust for the age distribution, the age-adjusted mor- tality rate for cigarette smokers (were they to have the same age dis- tribution as pipe and cigar smokers) would be 51 per 100,000\u2014almost twice as large as we got taking a simple na\u00efve calculation unadjusted for the age confounder.\nCochran uses a version of this subclassi\ufb01cation method in his paper and recalculates the mortality rates for the three countries and the three smoking groups (see Table 26). As can be seen, once we adjust for the age distribution, cigarette smokers have the highest death rates among any group.\nThis kind of adjustment raises a question\u2014which variable(s) should we use for adjustment? First, recall what we\u2019ve emphasized repeatedly. Both the backdoor criterion and CIA tell us precisely what we need to do. We need to choose a set of variables that satisfy the backdoor criterion. If the backdoor criterion is met, then all backdoor paths are closed, and if all backdoor paths are closed, then CIA is achieved. We call such a variable the covariate. A covariate is usually a random variable assigned to the individual units prior to treatment. This is sometimes also called exogenous. Harkening back to our DAG chapter, this variable must not be a collider as well.", "start_char_idx": 47641, "end_char_idx": 51885, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "21cb16d8-a7e9-4e0d-8a4f-faa13387ea80": {"__data__": {"id_": "21cb16d8-a7e9-4e0d-8a4f-faa13387ea80", "embedding": null, "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "81eef3b9-e34d-4b00-8a19-890225e31bcc", "node_type": "4", "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "hash": "8e8d6335ab9eedc454f6ee72e8291ccc11b17c3d8d6cbafa6d4014b4034606b2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "405f61bc-c0a2-456f-96b7-36932ba297d2", "node_type": "1", "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "hash": "d0f8a31813b492b6315db3b480278d727c870a22b06b7695b8413cc197d1c2b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e460fcaf-a992-4f29-8166-98955e5462a7", "node_type": "1", "metadata": {}, "hash": "7952e42d70931b3004857d10b016a9d6414684f22fa491e7f8df4836ddff9adf", "class_name": "RelatedNodeInfo"}}, "text": "As can be seen, once we adjust for the age distribution, cigarette smokers have the highest death rates among any group.\nThis kind of adjustment raises a question\u2014which variable(s) should we use for adjustment? First, recall what we\u2019ve emphasized repeatedly. Both the backdoor criterion and CIA tell us precisely what we need to do. We need to choose a set of variables that satisfy the backdoor criterion. If the backdoor criterion is met, then all backdoor paths are closed, and if all backdoor paths are closed, then CIA is achieved. We call such a variable the covariate. A covariate is usually a random variable assigned to the individual units prior to treatment. This is sometimes also called exogenous. Harkening back to our DAG chapter, this variable must not be a collider as well. A variable is exoge- nous with respect to D if the value of X does not depend on the value of D. Oftentimes, though not always and not necessarily, this variable will be time-invariant, such as race. Thus, when trying to adjust for a confounder using subclassi\ufb01cation, rely on a credible DAG to help guide the selection of variables. Remember\u2014your goal is to meet the backdoor criterion.\nIdentifying assumptions. Let me now formalize what we\u2019ve learned. In order to estimate a causal effect when there is a confounder, we need\n(1) CIA and (2) the probability of treatment to be between 0 and 1 for each strata. More formally,\n1. (Y1, Y0) \u22a5\u22a5 D | X (conditional independence) 2. 0 < Pr(D = 1 | X) <1 with probability one (common support)\nThese two assumptions yield the following identity\nwhere each value of Y is determined by the switching equation. Given common support, we get the following estimator:\nWhereas we need treatment to be conditionally independent of both potential outcomes to identify the ATE, we need only treatment to be conditionally independent of Y0 to identify the ATT and the fact that there exist some units in the control group for each treatment strata. Note, the reason for the common support assumption is because we are weighting the data; without common support, we cannot calculate the relevant weights.\nSubclassi\ufb01cation exercise: Titanic data set. For what we are going to do next, I \ufb01nd it useful to move into actual data. We will use an interesting data set to help us better understand subclassi\ufb01cation. As everyone knows, the Titanic ocean cruiser hit an iceberg and sank on its maiden voyage. Slightly more than 700 passengers and crew survived out of the 2,200 people on board. It was a horrible disaster. One of the things about it that was notable, though, was the role that wealth and norms played in passengers\u2019 survival.\nImagine that we wanted to know whether or not being seated in \ufb01rst class made someone more likely to survive. Given that the cruiser contained a variety of levels for seating and that wealth was highly con- centrated in the upper decks, it\u2019s easy to see why wealth might have a leg up for survival. But the problem was that women and children were\nexplicitly given priority for boarding the scarce lifeboats. If women and children were more likely to be seated in \ufb01rst class, then maybe differ- ences in survival by \ufb01rst class is simply picking up the effect of that social norm. Perhaps a DAG might help us here, as a DAG can help us outline the su\ufb03cient conditions for identifying the causal effect of \ufb01rst class on survival.\nNow before we commence, let\u2019s review what this DAG is telling us. This says that being a female made you more likely to be in \ufb01rst class but also made you more likely to survive because lifeboats were more likely to be allocated to women. Furthermore, being a child made you more likely to be in \ufb01rst class and made you more likely to survive. Finally, there are no other confounders, observed or unobserved.5\nHere we have one direct path (the causal effect) between \ufb01rst class (D) and survival (Y) and that\u2019s D \u2192 Y. But, we have two back- door paths. One travels through the variable Child (C): D \u2190 C \u2192 Y; the other travels through the variable Woman (W): D \u2190 W \u2192 Y. Fortunately for us, our data includes both age and gender, so it is possible to close each backdoor path and therefore satisfy the backdoor criterion.", "start_char_idx": 51094, "end_char_idx": 55296, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e460fcaf-a992-4f29-8166-98955e5462a7": {"__data__": {"id_": "e460fcaf-a992-4f29-8166-98955e5462a7", "embedding": null, "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "81eef3b9-e34d-4b00-8a19-890225e31bcc", "node_type": "4", "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "hash": "8e8d6335ab9eedc454f6ee72e8291ccc11b17c3d8d6cbafa6d4014b4034606b2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "21cb16d8-a7e9-4e0d-8a4f-faa13387ea80", "node_type": "1", "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "hash": "3287a4df9957e1191dfdae0707e2c8791b5df6d5d693f43078c4f6d8a5683e18", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea07c07b-5454-40cd-b8e4-6d522296be88", "node_type": "1", "metadata": {}, "hash": "5721686185019f673b39aaf45a606187536e7708e79addbc6f078abc39fd813f", "class_name": "RelatedNodeInfo"}}, "text": "This says that being a female made you more likely to be in \ufb01rst class but also made you more likely to survive because lifeboats were more likely to be allocated to women. Furthermore, being a child made you more likely to be in \ufb01rst class and made you more likely to survive. Finally, there are no other confounders, observed or unobserved.5\nHere we have one direct path (the causal effect) between \ufb01rst class (D) and survival (Y) and that\u2019s D \u2192 Y. But, we have two back- door paths. One travels through the variable Child (C): D \u2190 C \u2192 Y; the other travels through the variable Woman (W): D \u2190 W \u2192 Y. Fortunately for us, our data includes both age and gender, so it is possible to close each backdoor path and therefore satisfy the backdoor criterion. We will use subclassi\ufb01cation to do that, but before we do, let\u2019s calculate a na\u00efve simple difference in outcomes (SDO), which is just\n5 I\u2019m sure you can think of others, though, in which case this DAG is misleading.\n1 use https://github.com/scunning1975/mixtape/raw/master/titanic.dta, clear 2 gen female=(sex==0) 3 label variable female \"Female\" 4 gen male=(sex==1) 5 label variable male \"Male\" 6 gen s=1 if (female==1 & age==1) 7 replace s=2 if (female==1 & age==0) 8 replace s=3 if (female==0 & age==1) 9 replace s=4 if (female==0 & age==0) 10 gen d=1 if class==1 11 replace d=0 if class!=1 12 summarize survived if d==1 13 gen ey1=r(mean) 14 summarize survived if d==0 15 gen ey0=r(mean) 16 gen sdo=ey1-ey0 17 su sdo\n1 library(tidyverse) 2 library(haven) 3 4 read_data <- function(df) 5 { 6 7 8 9 10 } 11 12 titanic <- read_data(\"titanic.dta\") %>% 13 mutate(d = case_when(class == 1 ~ 1, TRUE ~ 0)) 14\ndf, sep = \"\")\n(continued)\nR (continued)\n15 ey1 <- titanic %>% 16 filter(d == 1) %>% 17 pull(survived) %>% 18 mean() 19 20 ey0 <- titanic %>% 21 filter(d == 0) %>% 22 pull(survived) %>% 23 mean() 24 25 sdo <- ey1 - ey0\nUsing the data set on the Titanic, we calculate a simple differ- ence in mean outcomes (SDO), which \ufb01nds that being seated in \ufb01rst class raised the probability of survival by 35.4%. But note, since this does not adjust for observable confounders age and gender, it is a biased estimate of the ATE. So next we use subclassi\ufb01cation weight- ing to control for these confounders. Here are the steps that will entail:\n1. Stratify the data into four groups: young males, young females,\n2. Calculate the difference in survival probabilities for each group. 3. Calculate the number of people in the non-\ufb01rst-class groups and divide by the total number of non-\ufb01rst-class population. These are our strata-speci\ufb01c weights.\n4. Calculate the weighted average survival rate using the strata\nLet\u2019s review this with some code so that you can better understand what these four steps actually entail.", "start_char_idx": 54544, "end_char_idx": 57303, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ea07c07b-5454-40cd-b8e4-6d522296be88": {"__data__": {"id_": "ea07c07b-5454-40cd-b8e4-6d522296be88", "embedding": null, "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "81eef3b9-e34d-4b00-8a19-890225e31bcc", "node_type": "4", "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "hash": "8e8d6335ab9eedc454f6ee72e8291ccc11b17c3d8d6cbafa6d4014b4034606b2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e460fcaf-a992-4f29-8166-98955e5462a7", "node_type": "1", "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "hash": "75ca5329627a99ec2c1131ece5d1ebee2f3dfeef35c0c27cb5e7f42cd0c3f304", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a598177b-2d6f-4508-bf88-f27d648c2172", "node_type": "1", "metadata": {}, "hash": "0835ccb1172463fbe8159e38d5d4763a9e2608ac4b6148fcf510c197a9826104", "class_name": "RelatedNodeInfo"}}, "text": "But note, since this does not adjust for observable confounders age and gender, it is a biased estimate of the ATE. So next we use subclassi\ufb01cation weight- ing to control for these confounders. Here are the steps that will entail:\n1. Stratify the data into four groups: young males, young females,\n2. Calculate the difference in survival probabilities for each group. 3. Calculate the number of people in the non-\ufb01rst-class groups and divide by the total number of non-\ufb01rst-class population. These are our strata-speci\ufb01c weights.\n4. Calculate the weighted average survival rate using the strata\nLet\u2019s review this with some code so that you can better understand what these four steps actually entail.\n1 * Subclassification 2 cap n drop ey1 ey0 3 su survived if s==1 & d==1 4 gen ey11=r(mean) 5 label variable ey11 \"Average survival for male child in treatment\" 6 su survived if s==1 & d==0 7 gen ey10=r(mean) 8 label variable ey10 \"Average survival for male child in control\" 9 gen diff1=ey11-ey10 10 label variable diff1 \"Difference in survival for male children\" 11 su survived if s==2 & d==1 12 gen ey21=r(mean) 13 su survived if s==2 & d==0 14 gen ey20=r(mean) 15 gen diff2=ey21-ey20 16 su survived if s==3 & d==1 17 gen ey31=r(mean) 18 su survived if s==3 & d==0 19 gen ey30=r(mean) 20 gen diff3=ey31-ey30 21 su survived if s==4 & d==1 22 gen ey41=r(mean) 23 su survived if s==4 & d==0 24 gen ey40=r(mean) 25 gen diff4=ey41-ey40 26 count if s==1 & d==0 27 count if s==2 & d==0 28 count if s==3 & d==0 29 count if s==4 & d==0 30 count 31 gen wt1=425/2201 32 gen wt2=45/2201 33 gen wt3=1667/2201 34 gen wt4=64/2201 35 gen wate=diff1*wt1 + diff2*wt2 + diff3*wt3 + diff4*wt4 36 sum wate sdo\n1 library(stargazer) 2 library(magrittr) # for %$% pipes 3 library(tidyverse) 4 library(haven) 5 6 titanic <- read_data(\"titanic.dta\") %>% 7 mutate(d = case_when(class == 1 ~ 1, TRUE ~ 0)) 8 9 10 titanic %<>% 11 mutate(s = case_when(sex == 0 & age == 1 ~ 1, 12 13 14 15 16 17 ey11 <- titanic %>% 18 19 mean(survived) 20 21 ey10 <- titanic %>% 22 23 mean(survived) 24 25 ey21 <- titanic %>% 26 27 mean(survived) 28 29 ey20 <- titanic %>% 30 31 mean(survived) 32 33 ey31 <- titanic %>% 34 35 mean(survived) 36\n(continued)\nR (continued)\n37 ey30 <- titanic %>% 38 39 mean(survived) 40 41 ey41 <- titanic %>% 42 43 mean(survived) 44 45 ey40 <- titanic %>% 46 47 mean(survived) 48 49 diff1 = ey11 - ey10 50 diff2 = ey21 - ey20 51 diff3 = ey31 - ey30 52 diff4 = ey41 - ey40 53 54 obs = nrow(titanic) 55 56 wt1 <- titanic %>% 57 58 59 60 wt2 <- titanic %>% 61 62 63 64 wt3 <- titanic %>% 65 66 67 68 wt4 <- titanic %>% 69 70 71 72 wate = diff1*wt1 + diff2*wt2 + diff3*wt3 + diff4*wt4 73 74 stargazer(wate, sdo, type = \"text\") 75\nHere we \ufb01nd that once we condition on the confounders gender and age, \ufb01rst-class seating has a much lower probability of survival associated with it (though frankly, still large).", "start_char_idx": 56603, "end_char_idx": 59495, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a598177b-2d6f-4508-bf88-f27d648c2172": {"__data__": {"id_": "a598177b-2d6f-4508-bf88-f27d648c2172", "embedding": null, "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "81eef3b9-e34d-4b00-8a19-890225e31bcc", "node_type": "4", "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "hash": "8e8d6335ab9eedc454f6ee72e8291ccc11b17c3d8d6cbafa6d4014b4034606b2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea07c07b-5454-40cd-b8e4-6d522296be88", "node_type": "1", "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "hash": "b9fde3f4ad333a6b3821ac2ebf1958939d582fb9c081334f06e81062d9e24fd1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "37c04e80-b693-4a58-8652-8a639ea99a8f", "node_type": "1", "metadata": {}, "hash": "8dce1791f2cb2a024817ea5ac491a095c3e18254b4b6055c97e79d4143561e0e", "class_name": "RelatedNodeInfo"}}, "text": "The weighted ATE is 16.1%, versus the SDO, which is 35.4%.\nCurse of dimensionality. Here we\u2019ve been assuming two covariates, each of which has two possible set of values. But this was for con- venience. Our data set, for instance, only came to us with two possible values for age\u2014child and adult. But what if it had come to us with multiple values for age, like speci\ufb01c age? Then once we condition on individual age and gender, it\u2019s entirely likely that we will not have the information necessary to calculate differences within strata, and there- fore be unable to calculate the strata-speci\ufb01c weights that we need for subclassi\ufb01cation.\nFor this next part, let\u2019s assume that we have precise data on Titanic survivor ages. But because this will get incredibly laborious, let\u2019s just focus on a few of them.\nHere we see an example of the common support assumption being violated. The common support assumption requires that for each strata, there exist observations in both the treatment and control group, but as you can see, there are not any 12-year-old male passen- gers in \ufb01rst class. Nor are there any 14-year-old male passengers in \ufb01rst class. And if we were to do this for every combination of age and gender, we would \ufb01nd that this problem was quite common. Thus, we cannot estimate the ATE using subclassi\ufb01cation. The problem is that our stratifying variable has too many dimensions, and as a result, we have sparseness in some cells because the sample is too small.\nBut let\u2019s say that the problem was always on the treatment group, not the control group. That is, let\u2019s assume that there is always some- one in the control group for a given combination of gender and age, but there isn\u2019t always for the treatment group. Then we can calculate the ATT. Because as you see in this table, for those two strata, 11-year-olds and 13-year-olds, there are both treatment and control group values for the calculation. So long as there exist controls for a given treat- ment strata, we can calculate the ATT. The equation to do so can be compactly written as:\n(cid:12)\u03b4ATT =\nWe\u2019ve seen a problem that arises with subclassi\ufb01cation\u2014in a \ufb01nite sample, subclassi\ufb01cation becomes less feasible as the number of covariates grows, because as K grows, the data becomes sparse. This is most likely caused by our sample being too small relative to the size of our covariate matrix. We will at some point be missing values, in other words, for those K categories. Imagine if we tried to add a third strata, say, race (black and white). Then we\u2019d have two age categories, two gender categories, and two race categories, giving us eight possi- bilities. In this small sample, we probably will end up with many cells having missing information. This is called the curse of dimensional- ity. If sparseness occurs, it means many cells may contain either only treatment units or only control units, but not both. If that happens, we can\u2019t use subclassi\ufb01cation, because we do not have common support. And therefore we are left searching for an alternative method to satisfy the backdoor criterion.", "start_char_idx": 59496, "end_char_idx": 62573, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "37c04e80-b693-4a58-8652-8a639ea99a8f": {"__data__": {"id_": "37c04e80-b693-4a58-8652-8a639ea99a8f", "embedding": null, "metadata": {"page number": "211 - 219"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7a2d50d1-1130-4536-8364-423c157b6059", "node_type": "4", "metadata": {"page number": "211 - 219"}, "hash": "432c2abdba6de9ecd810cc0879a5d14d40dbe439c87823a328bd85dc17504fa0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a598177b-2d6f-4508-bf88-f27d648c2172", "node_type": "1", "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}, "hash": "91e1d84f8449289f36c55802659906f1a5b72060c4b01bed3814dfe87f2fc741", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2bcd3c56-c03c-4451-ad1a-3440c181186b", "node_type": "1", "metadata": {}, "hash": "cbe10de1feaf356c69b8b88fbbc0034373e4e74240c985700585365880bb4bf3", "class_name": "RelatedNodeInfo"}}, "text": "Exact Matching:\nSubclassi\ufb01cation uses the difference between treatment and con- trol group units and achieves covariate balance by using the K proba- bility weights to weight the averages. It\u2019s a simple method, but it has the aforementioned problem of the curse of dimensionality. And prob- ably, that\u2019s going to be an issue in any research you undertake because it may not be merely one variable you\u2019re worried about but several\u2014in\nBut let\u2019s say that the problem was always on the treatment group, not the control group. That is, let\u2019s assume that there is always some- one in the control group for a given combination of gender and age, but there isn\u2019t always for the treatment group. Then we can calculate the ATT. Because as you see in this table, for those two strata, 11-year-olds and 13-year-olds, there are both treatment and control group values for the calculation. So long as there exist controls for a given treat- ment strata, we can calculate the ATT. The equation to do so can be compactly written as:\n(cid:12)\u03b4ATT =\nWe\u2019ve seen a problem that arises with subclassi\ufb01cation\u2014in a \ufb01nite sample, subclassi\ufb01cation becomes less feasible as the number of covariates grows, because as K grows, the data becomes sparse. This is most likely caused by our sample being too small relative to the size of our covariate matrix. We will at some point be missing values, in other words, for those K categories. Imagine if we tried to add a third strata, say, race (black and white). Then we\u2019d have two age categories, two gender categories, and two race categories, giving us eight possi- bilities. In this small sample, we probably will end up with many cells having missing information. This is called the curse of dimensional- ity. If sparseness occurs, it means many cells may contain either only treatment units or only control units, but not both. If that happens, we can\u2019t use subclassi\ufb01cation, because we do not have common support. And therefore we are left searching for an alternative method to satisfy the backdoor criterion.\nSubclassi\ufb01cation uses the difference between treatment and con- trol group units and achieves covariate balance by using the K proba- bility weights to weight the averages. It\u2019s a simple method, but it has the aforementioned problem of the curse of dimensionality. And prob- ably, that\u2019s going to be an issue in any research you undertake because it may not be merely one variable you\u2019re worried about but several\u2014in\nwhich case, you\u2019ll already be running into the curse. But the thing to emphasize here is that the subclassi\ufb01cation method is using the raw data, but weighting it so as to achieve balance. We are weighting the differences, and then summing over those weighted differences.\nBut there are alternative approaches. For instance, what if we esti- mated(cid:12)\u03b4ATT by imputing the missing potential outcomes by conditioning on the confounding, observed covariate? Speci\ufb01cally, what if we \ufb01lled in the missing potential outcome for each treatment unit using a con- trol group unit that was \u201cclosest\u201d to the treatment group unit for some X confounder? This would give us estimates of all the counterfactuals from which we could simply take the average over the differences. As we will show, this will also achieve covariate balance. This method is called matching.\nThere are two broad types of matching that we will consider: exact matching and approximate matching. We will \ufb01rst start by describing exact matching. Much of what I am going to be discussing is based on Abadie and Imbens [2006].\nA simple matching estimator is the following:\n(cid:12)\u03b4ATT = 1 NT\nwhere Yj(i) is the jth unit matched to the ith unit based on the jth being \u201cclosest to\u201d the ith unit for some X covariate. For instance, let\u2019s say that a unit in the treatment group has a covariate with a value of 2 and we \ufb01nd another unit in the control group (exactly one unit) with a covariate value of 2. Then we will impute the treatment unit\u2019s missing counterfactual with the matched unit\u2019s, and take a difference.\nBut, what if there\u2019s more than one variable \u201cclosest to\u201d the ith unit? For instance, say that the same ith unit has a covariate value of 2 and we \ufb01nd two j units with a value of 2. What can we then do?", "start_char_idx": 0, "end_char_idx": 4228, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2bcd3c56-c03c-4451-ad1a-3440c181186b": {"__data__": {"id_": "2bcd3c56-c03c-4451-ad1a-3440c181186b", "embedding": null, "metadata": {"page number": "211 - 219"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7a2d50d1-1130-4536-8364-423c157b6059", "node_type": "4", "metadata": {"page number": "211 - 219"}, "hash": "432c2abdba6de9ecd810cc0879a5d14d40dbe439c87823a328bd85dc17504fa0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "37c04e80-b693-4a58-8652-8a639ea99a8f", "node_type": "1", "metadata": {"page number": "211 - 219"}, "hash": "8918c3487246cb7e4858586a015dfe66ccce57673522e8dac972a097330aa8d3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "59353d60-5952-4f90-997a-e48e7388a3ad", "node_type": "1", "metadata": {}, "hash": "739e19bd954832bc39eef632778b5e756d86baf742afa9334a1f28749104ebbb", "class_name": "RelatedNodeInfo"}}, "text": "A simple matching estimator is the following:\n(cid:12)\u03b4ATT = 1 NT\nwhere Yj(i) is the jth unit matched to the ith unit based on the jth being \u201cclosest to\u201d the ith unit for some X covariate. For instance, let\u2019s say that a unit in the treatment group has a covariate with a value of 2 and we \ufb01nd another unit in the control group (exactly one unit) with a covariate value of 2. Then we will impute the treatment unit\u2019s missing counterfactual with the matched unit\u2019s, and take a difference.\nBut, what if there\u2019s more than one variable \u201cclosest to\u201d the ith unit? For instance, say that the same ith unit has a covariate value of 2 and we \ufb01nd two j units with a value of 2. What can we then do? Well, one option is to simply take the average of those two units\u2019 Y outcome value. But what if we found 3 close units? What if we found 4? And so on. However many matches M that we \ufb01nd, we would assign the\nNotationally, we can describe this estimator as (cid:16)(cid:9)\n(cid:12)\u03b4ATT = 1 NT\nThis estimator really isn\u2019t too different from the one just before it; the main difference is that this one averages over several close matches as opposed to just picking one. This approach works well when we can \ufb01nd a number of good matches for each treatment group unit. We usually de\ufb01ne M to be small, like M = 2. If M is greater than 2, then we may simply randomly select two units to average outcomes over.\nThose were both ATT estimators. You can tell that these are (cid:12)\u03b4ATT estimators because of the summing over the treatment group.6 But we can also estimate the ATE. But note, when estimating the ATE, we are \ufb01lling in both missing control group units like before and missing treatment group units. If observation i is treated, in other words, then we need to \ufb01ll in the missing Y0 i using the control matches, and if the observation i is a control group unit, then we need to \ufb01ll in the missing Y1 i using the treatment group matches. The estimator is below. It looks scarier than it really is. It\u2019s actually a very compact, nicely-written-out estimator equation.\nThe 2Di \u2212 1 is the nice little trick. When Di = 1, then that leading term becomes a 1.7 And when Di = 0, then that leading term becomes a negative 1, and the outcomes reverse order so that the treatment observation can be imputed. Nice little mathematical form!\nLet\u2019s see this work in action by working with an example. Table 28 shows two samples: a list of participants in a job trainings pro- gram and a list of non-participants, or non-trainees. The left-hand group is the treatment group and the right-hand group is the control group. The matching algorithm that we de\ufb01ned earlier will create a third group called the matched sample, consisting of each treatment\n6 Notice the Di = 1 in the subscript of the summation operator. 7 2 \u00d7 1 \u2212 1 = 1.\nTable 28. Training example with exact matching.\ngroup unit\u2019s matched counterfactual. Here we will match on the age of the participant.\nBefore we do this, though, I want to show you how the ages of the trainees differ on average from the ages of the non-trainees. We can see that in Table 28\u2014the average age of the participants is 24.3 years, and the average age of the non-participants is 31.95 years. Thus, the people in the control group are older, and since wages typically rise with age, we may suspect that part of the reason their average earnings are higher ($11,075 vs. $11,101) is because the control group is older. We say that the two groups are not exchangeable because the covari- ate is not balanced. Let\u2019s look at the age distribution. To illustrate this, we need to download the data \ufb01rst. We will create two histograms\u2014 the distribution of age for treatment and non-trainee group\u2014as well as summarize earnings for each group. That information is also displayed in Figure 16.", "start_char_idx": 3540, "end_char_idx": 7337, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "59353d60-5952-4f90-997a-e48e7388a3ad": {"__data__": {"id_": "59353d60-5952-4f90-997a-e48e7388a3ad", "embedding": null, "metadata": {"page number": "211 - 219"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7a2d50d1-1130-4536-8364-423c157b6059", "node_type": "4", "metadata": {"page number": "211 - 219"}, "hash": "432c2abdba6de9ecd810cc0879a5d14d40dbe439c87823a328bd85dc17504fa0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2bcd3c56-c03c-4451-ad1a-3440c181186b", "node_type": "1", "metadata": {"page number": "211 - 219"}, "hash": "2d71e498521939579f8f279619194d7cf28c46062ac772193daa5216dd52150e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d582b2ca-1e32-4551-8b24-06f85b4dbdd9", "node_type": "1", "metadata": {}, "hash": "d5e0ac775313c4c89ac4f9dc5819ccf45cf68669b3e88c415dbba63808d9be9f", "class_name": "RelatedNodeInfo"}}, "text": "We can see that in Table 28\u2014the average age of the participants is 24.3 years, and the average age of the non-participants is 31.95 years. Thus, the people in the control group are older, and since wages typically rise with age, we may suspect that part of the reason their average earnings are higher ($11,075 vs. $11,101) is because the control group is older. We say that the two groups are not exchangeable because the covari- ate is not balanced. Let\u2019s look at the age distribution. To illustrate this, we need to download the data \ufb01rst. We will create two histograms\u2014 the distribution of age for treatment and non-trainee group\u2014as well as summarize earnings for each group. That information is also displayed in Figure 16.\n2 histogram age_treat, bin(10) frequency 3 histogram age_control, bin(10) frequency 4 su age_treat age_control 5 su earnings_treat earnings_control 6 7 histogram age_treat, bin(10) frequency 8 histogram age_matched, bin(10) frequency 9 su age_treat age_control 10 su earnings_matched earnings_matched\n1 library(tidyverse) 2 library(haven) 3 4 read_data <- function(df) 5 { 6 7 8 9 10 } 11 12 training_example <- read_data(\"training_example.dta\") %>% 13 14 15 ggplot(training_example, aes(x=age_treat)) + 16 17 18 ggplot(training_example, aes(x=age_control)) + 19 geom_histogram(bins = 10, na.rm = TRUE)\nstat_bin(bins = 10, na.rm = TRUE)\ndf, sep = \"\")\nAs you can see from Figure 16, these two populations not only have different means (Table 28); the entire distribution of age across the samples is different. So let\u2019s use our matching algorithm and create the missing counterfactuals for each treatment group unit. This method, since it only imputes the missing units for each treatment unit, will yield an estimate of the (cid:12)\u03b4ATT.\nNow let\u2019s move to creating the matched sample. As this is exact matching, the distance traveled to the nearest neighbor will be zero integers. This won\u2019t always be the case, but note that as the control group sample size grows, the likelihood that we \ufb01nd a unit with the\nTable 29. Training example with exact matching (including matched sample).\nsame covariate value as one in the treatment group grows. I\u2019ve created a data set like this. The \ufb01rst treatment unit has an age of 18. Searching down through the non-trainees, we \ufb01nd exactly one person with an age of 18, and that\u2019s unit 14. So we move the age and earnings information to the new matched sample columns.\nWe continue doing that for all units, always moving the control group unit with the closest value on X to \ufb01ll in the missing counter- factual for each treatment unit. If we run into a situation where there\u2019s more than one control group unit \u201cclose,\u201d then we simply average over them. For instance, there are two units in the non-trainees group with an age of 30, and that\u2019s 10 and 18. So we averaged their earnings and matched that average earnings to unit 10. This is \ufb01lled out in Table 29.\nNow we see that the mean age is the same for both groups. We can also check the overall age distribution (Figure 17). As you can see, the two groups are exactly balanced on age. We might say the two groups are exchangeable. And the difference in earnings between those in the treatment group and those in the control group is $1,695. That is, we estimate that the causal effect of the program was $1,695 in higher earnings.\nLet\u2019s summarize what we\u2019ve learned. We\u2019ve been using a lot of different terms, drawn from different authors and different statistical traditions, so I\u2019d like to map them onto one another. The two groups were different in ways that were likely a direction function of potential outcomes. This means that the independence assumption was vio- lated. Assuming that treatment assignment was conditionally random, then matching on X created an exchangeable set of observations\u2014the matched sample\u2014and what characterized this matched sample was balance.", "start_char_idx": 6609, "end_char_idx": 10503, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d582b2ca-1e32-4551-8b24-06f85b4dbdd9": {"__data__": {"id_": "d582b2ca-1e32-4551-8b24-06f85b4dbdd9", "embedding": null, "metadata": {"page number": "219 - 262"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9fd33664-e6ea-4fe1-ae3b-699b771c4008", "node_type": "4", "metadata": {"page number": "219 - 262"}, "hash": "5a02e3ba8fff6bc12accb0f554c5cbd5e8cdb806355061364a0b698613b869b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "59353d60-5952-4f90-997a-e48e7388a3ad", "node_type": "1", "metadata": {"page number": "211 - 219"}, "hash": "0158c7d4ebbd6fe8199b1ab7e2966f0ef4ef2c6dcdefd2b82ba6d5f1bf75732e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b6ec7c3d-3467-4c30-936f-3c0cf47b3100", "node_type": "1", "metadata": {}, "hash": "5251fa387a979659674ff500db4584cbfc7144285067e629fd87d73428039260", "class_name": "RelatedNodeInfo"}}, "text": "Approximate Matching:\nThe previous example of matching was relatively simple\u2014\ufb01nd a unit or collection of units that have the same value of some covariate X and substitute their outcomes as some unit j\u2019s counterfactuals. Once you\u2019ve done that, average the differences for an estimate of the ATE.\nNow we see that the mean age is the same for both groups. We can also check the overall age distribution (Figure 17). As you can see, the two groups are exactly balanced on age. We might say the two groups are exchangeable. And the difference in earnings between those in the treatment group and those in the control group is $1,695. That is, we estimate that the causal effect of the program was $1,695 in higher earnings.\nLet\u2019s summarize what we\u2019ve learned. We\u2019ve been using a lot of different terms, drawn from different authors and different statistical traditions, so I\u2019d like to map them onto one another. The two groups were different in ways that were likely a direction function of potential outcomes. This means that the independence assumption was vio- lated. Assuming that treatment assignment was conditionally random, then matching on X created an exchangeable set of observations\u2014the matched sample\u2014and what characterized this matched sample was balance.\nThe previous example of matching was relatively simple\u2014\ufb01nd a unit or collection of units that have the same value of some covariate X and substitute their outcomes as some unit j\u2019s counterfactuals. Once you\u2019ve done that, average the differences for an estimate of the ATE.\nBut what if you couldn\u2019t \ufb01nd another unit with that exact same\nvalue? Then you\u2019re in the world of approximate matching.\nNearest neighbor covariate matching. One of the instances where exact matching can break down is when the number of covariates, K, grows large. And when we have to match on more than one variable but are not using the sub-classi\ufb01cation approach, then one of the \ufb01rst things we confront is the concept of distance. What does it mean for one unit\u2019s covariate to be \u201cclose\u201d to someone else\u2019s? Furthermore, what does it mean when there are multiple covariates with measurements in multiple dimensions?\nMatching on a single covariate is straightforward because dis- tance is measured in terms of the covariate\u2019s own values. For instance, distance in age is simply how close in years or months or days one per- son is to another person. But what if we have several covariates needed for matching? Say, age and log income. A 1-point change in age is very different from a 1-point change in log income, not to mention that we are now measuring distance in two, not one, dimensions. When the number of matching covariates is more than one, we need a new def- inition of distance to measure closeness. We begin with the simplest measure of distance, the Euclidean distance:\nThe problem with this measure of distance is that the distance mea- sure itself depends on the scale of the variables themselves. For this reason, researchers typically will use some modi\ufb01cation of the Euclidean distance, such as the normalized Euclidean distance, or they\u2019ll use a wholly different alternative distance. The normalized Euclidean distance is a commonly used distance, and what makes it different is that the distance of each variable is scaled by the variable\u2019s variance. The distance is measured as:\nNotice that the normalized Euclidean distance is equal to:\n(Xni \u2212 Xnj) (cid:12)\u03c3 2 n\nThus, if there are changes in the scale of X, these changes also affect its variance, and so the normalized Euclidean distance does not change.\nmalized Euclidean distance measure, metric. It is:\nFinally, there is the Mahalanobis distance, which like the nor- is a scale-invariant distance\nwhere (cid:12)(cid:7)X is the sample variance-covariance matrix of X.\nBasically, more than one covariate creates a lot of headaches. Not only does it create the curse-of-dimensionality problem; it also makes measuring distance harder. All of this creates some challenges for \ufb01nding a good match in the data. As you can see in each of these distance formulas, there are sometimes going to be matching dis- crepancies. Sometimes Xi (cid:9)= Xj. What does this mean? It means that some unit i has been matched with some unit j on the basis of a sim- ilar covariate value of X = x. Maybe unit i has an age of 25, but unit j has an age of 26. Their difference is 1. Sometimes the discrepan- cies are small, sometimes zero, sometimes large.", "start_char_idx": 0, "end_char_idx": 4447, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b6ec7c3d-3467-4c30-936f-3c0cf47b3100": {"__data__": {"id_": "b6ec7c3d-3467-4c30-936f-3c0cf47b3100", "embedding": null, "metadata": {"page number": "219 - 262"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9fd33664-e6ea-4fe1-ae3b-699b771c4008", "node_type": "4", "metadata": {"page number": "219 - 262"}, "hash": "5a02e3ba8fff6bc12accb0f554c5cbd5e8cdb806355061364a0b698613b869b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d582b2ca-1e32-4551-8b24-06f85b4dbdd9", "node_type": "1", "metadata": {"page number": "219 - 262"}, "hash": "2a1b7b41125ff3ce41e464d6581fa5362431f27257dd2b0ef8ee218bf46067cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd0e0e8a-1fba-4d5a-ad7f-af50ba2c1e87", "node_type": "1", "metadata": {}, "hash": "3a280072b408350e4cacd7eb932c89371e40b57cbfde5255120490566b9d41cd", "class_name": "RelatedNodeInfo"}}, "text": "Not only does it create the curse-of-dimensionality problem; it also makes measuring distance harder. All of this creates some challenges for \ufb01nding a good match in the data. As you can see in each of these distance formulas, there are sometimes going to be matching dis- crepancies. Sometimes Xi (cid:9)= Xj. What does this mean? It means that some unit i has been matched with some unit j on the basis of a sim- ilar covariate value of X = x. Maybe unit i has an age of 25, but unit j has an age of 26. Their difference is 1. Sometimes the discrepan- cies are small, sometimes zero, sometimes large. But, as they move away from zero, they become more problematic for our estimation and introduce bias.\nHow severe is this bias? First, the good news. What we know is that the matching discrepancies tend to converge to zero as the sample size increases\u2014which is one of the main reasons that approx- imate matching is so data greedy. It demands a large sample size for the matching discrepancies to be trivially small. But what if there are many covariates? The more covariates, the longer it takes for\nthat convergence to zero to occur. Basically, if it\u2019s hard to \ufb01nd good matches with an X that has a large dimension, then you will need a lot of observations as a result. The larger the dimension, the greater likelihood of matching discrepancies, and the more data you need. So you can take that to the bank\u2014most likely, your matching prob- lem requires a large data set in order to minimize the matching discrepancies.\nBias correction. Speaking of matching discrepancies, what sorts of options are available to us, putting aside seeking a large data set with lots of controls? Well, enter stage left, Abadie and Imbens [2011], who introduced bias-correction techniques with matching estimators when there are matching discrepancies in \ufb01nite samples. So let\u2019s look at that more closely, as you\u2019ll likely need this in your work.\nEverything we\u2019re getting at is suggesting that matching is biased because of these poor matching discrepancies. So let\u2019s derive this bias. First, we write out the sample ATT estimate, and then we subtract out the true ATT. So:\n(cid:12)\u03b4ATT = 1 NT\nwhere each i and j(i) units are matched, Xi \u2248 Xj(i) and Dj(i) = 0. Next we de\ufb01ne the conditional expection outcomes\nNotice, these are just the expected conditional outcome functions based on the switching equation for both control and treatment groups.\nAs always, we write out the observed value as a function of\nexpected conditional outcomes and some stochastic element:\nYi = \u03bcDi (Xi) + \u03b5i\nNow rewrite the ATT estimator using the above \u03bc terms:\n(cid:12)\u03b4ATT = 1 NT\n\u03bc1(Xi) + \u03b5i\nNotice, the \ufb01rst line is just the ATT with the stochastic element included from the previous line. And the second line rearranges it so that we get two terms: the estimated ATT plus the average difference in the stochastic terms for the matched sample.\nNow we compare this estimator with the true value of ATT.\n(cid:12)\u03b4ATT \u2212 \u03b4ATT = 1 NT\nwhich, with some simple algebraic manipulation is:\n(cid:12)\u03b4ATT \u2212 \u03b4ATT = 1 NT\nApplying the central limit theorem and the difference, converges to a normal distribution with zero mean. But:\nNT((cid:12)\u03b4ATT \u2212 \u03b4ATT)\nNT((cid:12)\u03b4ATT \u2212 \u03b4ATT)\nNow consider the implications if the number of covariates is large. First, the difference between Xi and Xj(i) converges to zero slowly. This therefore makes the difference \u03bc0(Xi) \u2212 \u03bc(Xj(i)) converge to zero very slowly. Third, E[ NT(\u03bc0(Xi) \u2212 \u03bc0(Xj(i))) | D = 1] may not converge to \u221a zero. And fourth, E[\nNT((cid:12)\u03b4ATT \u2212 \u03b4ATT)] may not converge to zero.\nAs you can see, the bias of the matching estimator can be severe depending on the magnitude of these matching discrepancies. How- ever, one piece of good news is that these discrepancies are observed. We can see the degree to which each unit\u2019s matched sample has\nTable 30. Another matching example (this time to illustrate bias correction).\nsevere mismatch on the covariates themselves.", "start_char_idx": 3846, "end_char_idx": 7819, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cd0e0e8a-1fba-4d5a-ad7f-af50ba2c1e87": {"__data__": {"id_": "cd0e0e8a-1fba-4d5a-ad7f-af50ba2c1e87", "embedding": null, "metadata": {"page number": "219 - 262"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9fd33664-e6ea-4fe1-ae3b-699b771c4008", "node_type": "4", "metadata": {"page number": "219 - 262"}, "hash": "5a02e3ba8fff6bc12accb0f554c5cbd5e8cdb806355061364a0b698613b869b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b6ec7c3d-3467-4c30-936f-3c0cf47b3100", "node_type": "1", "metadata": {"page number": "219 - 262"}, "hash": "87192cce0712c6ca22ae6f7694046883153d47b3b2332ce2a32d3a7d79782d79", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f054d891-0ddf-4d24-984d-0388df46402b", "node_type": "1", "metadata": {}, "hash": "ec5a47d0a77c2d0806051b8b4ff991296a836befe6c53df426e5736d51fed920", "class_name": "RelatedNodeInfo"}}, "text": "First, the difference between Xi and Xj(i) converges to zero slowly. This therefore makes the difference \u03bc0(Xi) \u2212 \u03bc(Xj(i)) converge to zero very slowly. Third, E[ NT(\u03bc0(Xi) \u2212 \u03bc0(Xj(i))) | D = 1] may not converge to \u221a zero. And fourth, E[\nNT((cid:12)\u03b4ATT \u2212 \u03b4ATT)] may not converge to zero.\nAs you can see, the bias of the matching estimator can be severe depending on the magnitude of these matching discrepancies. How- ever, one piece of good news is that these discrepancies are observed. We can see the degree to which each unit\u2019s matched sample has\nTable 30. Another matching example (this time to illustrate bias correction).\nsevere mismatch on the covariates themselves. Second, we can always make the matching discrepancy small by using a large donor pool of untreated units to select our matches, because recall, the like- lihood of \ufb01nding a good match grows as a function of the sample size, and so if we are content to estimating the ATT, then increasing the size of the donor pool can get us out of this mess. But let\u2019s say we can\u2019t do that and the matching discrepancies are large. Then we can apply bias-correction methods to minimize the size of the bias. So let\u2019s see what the bias-correction method looks like. This is based on Abadie and Imbens [2011].\nNote that the total bias is made up of the bias associated with each individual unit i. Thus, each treated observation contributes \u03bc0(Xi) \u2212 \u03bc0(Xj(i)) to the overall bias. The bias-corrected matching is the following estimator:\n(cid:12)\u03b4BC ATT\nwhere (cid:12)\u03bc0(X) is an estimate of E[Y | X = x, D = 0] using, for example, OLS. Again, I \ufb01nd it always helpful if we take a crack at these estimators with concrete data. Table 30 contains more make-believe data for eight units, four of whom are treated and the rest of whom are functioning as controls. According to the switching equation, we only observe the actual outcomes associated with the potential outcomes under treat- ment or control, which means we\u2019re missing the control values for our treatment group.\nTable 31. Nearest-neighbor matched sample.\nNotice in this example that we cannot implement exact matching because none of the treatment group units has an exact match in the control group. It\u2019s worth emphasizing that this is a consequence of \ufb01nite samples; the likelihood of \ufb01nding an exact match grows when the sample size of the control group grows faster than that of the treatment group. Instead, we use nearest-neighbor matching, which is simply going to match each treatment unit to the control group unit whose covariate value is nearest to that of the treatment group unit itself. But, when we do this kind of matching, we necessarily create matching discrepancies, which is simply another way of saying that the covariates are not perfectly matched for every unit. Nonetheless, the nearest-neighbor \u201calgorithm\u201d creates Table 31.\nWith the bias correction, we need to estimate (cid:12)\u03bc0(X). We\u2019ll use OLS. It should be clearer what (cid:12)\u03bc0(X) is. It is is the \ufb01tted values from a regres- sion of Y on X. Let\u2019s illustrate this using the data set shown in Table 31.\n(cid:12)\u2192 2 reg Y X 3 gen muhat = _b[_cons] + _b[X]*X 4 list\n1 library(tidyverse) 2 library(haven) 3 4 read_data <- function(df) 5 { 6 7 8 9 10 } 11 12 training_bias_reduction <- read_data(\"training_bias_reduction.dta\") %>% 13 mutate( 14 15 16 17 train_reg <- lm(Y ~ X, training_bias_reduction) 18 19 training_bias_reduction <- training_bias_reduction %>% 20 mutate(u_hat0 = predict(train_reg))\ndf, sep = \"\")\nWhen we regress Y onto X and D, we get the following estimated\nThis gives us the outcomes, treatment status, and predicted values in Table 32.\nAnd then this would be done for the other three simple differences, each of which is added to a bias-correction term based on the \ufb01tted values from the covariate values.\nNow, care must be given when using the \ufb01tted values for bias cor- rection, so let me walk you through it.", "start_char_idx": 7144, "end_char_idx": 11076, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f054d891-0ddf-4d24-984d-0388df46402b": {"__data__": {"id_": "f054d891-0ddf-4d24-984d-0388df46402b", "embedding": null, "metadata": {"page number": "219 - 262"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9fd33664-e6ea-4fe1-ae3b-699b771c4008", "node_type": "4", "metadata": {"page number": "219 - 262"}, "hash": "5a02e3ba8fff6bc12accb0f554c5cbd5e8cdb806355061364a0b698613b869b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd0e0e8a-1fba-4d5a-ad7f-af50ba2c1e87", "node_type": "1", "metadata": {"page number": "219 - 262"}, "hash": "c86db9a3118952bcf3613991e87e903db684ecd49cffe82ab264483e65cb745e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d7e0001-caaa-4e4e-9a63-045bfcb861a7", "node_type": "1", "metadata": {}, "hash": "ea0adab0230451a95484336ebd965605dd5e98aa5e25c8248390b84af3a80b25", "class_name": "RelatedNodeInfo"}}, "text": "And then this would be done for the other three simple differences, each of which is added to a bias-correction term based on the \ufb01tted values from the covariate values.\nNow, care must be given when using the \ufb01tted values for bias cor- rection, so let me walk you through it. You are still going to be taking the simple differences (e.g., 5 \u2013 4 for row 1), but now you will also sub- tract out the \ufb01tted values associated with each observation\u2019s unique covariate. So for instance, in row 1, the outcome 5 has a covariate of\nTable 32. Nearest-neighbor matched sample with \ufb01tted values for bias correction.\n11, which gives it a \ufb01tted value of 3.89, but the counterfactual has a value of 10, which gives it a predicted value of 3.94. So therefore we would use the following bias correction:\n(cid:12)\u03b4BC ATT\nNow that we see how a speci\ufb01c \ufb01tted value is calculated and how it contributes to the calculation of the ATT, let\u2019s look at the entire calculation now.\n(cid:12)\u03b4BC ATT\nwhich is slightly higher than the unadjusted ATE of 3.25. Note that this bias-correction adjustment becomes more signi\ufb01cant as the matching discrepancies themselves become more common. But, if the matching discrepancies are not very common in the \ufb01rst place, then by de\ufb01nition, bias adjustment doesn\u2019t change the estimated parameter very much. Bias arises because of the effect of large matching discrepancies. To minimize these discrepancies, we need a small number of M (e.g., M = 1). Larger values of M produce large matching discrepancies. Second, we need matching with replacement. Because matching\nwith replacement can use untreated units as a match more than once, matching with replacement produces smaller discrepancies. And \ufb01nally, try to match covariates with a large effect on \u03bc0(.).\nThe matching estimators have a normal distribution in large sam- ples provided that the bias is small. For matching without replacement, the usual variance estimator is valid. That is:\nFor matching with replacement:\nwhere Ki is the number of times that observation i is used as a match. Then )var(Yi | Xi, Di = 0) can be estimated by matching. For example, take two observations with Di = Dj = 0 and Xi \u2248 Xj:\nis an unbiased estimator of )var(\u03b5i | Xi, Di = 0). The bootstrap, though, doesn\u2019t create valid standard errors [Abadie and Imbens, 2008].\nPropensity score methods. There are several ways of achieving the conditioning strategy implied by the backdoor criterion, and we\u2019ve dis- cussed several. But one popular one was developed by Donald Rubin in the mid-1970s to early 1980s called the propensity score method [Rosenbaum and Rubin, 1983; Rubin, 1977]. The propensity score is similar in many respects to both nearest-neighbor covariate matching by Abadie and Imbens [2006] and subclassi\ufb01cation. It\u2019s a very popular method, particularly in the medical sciences, of addressing selection on observables, and it has gained some use among economists as well [Dehejia and Wahba, 2002].\nBefore we dig into it, though, a couple of words to help man- age expectations. Despite some early excitement caused by Dehejia\nand Wahba [2002], subsequent enthusiasm was more tempered [King and Nielsen, 2019; Smith and Todd, 2001, 2005]. As such, propensity score matching has not seen as wide adoption among economists as in other nonexperimental methods like regression discontinuity or difference-in-differences. The most common reason given for this is that economists are oftentimes skeptical that CIA can be achieved in any data set\u2014almost as an article of faith. This is because for many applications, economists as a group are usually more concerned about selection on unobservables than they are selection on observ- ables, and as such, they reach for matching methods less often. But I am agnostic as to whether CIA holds or doesn\u2019t hold in your particu- lar application. There\u2019s no theoretical reason to dismiss a procedure designed to estimate causal effects on some ad hoc principle one holds because of a hunch. Only prior knowledge and deep familiarity with the institutional details of your application can tell you what the appropriate identi\ufb01cation strategy is, and insofar as the backdoor crite- rion can be met, then matching methods may be perfectly appropriate.", "start_char_idx": 10801, "end_char_idx": 15042, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3d7e0001-caaa-4e4e-9a63-045bfcb861a7": {"__data__": {"id_": "3d7e0001-caaa-4e4e-9a63-045bfcb861a7", "embedding": null, "metadata": {"page number": "219 - 262"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9fd33664-e6ea-4fe1-ae3b-699b771c4008", "node_type": "4", "metadata": {"page number": "219 - 262"}, "hash": "5a02e3ba8fff6bc12accb0f554c5cbd5e8cdb806355061364a0b698613b869b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f054d891-0ddf-4d24-984d-0388df46402b", "node_type": "1", "metadata": {"page number": "219 - 262"}, "hash": "35145e0ba58f0130598839b2b3414c4b9f8f731add484bbb1e6690423aefb083", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "47ff8b58-7f29-45b3-82a1-5fcf6ec0deda", "node_type": "1", "metadata": {}, "hash": "2ee620f7b085c60f10d0b920a39fc0d37345f164917a439c6034def12b93d730", "class_name": "RelatedNodeInfo"}}, "text": "The most common reason given for this is that economists are oftentimes skeptical that CIA can be achieved in any data set\u2014almost as an article of faith. This is because for many applications, economists as a group are usually more concerned about selection on unobservables than they are selection on observ- ables, and as such, they reach for matching methods less often. But I am agnostic as to whether CIA holds or doesn\u2019t hold in your particu- lar application. There\u2019s no theoretical reason to dismiss a procedure designed to estimate causal effects on some ad hoc principle one holds because of a hunch. Only prior knowledge and deep familiarity with the institutional details of your application can tell you what the appropriate identi\ufb01cation strategy is, and insofar as the backdoor crite- rion can be met, then matching methods may be perfectly appropriate. And if it cannot, then matching is inappropriate. But then, so is a na\u00efve multivariate regression in such cases.\nWe\u2019ve mentioned that propensity score matching is an application used when a conditioning strategy can satisfy the backdoor criterion. But how exactly is it implemented? Propensity score matching takes those necessary covariates, estimates a maximum likelihood model of the conditional probability of treatment (usually a logit or probit so as to ensure that the \ufb01tted values are bounded between 0 and 1), and uses the predicted values from that estimation to collapse those covariates into a single scalar called the propensity score. All compar- isons between the treatment and control group are then based on that value.\nThere is some subtlety to the propensity score in practice, though. Consider this scenario: two units, A and B, are assigned to treatment and control, respectively. But their propensity score is 0.6. Thus, they had the same 60% conditional probability of being assigned to treat- ment, but by random chance, A was assigned to treatment and B was assigned to control. The idea with propensity score methods is to com- pare units who, based on observables, had very similar probabilities\nof being placed into the treatment group even though those units dif- fered with regard to actual treatment assignment. If conditional on X, two units have the same probability of being treated, then we say they have similar propensity scores, and all remaining variation in treatment assignment is due to chance. And insofar as the two units A and B have the same propensity score of 0.6, but one is the treatment group and one is not, and the conditional independence assumption credibly holds in the data, then differences between their observed outcomes are attributable to the treatment.\nImplicit in that example, though, we see another assumption needed for this procedure, and that\u2019s the common support assumption. Common support simply requires that there be units in the treatment and control group across the estimated propensity score. We had com- mon support for 0.6 because there was a unit in the treatment group (A) and one in the control group (B) for 0.6. In ways that are connected to this, the propensity score can be used to check for covariate bal- ance between the treatment group and control group such that the two groups become observationally equivalent. But before walking through an example using real data, let\u2019s review some papers that use it.8\nExample: The NSW job training program. The National Supported Work Demonstration (NSW) job-training program was operated by the Man- power Demonstration Research Corp (MRDC) in the mid-1970s. The NSW was a temporary employment program designed to help disad- vantaged workers lacking basic job skills move into the labor market by giving them work experience and counseling in a sheltered environ- ment. It was also unique in that it randomly assigned quali\ufb01ed appli- cants to training positions. The treatment group received all the ben- e\ufb01ts of the NSW program. The controls were basically left to fend for themselves. The program admitted women receiving Aid to Families\n8 I cannot emphasize this enough\u2014this method, like regression more generally,\nonly has value for your project if you can satisfy the backdoor criterion by conditioning on X. If you cannot satisfy the backdoor criterion in your data, then the propensity score does not assist you in identifying a causal effect. At best, it helps you better understand\nissues related to balance on observables (but not unobservables). It is absolutely critical that your DAG be, in other words, credible, defensible, and accurate, as you depend on\nthose theoretical relationships to design the appropriate identi\ufb01cation strategy.\nwith Dependent Children, recovering addicts, released offenders, and men and women of both sexes who had not completed high school.", "start_char_idx": 14175, "end_char_idx": 18958, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "47ff8b58-7f29-45b3-82a1-5fcf6ec0deda": {"__data__": {"id_": "47ff8b58-7f29-45b3-82a1-5fcf6ec0deda", "embedding": null, "metadata": {"page number": "219 - 262"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9fd33664-e6ea-4fe1-ae3b-699b771c4008", "node_type": "4", "metadata": {"page number": "219 - 262"}, "hash": "5a02e3ba8fff6bc12accb0f554c5cbd5e8cdb806355061364a0b698613b869b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d7e0001-caaa-4e4e-9a63-045bfcb861a7", "node_type": "1", "metadata": {"page number": "219 - 262"}, "hash": "1f1fe9b2efd198fcd7f18aa50c5c0e30a57f4084da6e65cb036053f6ddc78ed3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "85654770-e510-4a32-824b-64794e31ba4b", "node_type": "1", "metadata": {}, "hash": "d30378618b8287875893291acc3812eb8da79f93c7b4892b3452753a5d7ef3ed", "class_name": "RelatedNodeInfo"}}, "text": "The treatment group received all the ben- e\ufb01ts of the NSW program. The controls were basically left to fend for themselves. The program admitted women receiving Aid to Families\n8 I cannot emphasize this enough\u2014this method, like regression more generally,\nonly has value for your project if you can satisfy the backdoor criterion by conditioning on X. If you cannot satisfy the backdoor criterion in your data, then the propensity score does not assist you in identifying a causal effect. At best, it helps you better understand\nissues related to balance on observables (but not unobservables). It is absolutely critical that your DAG be, in other words, credible, defensible, and accurate, as you depend on\nthose theoretical relationships to design the appropriate identi\ufb01cation strategy.\nwith Dependent Children, recovering addicts, released offenders, and men and women of both sexes who had not completed high school.\nTreatment group members were guaranteed a job for nine to eigh- teen months depending on the target group and site. They were then divided into crews of three to \ufb01ve participants who worked together and met frequently with an NSW counselor to discuss grievances with the program and performance. Finally, they were paid for their work. NSW offered the trainees lower wages than they would\u2019ve received on a regular job, but allowed for earnings to increase for satisfactory performance and attendance. After participants\u2019 terms expired, they were forced to \ufb01nd regular employment. The kinds of jobs varied within sites\u2014some were gas-station attendants, some worked at a printer shop\u2014and men and women frequently performed different kinds of work.\nThe MDRC collected earnings and demographic information from both the treatment and the control group at baseline as well as every nine months thereafter. MDRC also conducted up to four post-baseline interviews. There were different sample sizes from study to study, which can be confusing.\nNSW was a randomized job-training program; therefore, the inde- pendence assumption was satis\ufb01ed. So calculating average treat- ment effects was straightforward\u2014it\u2019s the simple difference in means estimator that we discussed in the potential outcomes chapter.9 (cid:6)\nThe good news for MDRC, and the treatment group, was that the treatment bene\ufb01ted the workers.10 Treatment group participants\u2019 real earnings post-treatment in 1978 were more than earnings of the control group by approximately $900 [Lalonde, 1986] to $1,800 [Dehejia and Wahba, 2002], depending on the sample the researcher used.\n9 Remember, randomization means that the treatment was independent of the\npotential outcomes, so simple difference in means identi\ufb01es the average treatment effect.\n10 Lalonde [1986] lists several studies that discuss the \ufb01ndings from the program.\nLalonde [1986] is an interesting study both because he is evaluat- ing the NSW program and because he is evaluating commonly used econometric methods from that time. He evaluated the econometric estimators\u2019 performance by trading out the experimental control group data with data on the non-experimental control group drawn from the population of US citizens. He used three samples of the Current Popu- lation Survey (CPS) and three samples of the Panel Survey of Income Dynamics (PSID) for this non-experimental control group data, but I will use just one for each. Non-experimental data is, after all, the typi- cal situation an economist \ufb01nds herself in. But the difference with the NSW is that it was a randomized experiment, and therefore we know the average treatment effect. Since we know the average treatment effect, we can see how well a variety of econometric models perform. If the NSW program increased earnings by approximately $900, then we should \ufb01nd that if the other econometrics estimators does a good job, right?\nLalonde [1986] reviewed a number of popular econometric meth- ods used by his contemporaries with both the PSID and the CPS sam- ples as nonexperimental comparison groups, and his results were con- sistently horrible. Not only were his estimates usually very different in magnitude, but his results were almost always the wrong sign! This paper, and its pessimistic conclusion, was in\ufb02uential in policy circles and led to a greater push for more experimental evaluations.11 We can see these results in the following tables from Lalonde [1986]. Table 33 shows the effect of the treatment when comparing the treatment group to the experimental control group. The baseline difference in real earnings between the two groups was negligible. The treatment group made $39 more than the control group in the pre-treatment period with- out controls and $21 less in the multivariate regression model, but neither is statistically signi\ufb01cant.", "start_char_idx": 18038, "end_char_idx": 22805, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "85654770-e510-4a32-824b-64794e31ba4b": {"__data__": {"id_": "85654770-e510-4a32-824b-64794e31ba4b", "embedding": null, "metadata": {"page number": "219 - 262"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9fd33664-e6ea-4fe1-ae3b-699b771c4008", "node_type": "4", "metadata": {"page number": "219 - 262"}, "hash": "5a02e3ba8fff6bc12accb0f554c5cbd5e8cdb806355061364a0b698613b869b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "47ff8b58-7f29-45b3-82a1-5fcf6ec0deda", "node_type": "1", "metadata": {"page number": "219 - 262"}, "hash": "fd3f81017de344223fd4c93b753790d3f3ba4c748d93b123c00470f2fdfa1a4c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "76eb42df-45b6-4072-a2f8-049c7ddd3c01", "node_type": "1", "metadata": {}, "hash": "db8feded45a83f40067a16984064f4dba314ed71c343913d5684267f6acc22cb", "class_name": "RelatedNodeInfo"}}, "text": "Lalonde [1986] reviewed a number of popular econometric meth- ods used by his contemporaries with both the PSID and the CPS sam- ples as nonexperimental comparison groups, and his results were con- sistently horrible. Not only were his estimates usually very different in magnitude, but his results were almost always the wrong sign! This paper, and its pessimistic conclusion, was in\ufb02uential in policy circles and led to a greater push for more experimental evaluations.11 We can see these results in the following tables from Lalonde [1986]. Table 33 shows the effect of the treatment when comparing the treatment group to the experimental control group. The baseline difference in real earnings between the two groups was negligible. The treatment group made $39 more than the control group in the pre-treatment period with- out controls and $21 less in the multivariate regression model, but neither is statistically signi\ufb01cant. But the post-treatment difference in average earnings was between $798 and $886.12\nTable 33 also shows the results he got when he used the non- experimental data as the comparison group. Here I report his results when using one sample from the PSID and one from the CPS, although\n11 It\u2019s since been cited a little more than 1,700 times.\nTable 33. Earnings comparisons and estimated training effects for the NSW male\nparticipants using comparison groups from the PSID and the CPS-SSA.\nNote: Each column represents an estimated treatment effect per econometric measure and for different comparison groups. The dependent variable is earnings in 1978. Based\non experimental treatment and controls, the estimated impact of trainings is $886. Stan-\ndard errors are in parentheses. Exogenous covariates used in the regression adjusted\nequations are age, age squared, years of schooling, high school completion status,\nin his original paper he used three of each. In nearly every point esti- mate, the effect is negative. The one exception is the difference-in- differences model which is positive, small, and insigni\ufb01cant.\nSo why is there such a stark difference when we move from the NSW control group to either the PSID or CPS? The reason is because of selection bias:\nIn other words, it\u2019s highly likely that the real earnings of NSW partici- pants would have been much lower than the non-experimental control group\u2019s earnings. As you recall from our decomposition of the simple difference in means estimator, the second form of bias is selection bias, and if E[Y0 | D = 1] < E[Y0 | D = 0], this will bias the estimate of the ATE downward (e.g., estimates that show a negative effect).\nBut as I will show shortly, a violation of independence also implies that covariates will be unbalanced across the propensity score\u2014 something we call the balancing property. Table 34 illustrates this showing the mean values for each covariate for the treatment and\nTable 34. Completed matching example with single covariate.\ncontrol groups, where the control is the 15,992 observations from the CPS. As you can see, the treatment group appears to be very differ- ent on average from the control group CPS sample along nearly every covariate listed. The NSW participants are more black, more Hispanic, younger, less likely to be married, more likely to have no degree and less schooling, more likely to be unemployed in 1975, and more likely to have considerably lower earnings in 1975. In short, the two groups are not exchangeable on observables (and likely not exchangeable on unobservables either).\nThe \ufb01rst paper to reevaluate Lalonde [1986] using propensity score methods was Dehejia and Wahba [1999]. Their interest was twofold. First, they wanted to examine whether propensity score matching could be an improvement in estimating treatment effects using non- experimental data. And second, they wanted to show the diagnos- tic value of propensity score matching. The authors used the same non-experimental control group data sets from the CPS and PSID as Lalonde [1986] did.\nLet\u2019s walk through this, and what they learned from each of these steps. First, the authors estimated the propensity score using max- imum likelihood modeling. Once they had the estimated propensity score, they compared treatment units to control units within intervals of the propensity score itself. This process of checking whether there\nare units in both treatment and control for intervals of the propensity score is called checking for common support.\nOne easy way to check for common support is to plot the num- ber of treatment and control group observations separately across the propensity score with a histogram.", "start_char_idx": 21873, "end_char_idx": 26493, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "76eb42df-45b6-4072-a2f8-049c7ddd3c01": {"__data__": {"id_": "76eb42df-45b6-4072-a2f8-049c7ddd3c01", "embedding": null, "metadata": {"page number": "219 - 262"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9fd33664-e6ea-4fe1-ae3b-699b771c4008", "node_type": "4", "metadata": {"page number": "219 - 262"}, "hash": "5a02e3ba8fff6bc12accb0f554c5cbd5e8cdb806355061364a0b698613b869b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "85654770-e510-4a32-824b-64794e31ba4b", "node_type": "1", "metadata": {"page number": "219 - 262"}, "hash": "091b946ba636c7bc752606dbcd0f7daccd2d7f7e64b157e5818f578c2f2b229e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b3ed90a-1690-4ae1-ac0e-d8ade0bf9689", "node_type": "1", "metadata": {}, "hash": "e93eb01b341e4d64221698442aba1e4a38a4956ce21f0a8ef1a670c8634ca6ea", "class_name": "RelatedNodeInfo"}}, "text": "Their interest was twofold. First, they wanted to examine whether propensity score matching could be an improvement in estimating treatment effects using non- experimental data. And second, they wanted to show the diagnos- tic value of propensity score matching. The authors used the same non-experimental control group data sets from the CPS and PSID as Lalonde [1986] did.\nLet\u2019s walk through this, and what they learned from each of these steps. First, the authors estimated the propensity score using max- imum likelihood modeling. Once they had the estimated propensity score, they compared treatment units to control units within intervals of the propensity score itself. This process of checking whether there\nare units in both treatment and control for intervals of the propensity score is called checking for common support.\nOne easy way to check for common support is to plot the num- ber of treatment and control group observations separately across the propensity score with a histogram. Dehejia and Wahba [1999] did this using both the PSID and CPS samples and found that the overlap was nearly nonexistent, but here I\u2019ll focus on their CPS sample. The overlap was so bad that they opted to drop 12,611 observations in the con- trol group because their propensity scores were outside the treatment group range. Also, a large number of observations have low propensity scores, evidenced by the fact that the \ufb01rst bin contains 2,969 com- parison units. Once this \u201ctrimming\u201d was done, the overlap improved, though still wasn\u2019t great.\nWe learn some things from this kind of diagnostic, though. We learn, for one, that the selection bias on observables is probably extreme if for no other reason than the fact that there are so few units in both treatment and control for given values of the propensity score. When there is considerable bunching at either end of the propensity score distribution, it suggests you have units who differ remarkably on observables with respect to the treatment variable itself. Trimming around those extreme values has been a way of addressing this when employing traditional propensity score adjustment techniques.\nWith estimated propensity score in hand, Dehejia and Wahba [1999] estimated the treatment effect on real earnings 1978 using the experimental treatment group compared with the non-experimental control group. The treatment effect here differs from what we found in Lalonde because Dehejia and Wahba [1999] used a slightly differ- ent sample. Still, using their sample, they \ufb01nd that the NSW program caused earnings to increase between $1,672 and $1,794 depending on whether exogenous covariates were included in a regression. Both of these estimates are highly signi\ufb01cant.\nThe \ufb01rst two columns labeled \u201cunadjusted\u201d and \u201cadjusted\u201d repre- sent OLS regressions with and without controls. Without controls, both PSID and CPS estimates are extremely negative and precise. This, again, is because the selection bias is so severe with respect to the NSW program. When controls are included, effects become positive\nTable 35. Estimated training effects using propensity scores.\nPropensity score adjusted\nNote: Adjusted column 2 is OLS regressed onto treatment indicator, age and age squared, education, no degree, black hispanic, real earnings 1974 and 1975. Quadratic\nscore in column 3 is OLS regressed onto a quadratic on the propensity score and a\ntreatment indicator. Last column labeled \u201cadjusted\u201d is weighted least squares.\nand imprecise for the PSID sample though almost signi\ufb01cant at 5% for CPS. But each effect size is only about half the size of the true effect. Table 35 shows the results using propensity score weighting or matching.13 As can be seen, the results are a considerable improve- ment over Lalonde [1986]. I won\u2019t review every treatment effect the authors calculated, but I will note that they are all positive and simi- lar in magnitude to what they found in columns 1 and 2 using only the experimental data.\nFinally, the authors examined the balance between the covari- ates in the treatment group (NSW) and the various non-experimental (matched) samples in Table 36. In the next section, I explain why we expect covariate values to balance along the propensity score for the treatment and control group after trimming the outlier propensity score units from the data. Table 36 shows the sample means of charac- teristics in the matched control sample versus the experimental NSW sample (\ufb01rst row). Trimming on the propensity score, in effect, helped\n13 Let\u2019s hold off digging into exactly how they used the propensity score to\nTable 36. Sample means of characteristics for matched control samples.", "start_char_idx": 25495, "end_char_idx": 30164, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b3ed90a-1690-4ae1-ac0e-d8ade0bf9689": {"__data__": {"id_": "4b3ed90a-1690-4ae1-ac0e-d8ade0bf9689", "embedding": null, "metadata": {"page number": "219 - 262"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9fd33664-e6ea-4fe1-ae3b-699b771c4008", "node_type": "4", "metadata": {"page number": "219 - 262"}, "hash": "5a02e3ba8fff6bc12accb0f554c5cbd5e8cdb806355061364a0b698613b869b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "76eb42df-45b6-4072-a2f8-049c7ddd3c01", "node_type": "1", "metadata": {"page number": "219 - 262"}, "hash": "b1051b3f03800468bc160826a711b2d2de9ba728867ed0d7d3c2902a792a7a41", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "22d8ac8c-fbe5-47e2-bdd0-993692c9aed8", "node_type": "1", "metadata": {}, "hash": "352362f79425f4f62963af34189eb379a6f9c9e04ea19551be9d6ef15517e7e4", "class_name": "RelatedNodeInfo"}}, "text": "I won\u2019t review every treatment effect the authors calculated, but I will note that they are all positive and simi- lar in magnitude to what they found in columns 1 and 2 using only the experimental data.\nFinally, the authors examined the balance between the covari- ates in the treatment group (NSW) and the various non-experimental (matched) samples in Table 36. In the next section, I explain why we expect covariate values to balance along the propensity score for the treatment and control group after trimming the outlier propensity score units from the data. Table 36 shows the sample means of charac- teristics in the matched control sample versus the experimental NSW sample (\ufb01rst row). Trimming on the propensity score, in effect, helped\n13 Let\u2019s hold off digging into exactly how they used the propensity score to\nTable 36. Sample means of characteristics for matched control samples.\nNote: Standard error on the difference in means with NSW sample is given in parenthe- ses. RE74 stands for real earnings in 1974.\nbalance the sample. Covariates are much closer in mean value to the NSW sample after trimming on the propensity score.\nPropensity score is best explained using actual data. We will use data from Dehejia and Wahba [2002] for the following exercises. But before using the propensity score methods for estimating treat- ment effects, let\u2019s calculate the average treatment effect from the actual experiment. Using the following code, we calculate that the NSW job-training program caused real earnings in 1978 to increase by $1,794.343.\nclear 2 su re78 if treat 3 gen y1 = r(mean) 4 su re78 if treat==0 5 gen y0 = r(mean) 6 gen ate = y1-y0 7 su ate 8 di 6349.144 - 4554.801 9 * ATE is 1794.34 10 drop if treat==0 11 drop y1 y0 ate 12 compress\n1 library(tidyverse) 2 library(haven) 3 4 read_data <- function(df) 5 { 6 7 8 9 10 } 11 12 nsw_dw <- read_data(\"nsw_mixtape.dta\") 13 14 nsw_dw %>% 15 16 17 18 mean1 <- nsw_dw %>% 19 filter(treat == 1) %>% 20 pull(re78) %>% 21 mean() 22 23 nsw_dw$y1 <- mean1 24 25 nsw_dw %>% 26 27 28 29 mean0 <- nsw_dw %>% 30 filter(treat == 0) %>% 31 pull(re78) %>% 32 mean() 33 34 nsw_dw$y0 <- mean0 35 36 ate <- unique(nsw_dw$y1 - nsw_dw$y0) 37 38 nsw_dw <- nsw_dw %>% 39 40\ndf, sep = \"\")\nNext we want to go through several examples in which we estimate the average treatment effect or some if its variants such as the aver- age treatment effect on the treatment group or the average treatment effect on the untreated group. But here, rather than using the experi- mental control group from the original randomized experiment, we will use the non-experimental control group from the Current Population Survey. It is very important to stress that while the treatment group is an experimental group, the control group now consists of a random sample of Americans from that time period. Thus, the control group suffers from extreme selection bias since most Americans would not function as counterfactuals for the distressed group of workers who selected into the NSW program. In the following, we will append the CPS data to the experimental data and estimate the propensity score using logit so as to be consistent with Dehejia and Wahba [2002].\nclear 3 drop if treat==0 4 5 * Now merge in the CPS controls from footnote 2 of Table 2 (Dehejia and Wahba\n6 append using\n7 gen agesq=age*age 8 gen agecube=age*age*age 9 gen edusq=educ*edu 10 gen u74 = 0 if re74!=. 11 replace u74 = 1 if re74==0 12 gen u75 = 0 if re75!=.", "start_char_idx": 29270, "end_char_idx": 32736, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "22d8ac8c-fbe5-47e2-bdd0-993692c9aed8": {"__data__": {"id_": "22d8ac8c-fbe5-47e2-bdd0-993692c9aed8", "embedding": null, "metadata": {"page number": "219 - 262"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9fd33664-e6ea-4fe1-ae3b-699b771c4008", "node_type": "4", "metadata": {"page number": "219 - 262"}, "hash": "5a02e3ba8fff6bc12accb0f554c5cbd5e8cdb806355061364a0b698613b869b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b3ed90a-1690-4ae1-ac0e-d8ade0bf9689", "node_type": "1", "metadata": {"page number": "219 - 262"}, "hash": "f38b69f1197c622b9a5f34dd364827aa8527bae720c68fe0a0f63e86a35b879c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a358362-e01f-4432-b134-1863e105b843", "node_type": "1", "metadata": {}, "hash": "28a411456c1074eca8eff7f6cf987c7268b7265250216d2b1ff71955f2ec663f", "class_name": "RelatedNodeInfo"}}, "text": "Thus, the control group suffers from extreme selection bias since most Americans would not function as counterfactuals for the distressed group of workers who selected into the NSW program. In the following, we will append the CPS data to the experimental data and estimate the propensity score using logit so as to be consistent with Dehejia and Wahba [2002].\nclear 3 drop if treat==0 4 5 * Now merge in the CPS controls from footnote 2 of Table 2 (Dehejia and Wahba\n6 append using\n7 gen agesq=age*age 8 gen agecube=age*age*age 9 gen edusq=educ*edu 10 gen u74 = 0 if re74!=. 11 replace u74 = 1 if re74==0 12 gen u75 = 0 if re75!=. 13 replace u75 = 1 if re75==0 14 gen interaction1 = educ*re74 15 gen re74sq=re74^2 16 gen re75sq=re75^2 17 gen interaction2 = u74*hisp 18 19 * Now estimate the propensity score\n(continued)\nSTATA (continued)\n21 predict pscore 22 23 * Checking mean propensity scores for treatment and control groups 24 su pscore if treat==1, detail 25 su pscore if treat==0, detail 26 27 * Now look at the propensity score distribution for treatment and control groups 28 histogram pscore, by(treat) binrescale\n1 library(tidyverse) 2 library(haven) 3 4 read_data <- function(df) 5 { 6 7 8 9 10 } 11 12 nsw_dw_cpscontrol <- read_data(\"cps_mixtape.dta\") %>% 13 14 mutate(agesq = age^2, 15 agecube = age^3, 16 educsq = educ*educ, 17 u74 = case_when(re74 == 0 ~ 1, TRUE ~ 0), 18 u75 = case_when(re75 == 0 ~ 1, TRUE ~ 0), 19 interaction1 = educ*re74, 20 re74sq = re74^2, 21 re75sq = re75^2, 22 interaction2 = u74*hisp) 23 24 # estimating 25 logit_nsw <- glm(treat ~ age + agesq + agecube + educ + educsq + 26 27\nmarr + nodegree + black + hisp + re74 + re75 + u74 + u75 + interaction1, family = binomial(link = \"logit\"),\ndf, sep = \"\")\n(continued)\nR (continued)\n28 29 30 nsw_dw_cpscontrol <- nsw_dw_cpscontrol %>% 31 mutate(pscore = logit_nsw$fitted.values) 32 33 # mean pscore 34 pscore_control <- nsw_dw_cpscontrol %>% 35 36 37 mean() 38 39 pscore_treated <- nsw_dw_cpscontrol %>% 40 41 42 mean() 43 44 # histogram 45 nsw_dw_cpscontrol %>% 46 47 48 49 50 nsw_dw_cpscontrol %>% 51 52 53 54\nfilter(treat == 0) %>% ggplot() + geom_histogram(aes(x = pscore))\nfilter(treat == 1) %>% ggplot() + geom_histogram(aes(x = pscore))\nThe propensity score is the \ufb01tted values of the logit model. Put differently, we used the estimated coe\ufb03cients from that logit regres- sion to estimate the conditional probability of treatment, assuming that probabilities are based on the cumulative logistic distribution:\nwhere F() = e in the model.\n(1 + e) and X is the exogenous covariates we are including\nAs I said earlier, the propensity score used the \ufb01tted values from the maximum likelihood regression to calculate each unit\u2019s conditional probability of treatment regardless of actual treatment status. The\npropensity score is just the predicted conditional probability of treat- ment or \ufb01tted value for each unit. It is advisable to use maximum like- lihood when estimating the propensity score so that the \ufb01tted values are in the range [0, 1]. We could use a linear probability model, but lin- ear probability models routinely create \ufb01tted values below 0 and above 1, which are not true probabilities since 0 \u2264 p \u2264 1.\nThe de\ufb01nition of the propensity score is the selection probability conditional on the confounding variables; p(X) = Pr(D = 1 | X).", "start_char_idx": 32105, "end_char_idx": 35444, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a358362-e01f-4432-b134-1863e105b843": {"__data__": {"id_": "3a358362-e01f-4432-b134-1863e105b843", "embedding": null, "metadata": {"page number": "219 - 262"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9fd33664-e6ea-4fe1-ae3b-699b771c4008", "node_type": "4", "metadata": {"page number": "219 - 262"}, "hash": "5a02e3ba8fff6bc12accb0f554c5cbd5e8cdb806355061364a0b698613b869b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "22d8ac8c-fbe5-47e2-bdd0-993692c9aed8", "node_type": "1", "metadata": {"page number": "219 - 262"}, "hash": "ec845bb4424be8cd0fb392afa40cc0167b023890bc40313ae200553c942c4fbc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2100053e-99f0-4312-99e3-8641692d42f2", "node_type": "1", "metadata": {}, "hash": "fc11858e0ce270a5d1ae714b2d45846806f91bab2d0658a8a3cc3b46e915cfb4", "class_name": "RelatedNodeInfo"}}, "text": "(1 + e) and X is the exogenous covariates we are including\nAs I said earlier, the propensity score used the \ufb01tted values from the maximum likelihood regression to calculate each unit\u2019s conditional probability of treatment regardless of actual treatment status. The\npropensity score is just the predicted conditional probability of treat- ment or \ufb01tted value for each unit. It is advisable to use maximum like- lihood when estimating the propensity score so that the \ufb01tted values are in the range [0, 1]. We could use a linear probability model, but lin- ear probability models routinely create \ufb01tted values below 0 and above 1, which are not true probabilities since 0 \u2264 p \u2264 1.\nThe de\ufb01nition of the propensity score is the selection probability conditional on the confounding variables; p(X) = Pr(D = 1 | X). Recall that we said there are two identifying assumptions for propensity score methods. The \ufb01rst assumption is CIA. That is, (Y0, Y1) \u22a5\u22a5 D | X. It is not testable, because the assumption is based on unobservable poten- tial outcomes. The second assumption is called the common support assumption. That is, 0 < Pr(D = 1 | X) <1. This simply means that for any probability, there must be units in both the treatment group and the control group. The conditional independence assumption simply means that the backdoor criterion is met in the data by conditioning on a vector X. Or, put another way, conditional on X, the assignment of units to the treatment is as good as random.14\nCommon support is required to calculate any particular kind of de\ufb01ned average treatment effect, and without it, you will just get some kind of weird weighted average treatment effect for only those regions that do have common support. The reason it is \u201cweird\u201d is that average treatment effect doesn\u2019t correspond to any of the inter- esting treatment effects the policymaker needed. Common support requires that for each value of X, there is a positive probability of being both treated and untreated, or 0 < Pr(Di = 1 | Xi) < 1. This implies that the probability of receiving treatment for every value of the vector X is strictly within the unit interval. Common support ensures there is su\ufb03cient overlap in the characteristics of treated and untreated units to \ufb01nd adequate matches. Unlike CIA, the com- mon support requirement is testable by simply plotting histograms\n14 CIA is expressed in different ways according to the econometric or statistical\ntradition. Rosenbaum and Rubin [1983] called it the ignorable treatment assignment, or unconfoundedness. Barnow et al. [1981] and Dale and Krueger [2002] called it selection on observables. In the traditional econometric pedagogy, as we discussed earlier, it\u2019s called the zero conditional mean assumption.\nor summarizing the data. Here we do that two ways: by looking at the summary statistics and by looking at a histogram. Let\u2019s start with looking at a distribution in table form before looking at the histogram.\nThe mean value of the propensity score for the treatment group is 0.43, and the mean for the CPS control group is 0.007. The 50th per- centile for the treatment group is 0.4, but the control group doesn\u2019t reach that high a number until the 99th percentile. Let\u2019s look at the distribution of the propensity score for the two groups using a histogram now.\nThese two simple diagnostic tests show what is going to be a prob- lem later when we use inverse probability weighting. The probability of treatment is spread out across the units in the treatment group, but there is a very large mass of nearly zero propensity scores in the CPS. How do we interpret this? What this means is that the characteristics of individuals in the treatment group are rare in the CPS sample. This is not surprising given the strong negative selection into treatment. These individuals are younger, less likely to be married, and more likely to be uneducated and a minority. The lesson is, if the two groups are signi\ufb01cantly different on background characteristics, then the propen- sity scores will have grossly different distributions by treatment status. We will discuss this in greater detail later.\nFor now, let\u2019s look at the treatment parameter under both assump-\nE[\u03b4i(Xi)] =E [Y1 i = E[Y1 i\nindependence assumption allows us to make the\nand same for the other term. Common support means we can estimate both terms.", "start_char_idx": 34636, "end_char_idx": 38989, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2100053e-99f0-4312-99e3-8641692d42f2": {"__data__": {"id_": "2100053e-99f0-4312-99e3-8641692d42f2", "embedding": null, "metadata": {"page number": "219 - 262"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9fd33664-e6ea-4fe1-ae3b-699b771c4008", "node_type": "4", "metadata": {"page number": "219 - 262"}, "hash": "5a02e3ba8fff6bc12accb0f554c5cbd5e8cdb806355061364a0b698613b869b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a358362-e01f-4432-b134-1863e105b843", "node_type": "1", "metadata": {"page number": "219 - 262"}, "hash": "bbcf03b8d332cfe60e27419f139906c9a68b810cae5a11092ac7db2574b325c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "44f90f56-7618-406b-ac7d-9b60e535bfee", "node_type": "1", "metadata": {}, "hash": "dfbee31dcb17ab49ecdc370e13eb9e741653d136cf5743702cab8ffe1e111ff5", "class_name": "RelatedNodeInfo"}}, "text": "How do we interpret this? What this means is that the characteristics of individuals in the treatment group are rare in the CPS sample. This is not surprising given the strong negative selection into treatment. These individuals are younger, less likely to be married, and more likely to be uneducated and a minority. The lesson is, if the two groups are signi\ufb01cantly different on background characteristics, then the propen- sity scores will have grossly different distributions by treatment status. We will discuss this in greater detail later.\nFor now, let\u2019s look at the treatment parameter under both assump-\nE[\u03b4i(Xi)] =E [Y1 i = E[Y1 i\nindependence assumption allows us to make the\nand same for the other term. Common support means we can estimate both terms. Therefore, under both assumptions:\nFrom these assumptions we get the propensity score theorem, which states that under CIA\nwhere p(X) = Pr(D = 1 | X), the propensity score. In English, this means that in order to achieve independence, assuming CIA, all we have to do is condition on the propensity score. Conditioning on the propensity score is enough to have independence between the treatment and the potential outcomes.\nThis is an extremely valuable theorem because stratifying on X tends to run into the sparseness-related problems (i.e., empty cells) in \ufb01nite samples for even a moderate number of covariates. But the propensity scores are just a scalar. So stratifying across a probability is going to reduce that dimensionality problem.\nThe proof of the propensity score theorem is fairly straightfor- ward, as it\u2019s just an application of the law of iterated expectations with nested conditioning.15 If we can show that the probability an individual receives treatment conditional on potential outcomes and the propen- sity score is not a function of potential outcomes, then we will have\nproved that there is independence between the potential outcomes and the treatment conditional on X. Before diving into the proof, \ufb01rst recognize that\nand the second term cancels out because it\u2019s multiplied by zero. The formal proof is as follows:\nUsing a similar argument, we obtain:\nLike the omitted variable bias formula for regression, the propen- sity score theorem says that you need only control for covariates that determine the likelihood a unit receives the treatment. But it also says something more than that. It technically says that the only covariate you need to condition on is the propensity score. All of the informa- tion from the X matrix has been collapsed into a single number: the propensity score.\nA corollary of the propensity score theorem, therefore, states that given CIA, we can estimate average treatment effects by weighting appropriately the simple difference in means.16\nBecause the propensity score is a function of X, we know\nTherefore, conditional on the propensity score, the probability that D = 1 does not depend on X any longer. That is, D and X are independent of one another conditional on the propensity score, or\nSo from this we also obtain the balancing property of the propensity score:\nwhich states that conditional on the propensity score, the distribution of the covariates is the same for treatment as it is for control group units. See this in the following DAG:\n16 This all works if we match on the propensity score and then calculate dif-\nferences in means. Direct propensity score matching works in the same way as the\ncovariate matching we discussed earlier (e.g., nearest-neighbor matching), except that we match on the score instead of the covariates directly.\nNotice that there exist two paths between X and D. There\u2019s the direct path of X \u2192 p(X) \u2192 D, and there\u2019s the backdoor path X \u2192 Y \u2190 D. The backdoor path is blocked by a collider, so there is no systematic correlation between X and D through it. But there is systematic cor- relation between X and D through the \ufb01rst directed path. But, when we condition on p(X), the propensity score, notice that D and X are statistically independent. This implies that D \u22a5\u22a5 X | p(X), which implies\nThis is something we can directly test, but note the implication: condi- tional on the propensity score, treatment and control should on aver- age be the same with respect to X. In other words, the propensity score theorem implies balanced observable covariates.17\nWeighting on the propensity score. There are several ways researchers can estimate average treatment effects using an estimated propen- sity score. Busso et al. [2014] examined the properties of various approaches and found that inverse probability weighting was com- petitive in several simulations.", "start_char_idx": 38225, "end_char_idx": 42850, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "44f90f56-7618-406b-ac7d-9b60e535bfee": {"__data__": {"id_": "44f90f56-7618-406b-ac7d-9b60e535bfee", "embedding": null, "metadata": {"page number": "219 - 262"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9fd33664-e6ea-4fe1-ae3b-699b771c4008", "node_type": "4", "metadata": {"page number": "219 - 262"}, "hash": "5a02e3ba8fff6bc12accb0f554c5cbd5e8cdb806355061364a0b698613b869b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2100053e-99f0-4312-99e3-8641692d42f2", "node_type": "1", "metadata": {"page number": "219 - 262"}, "hash": "9031d8e3a80c0cd7ddac88bcfab00b02297d7797461697271ee1d2989ea04d60", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5e8e915e-65f2-4b90-961c-0ebdb6d58c35", "node_type": "1", "metadata": {}, "hash": "295948cce11cb72af2b2b3853887c835348d04ba6c490ed61d62385b8d54bbeb", "class_name": "RelatedNodeInfo"}}, "text": "But there is systematic cor- relation between X and D through the \ufb01rst directed path. But, when we condition on p(X), the propensity score, notice that D and X are statistically independent. This implies that D \u22a5\u22a5 X | p(X), which implies\nThis is something we can directly test, but note the implication: condi- tional on the propensity score, treatment and control should on aver- age be the same with respect to X. In other words, the propensity score theorem implies balanced observable covariates.17\nWeighting on the propensity score. There are several ways researchers can estimate average treatment effects using an estimated propen- sity score. Busso et al. [2014] examined the properties of various approaches and found that inverse probability weighting was com- petitive in several simulations. As there are different ways in which the weights are incorporated into a weighting design, I discuss a few canonical versions of the method of inverse probability weighting and associated methods for inference. This is an expansive area in causal\n17 Just because something is exchangeable on observables does not make it exchangeable on unobservables. The propensity score theorem does not imply balanced unobserved covariates. See Brooks and Ohsfeldt [2013].\ninference econometrics, so consider this merely an overview of and introduction to the main concepts.\nAssuming that CIA holds in our data, then one way we can estimate treatment effects is to use a weighting procedure in which each individ- ual\u2019s propensity score is a weight of that individual\u2019s outcome [Imbens, 2000]. When aggregated, this has the potential to identify some aver- age treatment effect. This estimator is based on earlier work in sur- vey methodology \ufb01rst proposed by Horvitz and Thompson [1952].The weight enters the expression differently depending on each unit\u2019s treat- ment status and takes on two different forms depending on whether the target parameter is the ATE or the ATT (or the ATU, which is not shown here):\nA proof for ATE is provided: (cid:16)\nand the results follow from integrating over P(X) and P(X | D = 1).\nThe sample versions of both ATE and ATT are obtained by a two- step estimation procedure. In the \ufb01rst step, the researcher estimates the propensity score using logit or probit. In the second step, the researcher uses the estimated score to produce sample versions of one of the average treatment effect estimators shown above. Those sample versions can be written as follows:\n(cid:12)\u03b4ATT = 1 NT\nWe have a few options for estimating the variance of this esti- mator, but one is simply to use bootstrapping. First created by Efron [1979], bootstrapping is a procedure used to estimate the variance of an estimator. In the context of inverse probability weighting, we would repeatedly draw (\u201cwith replacement\u201d) a random sample of our original data and then use that smaller sample to calculate the sample analogs of the ATE or ATT. More speci\ufb01cally, using the smaller \u201cbootstrapped\u201d data, we would \ufb01rst estimate the propensity score and then use the estimated propensity score to calculate sample analogs of the ATE or ATT over and over to obtain a distribution of treatment effects cor- responding to different cuts of the data itself.18 If we do this 1,000 or 10,000 times, we get a distribution of parameter estimates from which we can calculate the standard deviation. This standard devi- ation becomes like a standard error and gives us a measure of the dispersion of the parameter estimate under uncertainty regarding the sample itself.19 Adudumilli [2018] and Bodory et al. [2020] discuss the performance of various bootstrapping procedures, such as the stan- dard bootstrap or the wild bootstrap. I encourage you to read these\n18 Bootstrapping and randomization inference are mechanically similar. Each ran- domizes something over and over, and under each randomization, reestimates treatment effects to obtain a distribution of treatment effects. But that is where the similarity ends.\nBootstrapping is a method for computing the variance in an estimator where we take the\ntreatment assignment as given. The uncertainty in bootstrapping stems from the sam-\nple, not the treatment assignment. And thus with each bootstrapped sample, we use\nfewer observations than exist in our real sample. That is not the source of uncertainty\nin randomization inference, though. In randomization inference, as you recall from the\nsample. And thus in randomization inference, we randomly assign the treatment in order\nto reject or fail to reject Fisher\u2019s sharp null of no individual treatment effects.", "start_char_idx": 42047, "end_char_idx": 46648, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5e8e915e-65f2-4b90-961c-0ebdb6d58c35": {"__data__": {"id_": "5e8e915e-65f2-4b90-961c-0ebdb6d58c35", "embedding": null, "metadata": {"page number": "219 - 262"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9fd33664-e6ea-4fe1-ae3b-699b771c4008", "node_type": "4", "metadata": {"page number": "219 - 262"}, "hash": "5a02e3ba8fff6bc12accb0f554c5cbd5e8cdb806355061364a0b698613b869b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "44f90f56-7618-406b-ac7d-9b60e535bfee", "node_type": "1", "metadata": {"page number": "219 - 262"}, "hash": "3e60537d0b112173d055a8739c08ab6bb22f3419db860c24558ab4cef92040cf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a14a8db0-fd67-4dda-99c1-c6fb16340558", "node_type": "1", "metadata": {}, "hash": "97997b35bd32e01636ef6cc06412d9bd52caa1f3613645d1d5a44348f5be20e8", "class_name": "RelatedNodeInfo"}}, "text": "I encourage you to read these\n18 Bootstrapping and randomization inference are mechanically similar. Each ran- domizes something over and over, and under each randomization, reestimates treatment effects to obtain a distribution of treatment effects. But that is where the similarity ends.\nBootstrapping is a method for computing the variance in an estimator where we take the\ntreatment assignment as given. The uncertainty in bootstrapping stems from the sam-\nple, not the treatment assignment. And thus with each bootstrapped sample, we use\nfewer observations than exist in our real sample. That is not the source of uncertainty\nin randomization inference, though. In randomization inference, as you recall from the\nsample. And thus in randomization inference, we randomly assign the treatment in order\nto reject or fail to reject Fisher\u2019s sharp null of no individual treatment effects.\n19 Abadie and Imbens [2008] show that the bootstrap fails for matching, but inverse probability weighting is not matching. This may seem like a subtle point, but\nin my experience many people con\ufb02ate propensity score based matching with other\nmethods that use the propensity score, calling all of them \u201cmatching.\u201d But inverse prob- ability weighting is not a matching procedure. Rather, it is a weighting procedure whose properties differ from that of using imputation and generally the bootstrap is \ufb01ne.\npapers more closely when choosing which bootstrap is suitable for\nThe sensitivity of inverse probability weighting to extreme val- ues of the propensity score has led some researchers to propose an\nalternative that can handle extremes a bit better. Hirano and Imbens [2001] propose an inverse probability weighting estimator of the aver- age treatment effect that assigns weights normalized by the sum of\npropensity scores for treated and control groups as opposed to equal\nciated with H\u00e1jek [1971]. Millimet and Tchernis [2009] refer to this\nto each observation. This procedure is sometimes asso-\nestimator as the normalized estimator. Its weights sum to one within\neach group, which tends to make it more stable. The expression of this\nnormalized estimator is shown here: (cid:15) N(cid:6) (cid:16)\n(cid:12)\u03b4ATT =\nMost software packages have programs that will estimate the\nsample analog of these inverse probability weighted parameters that\nuse the second method with normalized weights. For instance, Stata\u2019s\nsity score to construct either non-normalized or normalized weights\nand then estimate ATT.", "start_char_idx": 45760, "end_char_idx": 48256, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a14a8db0-fd67-4dda-99c1-c6fb16340558": {"__data__": {"id_": "a14a8db0-fd67-4dda-99c1-c6fb16340558", "embedding": null, "metadata": {"page number": "219 - 262"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9fd33664-e6ea-4fe1-ae3b-699b771c4008", "node_type": "4", "metadata": {"page number": "219 - 262"}, "hash": "5a02e3ba8fff6bc12accb0f554c5cbd5e8cdb806355061364a0b698613b869b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5e8e915e-65f2-4b90-961c-0ebdb6d58c35", "node_type": "1", "metadata": {"page number": "219 - 262"}, "hash": "67821c51c317e8f122aae5ed9f31fae2d8d61af6ba332ec0f4eb8e29c2e6f651", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dd5016ef-f821-422c-8809-4f03d2f6a652", "node_type": "1", "metadata": {}, "hash": "eb829b3f23ffe8d6b1b9b7707da771e3b153014fc9ce274a9123060426d6afd4", "class_name": "RelatedNodeInfo"}}, "text": "Hirano and Imbens [2001] propose an inverse probability weighting estimator of the aver- age treatment effect that assigns weights normalized by the sum of\npropensity scores for treated and control groups as opposed to equal\nciated with H\u00e1jek [1971]. Millimet and Tchernis [2009] refer to this\nto each observation. This procedure is sometimes asso-\nestimator as the normalized estimator. Its weights sum to one within\neach group, which tends to make it more stable. The expression of this\nnormalized estimator is shown here: (cid:15) N(cid:6) (cid:16)\n(cid:12)\u03b4ATT =\nMost software packages have programs that will estimate the\nsample analog of these inverse probability weighted parameters that\nuse the second method with normalized weights. For instance, Stata\u2019s\nsity score to construct either non-normalized or normalized weights\nand then estimate ATT.\n1 * Manual with non-normalized weights using all the data 2 gen d1=treat/pscore 3 gen d0=(1-treat)/(1-pscore) 4 egen s1=sum(d1) 5 egen s0=sum(d0) 6\n(continued)\nSTATA (continued)\n7 gen y1=treat*re78/pscore 8 gen y0=(1-treat)*re78/(1-pscore) 9 gen ht=y1-y0 10 11 * Manual with normalized weights 12 replace y1=(treat*re78/pscore)/(s1/_N) 13 replace y0=((1-treat)*re78/(1-pscore))/(s0/_N) 14 gen norm=y1-y0 15 su ht norm 16 17 * ATT under non-normalized weights is -$11,876 18 * ATT under normalized weights is -$7,238 19 20 drop d1 d0 s1 s0 y1 y0 ht norm 21 22 * Trimming the propensity score 23 drop if pscore <= 0.1 24 drop if pscore >= 0.9 25 26 * Manual with non-normalized weights using trimmed data 27 gen d1=treat/pscore 28 gen d0=(1-treat)/(1-pscore) 29 egen s1=sum(d1) 30 egen s0=sum(d0) 31 32 gen y1=treat*re78/pscore 33 gen y0=(1-treat)*re78/(1-pscore) 34 gen ht=y1-y0 35 36 * Manual with normalized weights using trimmed data 37 replace y1=(treat*re78/pscore)/(s1/_N) 38 replace y0=((1-treat)*re78/(1-pscore))/(s0/_N) 39 gen norm=y1-y0 40 su ht norm 41 42 * ATT under non-normalized weights is $2,006 43 * ATT under normalized weights is $1,806\n1 library(tidyverse) 2 library(haven) 3 4 #continuation 5 N <- nrow(nsw_dw_cpscontrol) 6 #- Manual with non-normalized weights using all data 7 nsw_dw_cpscontrol <- nsw_dw_cpscontrol %>% 8 mutate(d1 = treat/pscore, 9 10 11 s1 <- sum(nsw_dw_cpscontrol$d1) 12 s0 <- sum(nsw_dw_cpscontrol$d0) 13 14 15 nsw_dw_cpscontrol <- nsw_dw_cpscontrol %>% 16 mutate(y1 = treat * re78/pscore, 17 18 19 20 #- Manual with normalized weights 21 nsw_dw_cpscontrol <- nsw_dw_cpscontrol %>% 22 mutate(y1 = (treat*re78/pscore)/(s1/N), 23 24 25 26 nsw_dw_cpscontrol %>% 27 28 mean() 29 30 nsw_dw_cpscontrol %>% 31 pull(norm) %>% 32 mean() 33 34 #-- trimming propensity score 35 nsw_dw_cpscontrol <- nsw_dw_cpscontrol %>% 36 select(-d1, -d0, -y1, -y0, -ht, -norm) %>% 37 filter(!(pscore >= 0.9)) %>% 38 filter(!", "start_char_idx": 47402, "end_char_idx": 50199, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dd5016ef-f821-422c-8809-4f03d2f6a652": {"__data__": {"id_": "dd5016ef-f821-422c-8809-4f03d2f6a652", "embedding": null, "metadata": {"page number": "219 - 262"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9fd33664-e6ea-4fe1-ae3b-699b771c4008", "node_type": "4", "metadata": {"page number": "219 - 262"}, "hash": "5a02e3ba8fff6bc12accb0f554c5cbd5e8cdb806355061364a0b698613b869b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a14a8db0-fd67-4dda-99c1-c6fb16340558", "node_type": "1", "metadata": {"page number": "219 - 262"}, "hash": "11f874b6780986b3d4a0c2ba077522595aa94513a5d3c20cc1c28188ccd51e0f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "afdf4866-84de-44a2-95d3-1ab79cc77502", "node_type": "1", "metadata": {}, "hash": "d9dd05396069453fc888e1c6e573348ad942bb204ed8146353229b3063ddc4d3", "class_name": "RelatedNodeInfo"}}, "text": "(pscore >= 0.9)) %>% 38 filter(!(pscore <= 0.1)) 39 40 N <- nrow(nsw_dw_cpscontrol)\n(continued)\nR (continued)\n41 42 #- Manual with non-normalized weights using trimmed data 43 nsw_dw_cpscontrol <- nsw_dw_cpscontrol %>% 44 mutate(d1 = treat/pscore, 45 46 47 s1 <- sum(nsw_dw_cpscontrol$d1) 48 s0 <- sum(nsw_dw_cpscontrol$d0) 49 50 nsw_dw_cpscontrol <- nsw_dw_cpscontrol %>% 51 mutate(y1 = treat * re78/pscore, 52 53 54 55 #- Manual with normalized weights with trimmed data 56 nsw_dw_cpscontrol <- nsw_dw_cpscontrol %>% 57 mutate(y1 = (treat*re78/pscore)/(s1/N), 58 59 60 61 nsw_dw_cpscontrol %>% 62 63 mean() 64 65 nsw_dw_cpscontrol %>% 66 pull(norm) %>% 67 mean()\nWhen we estimate the treatment effect using inverse probability weighting using the non-normalized weighting procedure described earlier, we \ufb01nd an estimated ATT of \u2212$11, 876. Using the normalization of the weights, we get \u2212$7, 238. Why is this so much different than what we get using the experimental data?\nRecall what inverse probability weighting is doing. It is weighting treatment and control units according to (cid:12)p(X), which is causing units with very small values of the propensity score to blow up and become unusually in\ufb02uential in the calculation of ATT. Thus, we will need to trim the data. Here we will do a very small trim to eliminate the mass of values at the far-left tail. Crump et al. [2009] develop a principled method for addressing a lack of overlap. A good rule of thumb, they\nnote, is to keep only observations on the interval [0.1,0.9], which was performed at the end of the program.\nNow let\u2019s repeat the analysis having trimmed the propensity score, keeping only values whose scores are between 0.1 and 0.9. Now we \ufb01nd $2,006 using the non-normalized weights and $1,806 using the normalized weights. This is very similar to what we know is the true causal effect using the experimental data, which was $1,794. And we can see that the normalized weights are even closer. We still need to calculate standard errors, such as based on a bootstrapping method, but I leave it to you investigate that more carefully by reading Adudumilli [2018] and Bodory et al. [2020], who, as I mentioned, dis- cuss the performance of various bootstrapping procedures such as the standard bootstrap and the wild bootstrap.\nNearest-neighbor matching. An alternative, very popular approach to inverse probability weighting is matching on the propensity score. This is often done by \ufb01nding a couple of units with comparable propen- sity scores from the control unit donor pool within some ad hoc cho- sen radius distance of the treated unit\u2019s own propensity score. The researcher then averages the outcomes and then assigns that average as an imputation to the original treated unit as a proxy for the potential outcome under counterfactual control. Then effort is made to enforce common support through trimming.\nBut this method has been criticized by King and Nielsen [2019]. The King and Nielsen [2019] critique is not of the propensity score itself. For instance, the critique does not apply to strati\ufb01cation based on the propensity score [Rosenbaum and Rubin, 1983], regression adjust- ment or inverse probability weighting. The problem is only focused on nearest-neighbor matching and is related to the forced balance through trimming as well as myriad other common research choices made in the course of the project that together ultimately amplify bias. King and Nielsen write: \u201cThe more balanced the data, or the more balance it becomes by [trimming] some of the observations through matching, the more likely propensity score matching will degrade inferences\u201d [2019, 1].\nNevertheless, nearest-neighbor matching, along with inverse prob- ability weighting, is perhaps the most common method for estimating\na propensity score model. Nearest-neighbor matching using the propensity score pairs each treatment unit i with one or more compa- rable control group units j, where comparability is measured in terms of distance to the nearest propensity score. This control group unit\u2019s outcome is then plugged into a matched sample.", "start_char_idx": 50167, "end_char_idx": 54273, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "afdf4866-84de-44a2-95d3-1ab79cc77502": {"__data__": {"id_": "afdf4866-84de-44a2-95d3-1ab79cc77502", "embedding": null, "metadata": {"page number": "219 - 262"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9fd33664-e6ea-4fe1-ae3b-699b771c4008", "node_type": "4", "metadata": {"page number": "219 - 262"}, "hash": "5a02e3ba8fff6bc12accb0f554c5cbd5e8cdb806355061364a0b698613b869b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dd5016ef-f821-422c-8809-4f03d2f6a652", "node_type": "1", "metadata": {"page number": "219 - 262"}, "hash": "8802b9ec8a083cff7d7832b920f931ab709bee66e8c6f6f65e9424d57504d4e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4f4c6529-934a-4f73-bd45-2f1b69e92f10", "node_type": "1", "metadata": {}, "hash": "f900061b86ac392cdbd966e07811570805e2d2f59cfcdaaffc555f3a2dc36744", "class_name": "RelatedNodeInfo"}}, "text": "The problem is only focused on nearest-neighbor matching and is related to the forced balance through trimming as well as myriad other common research choices made in the course of the project that together ultimately amplify bias. King and Nielsen write: \u201cThe more balanced the data, or the more balance it becomes by [trimming] some of the observations through matching, the more likely propensity score matching will degrade inferences\u201d [2019, 1].\nNevertheless, nearest-neighbor matching, along with inverse prob- ability weighting, is perhaps the most common method for estimating\na propensity score model. Nearest-neighbor matching using the propensity score pairs each treatment unit i with one or more compa- rable control group units j, where comparability is measured in terms of distance to the nearest propensity score. This control group unit\u2019s outcome is then plugged into a matched sample. Once we have the matched sample, we can calculate the ATT as\nwhere Yi(j) is the matched control group unit to i. We will focus on the ATT because of the problems with overlap that we discussed earlier.\n1 library(MatchIt) 2 library(Zelig) 3 4 m_out <- matchit(treat ~ age + agesq + agecube + educ + 5 6 7 8 9 10 m_data <- match.data(m_out) 11 12 z_out <- zelig(re78 ~ treat + age + agesq + agecube + educ + 13 14 15 16 17 x_out <- setx(z_out, treat = 0) 18 x1_out <- setx(z_out, treat = 1) 19 20 s_out <- sim(z_out, x = x_out, x1 = x1_out) 21 22 summary(s_out)\neducsq + marr + nodegree + black + hisp + re74 + re75 + u74 + u75 + interaction1, data = nsw_dw_cpscontrol, method = \"nearest\", distance = \"logit\", ratio =5)\nI chose to match using \ufb01ve nearest neighbors. Nearest neighbors, in other words, will \ufb01nd the \ufb01ve nearest units in the control group, where \u201cnearest\u201d is measured as closest on the propensity score itself. Unlike covariate matching, distance here is straightforward because of the dimension reduction afforded by the propensity score. We then average actual outcome, and match that average outcome to each treatment unit. Once we have that, we subtract each unit\u2019s matched control from its treatment value, and then divide by NT, the number of treatment units. When we do that in Stata, we get an ATT of $1,725 with p < 0.05. Thus, it is both relatively precise and similar to what we \ufb01nd with the experiment itself.\nCoarsened exact matching. There are two kinds of matching we\u2019ve reviewed so far. Exact matching matches a treated unit to all of the con- trol units with the same covariate value. But sometimes this is impos- sible, and therefore there are matching discrepancies. For instance, say that we are matching continuous age and continuous income. The probability we \ufb01nd another person with the exact same value of both is very small, if not zero. This leads therefore to mismatching on the covariates, which introduces bias.\nThe second kind of matching we\u2019ve discussed are approximate matching methods, which specify a metric to \ufb01nd control units that are \u201cclose\u201d to the treated unit. This requires a distance metric, such as Euclidean, Mahalanobis, or the propensity score. All of these can be implemented in Stata or R.\nIacus et al. [2012] introduced a kind of exact matching called coars- ened exact matching (CEM). The idea is very simple. It\u2019s based on the notion that sometimes it\u2019s possible to do exact matching once we coarsen the data enough. If we coarsen the data, meaning we create categorical variables (e.g., 0- to 10-year-olds, 11- to 20-year olds), then oftentimes we can \ufb01nd exact matches. Once we \ufb01nd those matches, we calculate weights on the basis of where a person \ufb01ts in some strata, and those weights are used in a simple weighted regression.\nFirst, we begin with covariates X and make a copy called X\u2217. Next we coarsen X\u2217 according to user-de\ufb01ned cutpoints or CEM\u2019s automatic binning algorithm. For instance, schooling becomes less than high\nschool, high school only, some college, college graduate, post col- lege. Then we create one stratum per unique observation of X\u2217 and place each observation in a stratum.", "start_char_idx": 53370, "end_char_idx": 57438, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4f4c6529-934a-4f73-bd45-2f1b69e92f10": {"__data__": {"id_": "4f4c6529-934a-4f73-bd45-2f1b69e92f10", "embedding": null, "metadata": {"page number": "219 - 262"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9fd33664-e6ea-4fe1-ae3b-699b771c4008", "node_type": "4", "metadata": {"page number": "219 - 262"}, "hash": "5a02e3ba8fff6bc12accb0f554c5cbd5e8cdb806355061364a0b698613b869b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "afdf4866-84de-44a2-95d3-1ab79cc77502", "node_type": "1", "metadata": {"page number": "219 - 262"}, "hash": "02aabf23c152146dea5d0f9b6bb2a3bd4c264aa431218002a44ad735ee8c18be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "17ca8703-e1d5-43ef-83e9-453a59278a17", "node_type": "1", "metadata": {}, "hash": "fde5abba90cd67c709574c5592551909eef9d8da7a5dc3869c34939d99864a64", "class_name": "RelatedNodeInfo"}}, "text": "If we coarsen the data, meaning we create categorical variables (e.g., 0- to 10-year-olds, 11- to 20-year olds), then oftentimes we can \ufb01nd exact matches. Once we \ufb01nd those matches, we calculate weights on the basis of where a person \ufb01ts in some strata, and those weights are used in a simple weighted regression.\nFirst, we begin with covariates X and make a copy called X\u2217. Next we coarsen X\u2217 according to user-de\ufb01ned cutpoints or CEM\u2019s automatic binning algorithm. For instance, schooling becomes less than high\nschool, high school only, some college, college graduate, post col- lege. Then we create one stratum per unique observation of X\u2217 and place each observation in a stratum. Assign these strata to the origi- nal and uncoarsened data, X, and drop any observation whose stratum doesn\u2019t contain at least one treated and control unit. Then add weights for stratum size and analyze without matching.\nBut there are trade-offs. Larger bins mean more coarsening of the data, which results in fewer strata. Fewer strata result in more diverse observations within the same strata and thus higher covariate imbalance. CEM prunes both treatment and control group units, which changes the parameter of interest, but so long as you\u2019re transparent about this and up front, readers may be willing to give you the bene\ufb01t of the doubt.20 Just know, though, that you are not estimating the ATE or the ATT when you start trimming (just as you aren\u2019t doing so when you trim propensity scores).\nThe key bene\ufb01t of CEM is that it is part of a class of matching methods called monotonic imbalance bounding (MIB). MIB methods bound the maximum imbalance in some feature of the empirical distri- butions by an ex ante decision by the user. In CEM, this ex ante choice is the coarsening decision. By choosing the coarsening beforehand, users can control the amount of imbalance in the matching solution. It\u2019s also very fast.\nThere are several ways of measuring imbalance, but here we focus\non the L1(f, g) measure, which is\nwhere f and g record the relative frequencies for the treatment and con- trol group units. Perfect global balance is indicated by L1 = 0. Larger values indicate larger imbalance between the groups, with a maximum of L1 = 1. Hence the \u201cimbalance bounding\u201d between 0 and 1.\nNow let\u2019s get to the fun part: estimation. We will use the same\njob-training data we\u2019ve been working with for this estimation.\n20 They also may not. The methods are easy. It\u2019s convincing readers that\u2019s hard.\n1 ssc install cem 2 3 * Reload experimental group data 4 use https://github.com/scunning1975/mixtape/raw/master/nsw_mixtape.dta,\nclear 5 drop if treat==0 6 7 * Now merge in the CPS controls from footnote 2 of Table 2 (Dehejia and Wahba\n8 append using\n9 gen agesq=age*age 10 gen agecube=age*age*age 11 gen edusq=educ*edu 12 gen u74 = 0 if re74!=. 13 replace u74 = 1 if re74==0 14 gen u75 = 0 if re75!=. 15 replace u75 = 1 if re75==0 16 gen interaction1 = educ*re74 17 gen re74sq=re74^2 18 gen re75sq=re75^2 19 gen interaction2 = u74*hisp 20 21 cem age (10 20 30 40 60) age agesq agecube educ edusq marr nodegree black\n(continued)\nR (continued)\n8 m_out <- matchit(treat ~ age + agesq + agecube + educ + 9 10 11 12 13 14 15 16 m_data <- match.data(m_out) 17 18 m_ate <- lm_robust(re78 ~ treat, 19 20\ndata = m_data, weights = m_data$weights)\nThe estimated ATE is $2,152, which is larger than our estimated experimental effect. But this ensured a high degree of balance on the covariates, as can be seen from the output from the cem command itself.\nAs can be seen from Table 39, the values of L1 are close to zero in\nmost cases. The largest L1 gets is 0.12 for age squared.\nTable 39.", "start_char_idx": 56754, "end_char_idx": 60419, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "17ca8703-e1d5-43ef-83e9-453a59278a17": {"__data__": {"id_": "17ca8703-e1d5-43ef-83e9-453a59278a17", "embedding": null, "metadata": {"page number": "219 - 262"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9fd33664-e6ea-4fe1-ae3b-699b771c4008", "node_type": "4", "metadata": {"page number": "219 - 262"}, "hash": "5a02e3ba8fff6bc12accb0f554c5cbd5e8cdb806355061364a0b698613b869b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4f4c6529-934a-4f73-bd45-2f1b69e92f10", "node_type": "1", "metadata": {"page number": "219 - 262"}, "hash": "ccce67cc59b0caa5978c507b4fcc677cc13b9eec12fd3ba05eb5519a19f9145a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f295f988-3d16-4de6-b1c5-6b009099f0a9", "node_type": "1", "metadata": {}, "hash": "3fa0fea92be6f9cde6a70ea2b80f203199888aefd3e83d99ae62feb048d3e440", "class_name": "RelatedNodeInfo"}}, "text": "But this ensured a high degree of balance on the covariates, as can be seen from the output from the cem command itself.\nAs can be seen from Table 39, the values of L1 are close to zero in\nmost cases. The largest L1 gets is 0.12 for age squared.\nTable 39. Balance in covariates after coarsened exact matching.\nage agesq agecube school schoolsq married nodegree black hispanic re74 re75 u74 u75 interaction1\nConclusion. Matching methods are an important member of the causal inference arsenal. Propensity scores are an excellent tool to check the balance and overlap of covariates. It\u2019s an under-appreciated diagnos- tic, and one that you might miss if you only ran regressions. There are extensions for more than two treatments, like multinomial models, but I don\u2019t cover those here. The propensity score can make groups com- parable, but only on the variables used to estimate the propensity score in the \ufb01rst place. It is an area that continues to advance to include covariate balancing [Imai and Ratkovic, 2013; Zhao, 2019; Zubizarreta, 2015] and doubly robust estimators [Band and Robins, 2005]. Consider this chapter more about the mechanics of matching when you have exact and approximate matching situations.\nLearning about the propensity score is particularly valuable given that it appears to have a very long half-life. For instance, propensity scores make their way into other contemporary designs too, such as difference-in-differences [Sant\u2019Anna and Zhao, 2018]. So investing in a basic understanding of these ideas and methods is likely worthwhile. You never know when the right project comes along for which these methods are the perfect solution, so there\u2019s no intelligent reason to write them off.\nBut remember, every matching solution to a causality problem requires a credible belief that the backdoor criterion can be achieved by conditioning on some matrix X, or what we\u2019ve called CIA. This explicitly requires that there are no unobservable variables opening backdoor paths as confounders, which to many researchers requires a leap of faith so great they are unwilling to make it. In some respects, CIA is somewhat advanced because it requires deep institutional knowledge to say with con\ufb01dence that no such unobserved confounder exists. The method is easy compared to such domain-speci\ufb01c knowledge. So if you have good reason to believe that there are important, unobserv- able variables, you will need another tool. But if you are willing to make such an assumption, then these methods and others could be useful for you in your projects.", "start_char_idx": 60164, "end_char_idx": 62726, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f295f988-3d16-4de6-b1c5-6b009099f0a9": {"__data__": {"id_": "f295f988-3d16-4de6-b1c5-6b009099f0a9", "embedding": null, "metadata": {"page number": "263 - 263", "chapter": "Regression Discontinuity"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3f6fab02-0aa2-4ef7-a94c-950957c48764", "node_type": "4", "metadata": {"page number": "263 - 263", "chapter": "Regression Discontinuity"}, "hash": "d5e558d1a65cda46ab638abf00141eab3e1386e5ab12e4a816317931541a4c75", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "17ca8703-e1d5-43ef-83e9-453a59278a17", "node_type": "1", "metadata": {"page number": "219 - 262"}, "hash": "66f6903ebe90a2a369b0b0d777c07b1dc3746b016154255ea36d77ac2f020ede", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd244716-6b65-423a-a321-a377abd5f8ac", "node_type": "1", "metadata": {}, "hash": "9d9830923e739f35f086bea2eda328cd7e4dd057eb5391f8a947c9df469be38a", "class_name": "RelatedNodeInfo"}}, "text": "Regression Discontinuity:\nJump around! Jump around! Jump up, jump up, and get", "start_char_idx": 0, "end_char_idx": 77, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd244716-6b65-423a-a321-a377abd5f8ac": {"__data__": {"id_": "fd244716-6b65-423a-a321-a377abd5f8ac", "embedding": null, "metadata": {"page number": "263 - 274", "chapter": "Regression Discontinuity"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3d8b6c2b-0a22-45a8-bbb1-03011dbcff45", "node_type": "4", "metadata": {"page number": "263 - 274", "chapter": "Regression Discontinuity"}, "hash": "ea2ad24aafa5620f6f3f6b9022fda32021a06594845cd8bc20ef2423f02119e6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f295f988-3d16-4de6-b1c5-6b009099f0a9", "node_type": "1", "metadata": {"page number": "263 - 263", "chapter": "Regression Discontinuity"}, "hash": "9b0dfa37abe4d66277b5ddaf9ab2ad0fbe5d27d53b8bab18a45da88f2da35ced", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "920e24fe-4a2e-4074-ac1a-ea33b04ed8f1", "node_type": "1", "metadata": {}, "hash": "654c37eecc7870edb19717481b504a45f4acc8c6f555b13a2a85dc73fa8dcd35", "class_name": "RelatedNodeInfo"}}, "text": "Huge Popularity of Regression Discontinuity:\nWaiting for life. Over the past twenty years, interest in the regression- discontinuity design (RDD) has increased (Figure 19). It was not always so popular, though. The method dates back about sixty years to Don- ald Campbell, an educational psychologist, who wrote several stud- ies using it, beginning with Thistlehwaite and Campbell [1960].1 In a wonderful article on the history of thought around RDD, Cook [2008] documents its social evolution. Despite Campbell\u2019s many efforts to advocate for its usefulness and understand its properties, RDD did not catch on beyond a few doctoral students and a handful of papers here and there. Eventually, Campbell too moved on from it.\n1 Thistlehwaite and Campbell [1960] studied the effect of merit awards on future\nacademic outcomes. Merit awards were given out to students based on a score, and\nanyone with a score above some cutoff received the merit award, whereas everyone\nbelow that cutoff did not. Knowing the treatment assignment allowed the authors to carefully estimate the causal effect of merit awards on future academic performance.\nTo see its growing popularity, let\u2019s look at counts of papers from Google Scholar by year that mentioned the phrase \u201cregression dis- continuity design\u201d (see Figure 19).2 Thistlehwaite and Campbell [1960] had no in\ufb02uence on the broader community of scholars using his design, con\ufb01rming what Cook [2008] wrote. The \ufb01rst time RDD appears in the economics community is with an unpublished econometrics paper [Goldberger, 1972]. Starting in 1976, RDD \ufb01nally gets annual double-digit usage for the \ufb01rst time, after which it begins to slowly tick upward. But for the most part, adoption was imperceptibly slow.\nBut then things change starting in 1999. That\u2019s the year when a couple of notable papers in the prestigious Quarterly Journal of Eco- nomics resurrected the method. These papers were Angrist and Lavy [1999] and Black [1999], followed by Hahn et al. [2001] two years later. Angrist and Lavy [1999], which we discuss in detail later, studied the effect of class size on pupil achievement using an unusual feature in Israeli public schools that created smaller classes when the number of students passed a particular threshold. Black [1999] used a kind of\n2 Hat tip to John Holbein for giving me these data.\nRDD approach when she creatively exploited discontinuities at the geo- graphical level created by school district zoning to estimate people\u2019s willingness to pay for better schools. The year 1999 marks a watershed in the design\u2019s widespread adoption. A 2010 Journal of Economic Liter- ature article by Lee and Lemieux, which has nearly 4,000 cites shows up in a year with nearly 1,500 new papers mentioning the method. By 2019, RDD output would be over 5,600. The design is today incredibly popular and shows no sign of slowing down.\nBut 1972 to 1999 is a long time without so much as a peep for what is now considered one of the most credible research designs with observational data, so what gives? Cook [2008] says that RDD was \u201cwaiting for life\u201d during this time. The conditions for life in empiri- cal microeconomics were likely the growing acceptance of the poten- tial outcomes framework among microeconomists (i.e., the so-called credibility revolution led by Josh Angrist, David Card, Alan Krueger, Steven Levitt, and many others) as well as, and perhaps even more importantly, the increased availability of large digitized the adminis- trative data sets, many of which often captured unusual administra- tive rules for treatment assignments. These unusual rules, combined with the administrative data sets\u2019 massive size, provided the much- needed necessary conditions for Campbell\u2019s original design to bloom into thousands of \ufb02owers.\nGraphical representation of RDD. So what\u2019s the big deal? Why is RDD so special? The reason RDD is so appealing to many is because of its ability to convincingly eliminate selection bias. This appeal is partly due to the fact that its underlying identifying assumptions are viewed by many as easier to accept and evaluate. Rendering selection bias impotent, the procedure is capable of recovering average treatment effects for a given subpopulation of units. The method is based on a simple, intuitive idea. Consider the following DAG developed by Steiner et al. [2017] that illustrates this method very well.", "start_char_idx": 0, "end_char_idx": 4396, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "920e24fe-4a2e-4074-ac1a-ea33b04ed8f1": {"__data__": {"id_": "920e24fe-4a2e-4074-ac1a-ea33b04ed8f1", "embedding": null, "metadata": {"page number": "263 - 274", "chapter": "Regression Discontinuity"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3d8b6c2b-0a22-45a8-bbb1-03011dbcff45", "node_type": "4", "metadata": {"page number": "263 - 274", "chapter": "Regression Discontinuity"}, "hash": "ea2ad24aafa5620f6f3f6b9022fda32021a06594845cd8bc20ef2423f02119e6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fd244716-6b65-423a-a321-a377abd5f8ac", "node_type": "1", "metadata": {"page number": "263 - 274", "chapter": "Regression Discontinuity"}, "hash": "e53286ed95d576b4b3b24a589a7fe1747ff087a459cb324bd438ad1c8698d70c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ed91555d-571b-4339-9bd1-97891f68a8a8", "node_type": "1", "metadata": {}, "hash": "be7980a26b0b12dae875d8ccd71d467ca2ad74de6925d3317aed094a981da39a", "class_name": "RelatedNodeInfo"}}, "text": "These unusual rules, combined with the administrative data sets\u2019 massive size, provided the much- needed necessary conditions for Campbell\u2019s original design to bloom into thousands of \ufb02owers.\nGraphical representation of RDD. So what\u2019s the big deal? Why is RDD so special? The reason RDD is so appealing to many is because of its ability to convincingly eliminate selection bias. This appeal is partly due to the fact that its underlying identifying assumptions are viewed by many as easier to accept and evaluate. Rendering selection bias impotent, the procedure is capable of recovering average treatment effects for a given subpopulation of units. The method is based on a simple, intuitive idea. Consider the following DAG developed by Steiner et al. [2017] that illustrates this method very well.\nIn the \ufb01rst graph, X is a continuous variable assigning units to treat- ment D (X \u2192 D). This assignment of units to treatment is based on a \u201ccutoff\u201d score c0 such that any unit with a score above the cutoff gets placed into the treatment group, and units below do not. An example\nmight be a charge of driving while intoxicated (or impaired; DWI). Indi- viduals with a blood-alcohol content of 0.08 or higher are arrested and charged with DWI, whereas those with a blood-alcohol level below 0.08 are not [Hansen, 2015]. The assignment variable may itself indepen- dently affect the outcome via the X \u2192 Y path and may even be related to a set of variables U that independently determine Y. Notice for the moment that a unit\u2019s treatment status is exclusively determined by the assignment rule. Treatment is not determined by U.\nThis DAG clearly shows that the assignment variable X\u2014or what is often called the \u201crunning variable\u201d\u2014is an observable confounder since it causes both D and Y. Furthermore, because the assignment vari- able assigns treatment on the basis of a cutoff, we are never able to observe units in both treatment and control for the same value of X. Calling back to our matching chapter, this means a situation such as this one does not satisfy the overlap condition needed to use matching methods, and therefore the backdoor criterion cannot be met.3\nHowever, we can identify causal effects using RDD, which is illus- trated in the limiting graph DAG. We can identify causal effects for those subjects whose score is in a close neighborhood around some cutoff c0. Speci\ufb01cally, as we will show, the average causal effect for\n3 Think about it for a moment. The backdoor criterion calculates differences in expected outcomes between treatment and control for a given value of X. But if the assignment variable only moves units into treatment when X passes some cutoff, then such calculations are impossible because there will not be units in treatment and control for any given value of X.\nthis subpopulation is identi\ufb01ed as X \u2192 c0 in the limit. This is possi- ble because the cutoff is the sole point where treatment and control subjects overlap in the limit.\nThere are a variety of explicit assumptions buried in this graph that must hold in order for the methods we will review later to recover any average causal effect. But the main one I discuss here is that the cutoff itself cannot be endogenous to some competing intervention occur- ring at precisely the same moment that the cutoff is triggering units into the D treatment category. This assumption is called continuity, and what it formally means is that the expected potential outcomes are continuous at the cutoff. If expected potential outcomes are continu- ous at the cutoff, then it necessarily rules out competing interventions occurring at the same time.\nThe continuity assumption is re\ufb02ected graphically by the absence of an arrow from X \u2192 Y in the second graph because the cutoff c0 has cut it off. At c0, the assignment variable X no longer has a direct effect on Y. Understanding continuity should be one of your main goals in this chapter. It is my personal opinion that the null hypothesis should always be continuity and that any discontinuity necessarily implies some cause, because the tendency for things to change gradually is what we have come to expect in nature. Jumps are so unnatural that when we see them happen, they beg for explanation. Charles Darwin, in his On the Origin of Species, summarized this by saying Natura non facit saltum, or \u201cnature does not make jumps.\u201d Or to use a favorite phrase of mine from growing up in Mississippi, if you see a turtle on a fencepost, you know he didn\u2019t get there by himself.\nThat\u2019s the heart and soul of RDD. We use our knowledge about selection into treatment in order to estimate average treatment effects.", "start_char_idx": 3596, "end_char_idx": 8245, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ed91555d-571b-4339-9bd1-97891f68a8a8": {"__data__": {"id_": "ed91555d-571b-4339-9bd1-97891f68a8a8", "embedding": null, "metadata": {"page number": "263 - 274", "chapter": "Regression Discontinuity"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3d8b6c2b-0a22-45a8-bbb1-03011dbcff45", "node_type": "4", "metadata": {"page number": "263 - 274", "chapter": "Regression Discontinuity"}, "hash": "ea2ad24aafa5620f6f3f6b9022fda32021a06594845cd8bc20ef2423f02119e6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "920e24fe-4a2e-4074-ac1a-ea33b04ed8f1", "node_type": "1", "metadata": {"page number": "263 - 274", "chapter": "Regression Discontinuity"}, "hash": "a2f4a8aa02eefee40da775b2db0ad0aa351ae52c2273019347027c584c72cd67", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "524756d1-9882-496d-b338-067374826496", "node_type": "1", "metadata": {}, "hash": "04ee291c56da1d9877b0b5e343935b1d406f247d611821a659ab520ab5c8f212", "class_name": "RelatedNodeInfo"}}, "text": "At c0, the assignment variable X no longer has a direct effect on Y. Understanding continuity should be one of your main goals in this chapter. It is my personal opinion that the null hypothesis should always be continuity and that any discontinuity necessarily implies some cause, because the tendency for things to change gradually is what we have come to expect in nature. Jumps are so unnatural that when we see them happen, they beg for explanation. Charles Darwin, in his On the Origin of Species, summarized this by saying Natura non facit saltum, or \u201cnature does not make jumps.\u201d Or to use a favorite phrase of mine from growing up in Mississippi, if you see a turtle on a fencepost, you know he didn\u2019t get there by himself.\nThat\u2019s the heart and soul of RDD. We use our knowledge about selection into treatment in order to estimate average treatment effects. Since we know the probability of treatment assignment changes dis- continuously at c0, then our job is simply to compare people above and below c0 to estimate a particular kind of average treatment effect called the local average treatment effect, or LATE [Imbens and Angrist, 1994]. Because we do not have overlap, or \u201ccommon support,\u201d we must rely on extrapolation, which means we are comparing units with differ- ent values of the running variable. They only overlap in the limit as X approaches the cutoff from either direction. All methods used for RDD are ways of handling the bias from extrapolation as cleanly as possible.\nA picture is worth a thousand words. As I\u2019ve said before, and will say again and again\u2014pictures of your main results, including your identi- \ufb01cation strategy, are absolutely essential to any study attempting to convince readers of a causal effect. And RDD is no different. In fact, pictures are the comparative advantage of RDD. RDD is, like many modern designs, a very visually intensive design. It and synthetic con- trol are probably two of the most visually intensive designs you\u2019ll ever encounter, in fact. So to help make RDD concrete, let\u2019s \ufb01rst look at a couple of pictures. The following discussion derives from Hoekstra [2009].4\nLabor economists had for decades been interested in estimating the causal effect of college on earnings. But Hoekstra wanted to crack open the black box of college\u2019s returns a little by checking whether there were heterogeneous returns to college. He does this by estimat- ing the causal effect of attending the state \ufb02agship university on earn- ings. State \ufb02agship universities are often more selective than other public universities in the same state. In Texas, the top 7% of gradu- ating high school students can select their university in state, and the modal \ufb01rst choice is University of Texas at Austin. These universities are often environments of higher research, with more resources and strongly positive peer effects. So it is natural to wonder whether there are heterogeneous returns across public universities.\nThe challenge in this type of question should be easy to see. Let\u2019s say that we were to compare individuals who attended the University of Florida to those who attended the University of South Florida. Insofar as there is positive selection into the state \ufb02agship school, we might expect individuals with higher observed and unobserved ability to sort into the state \ufb02agship school. And insofar as that ability increases one\u2019s marginal product, then we expect those individuals to earn more in the workforce regardless of whether they had in fact attended the state \ufb02agship. Such basic forms of selection bias confound our ability to estimate the causal effect of attending the state \ufb02agship on earn- ings. But Hoekstra [2009] had an ingenious strategy to disentangle the causal effect from the selection bias using an RDD. To illustrate, let\u2019s look at two pictures associated with this interesting study.\n4 Mark Hoekstra is one of the more creative microeconomists I have met when it comes to devising compelling strategies for identifying causal effects in observational data, and this is one of my favorite papers by him.\nBefore talking about the picture, I want to say something about the data. Hoekstra has data on all applications to the state \ufb02agship university. To get these data, he would\u2019ve had to build a relationship with the admissions o\ufb03ce. This would have involved making introduc- tions, holding meetings to explain his project, convincing administra- tors the project had value for them as well as him, and ultimately win- ning their approval to cooperatively share the data.", "start_char_idx": 7379, "end_char_idx": 11931, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "524756d1-9882-496d-b338-067374826496": {"__data__": {"id_": "524756d1-9882-496d-b338-067374826496", "embedding": null, "metadata": {"page number": "263 - 274", "chapter": "Regression Discontinuity"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3d8b6c2b-0a22-45a8-bbb1-03011dbcff45", "node_type": "4", "metadata": {"page number": "263 - 274", "chapter": "Regression Discontinuity"}, "hash": "ea2ad24aafa5620f6f3f6b9022fda32021a06594845cd8bc20ef2423f02119e6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ed91555d-571b-4339-9bd1-97891f68a8a8", "node_type": "1", "metadata": {"page number": "263 - 274", "chapter": "Regression Discontinuity"}, "hash": "b4ddc5d3b957d8d7ab1b633a455b9d18634673587c2a1bfe0b218542e78816f2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "305a7aac-41cd-437d-9711-aab926ecee25", "node_type": "1", "metadata": {}, "hash": "e3ebbaf3adbefae71f6aa75d6f387ebd070cb999cef4912b56e5d2bbeccc0ba1", "class_name": "RelatedNodeInfo"}}, "text": "But Hoekstra [2009] had an ingenious strategy to disentangle the causal effect from the selection bias using an RDD. To illustrate, let\u2019s look at two pictures associated with this interesting study.\n4 Mark Hoekstra is one of the more creative microeconomists I have met when it comes to devising compelling strategies for identifying causal effects in observational data, and this is one of my favorite papers by him.\nBefore talking about the picture, I want to say something about the data. Hoekstra has data on all applications to the state \ufb02agship university. To get these data, he would\u2019ve had to build a relationship with the admissions o\ufb03ce. This would have involved making introduc- tions, holding meetings to explain his project, convincing administra- tors the project had value for them as well as him, and ultimately win- ning their approval to cooperatively share the data. This likely would\u2019ve involved the school\u2019s general counsel, careful plans to de-identify the data, agreements on data storage, and many other assurances that students\u2019 names and identities were never released and could not be identi\ufb01ed. There is a lot of trust and social capital that must be created to do projects like this, and this is the secret sauce in most RDDs\u2014your acquisition of the data requires far more soft skills, such as friendship, respect, and the building of alliances, than you may be accustomed to. This isn\u2019t as straightforward as simply downloading the CPS from IPUMS; it\u2019s going to take genuine smiles, hustle, and luck. Given that these agencies have considerable discretion in whom they release data to, it is likely that certain groups will have more trouble than others in acquiring the data. So it is of utmost importance that you approach these individuals with humility, genuine curiosity, and most of all, sci- enti\ufb01c integrity. They ultimately are the ones who can give you the data if it is not public use, so don\u2019t be a jerk.5\nBut on to the picture. Figure 20 has a lot going on, and it\u2019s worth carefully unpacking each element for the reader. There are four dis- tinct elements to this picture that I want to focus on. First, notice the horizontal axis. It ranges from a negative number to a positive number with a zero around the center of the picture. The caption reads \u201cSAT Points Above or Below the Admission Cutoff.\u201d Hoekstra has \u201crecen- tered\u201d the university\u2019s admissions criteria by subtracting the admission cutoff from the students\u2019 actual score, which is something I discuss in more detail later in this chapter. The vertical line at zero marks the \u201ccutoff,\u201d which was this university\u2019s minimum SAT score for admis- sions. It appears it was binding, but not deterministically, for there are some students who enrolled but did not have the minimum SAT\n5 \u201cDon\u2019t be a jerk\u201d applies even to situations when you aren\u2019t seeking proprietary\nFigure 20. Attending the state \ufb02agship university as a function of recentered standard-\nized test scores. Reprinted from Mark Hoekstra, \u201cThe Effect of Attending\nthe Flagship State University on Earnings: A Discontinuity-Based Approach,\u201d The Review of Economics and Statistics, 91:4 (November, 2009), pp. 717\u2013 724. \u00a9 2009 by the President and Fellows of Harvard College and the Massachusetts Institute of Technology.\nrequirements. These individuals likely had other quali\ufb01cations that compensated for their lower SAT scores. This recentered SAT score is in today\u2019s parlance called the \u201crunning variable.\u201d\nSecond, notice the dots. Hoekstra used hollow dots at regular intervals along the recentered SAT variable. These dots represent con- ditional mean enrollments per recentered SAT score. While his admin- istrative data set contains thousands and thousands of observations, he only shows the conditional means along evenly spaced out bins of the recentered SAT score.\nThird are the curvy lines \ufb01tting the data. Notice that the picture has two such lines\u2014there is a curvy line \ufb01tted to the left of zero, and there is a separate line \ufb01t to the right. These lines are the least squares \ufb01tted values of the running variable, where the running variable was allowed to take on higher-order terms. By including higher-order terms in the regression itself, the \ufb01tted values are allowed to more \ufb02exibly track the central tendencies of the data itself. But the thing I really want to focus\nyour attention on is that there are two lines, not one.", "start_char_idx": 11046, "end_char_idx": 15448, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "305a7aac-41cd-437d-9711-aab926ecee25": {"__data__": {"id_": "305a7aac-41cd-437d-9711-aab926ecee25", "embedding": null, "metadata": {"page number": "263 - 274", "chapter": "Regression Discontinuity"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3d8b6c2b-0a22-45a8-bbb1-03011dbcff45", "node_type": "4", "metadata": {"page number": "263 - 274", "chapter": "Regression Discontinuity"}, "hash": "ea2ad24aafa5620f6f3f6b9022fda32021a06594845cd8bc20ef2423f02119e6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "524756d1-9882-496d-b338-067374826496", "node_type": "1", "metadata": {"page number": "263 - 274", "chapter": "Regression Discontinuity"}, "hash": "f45f17f0a78db12c3268cf227de3bb868a3022ea486ba94b049679fb64134bda", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5bd2545-bd5c-4c98-92d9-7543ca302bb5", "node_type": "1", "metadata": {}, "hash": "907cfdd8516e316d1accd7cc9e1376d1e3374976ad31ce96b791909b0093677d", "class_name": "RelatedNodeInfo"}}, "text": "These dots represent con- ditional mean enrollments per recentered SAT score. While his admin- istrative data set contains thousands and thousands of observations, he only shows the conditional means along evenly spaced out bins of the recentered SAT score.\nThird are the curvy lines \ufb01tting the data. Notice that the picture has two such lines\u2014there is a curvy line \ufb01tted to the left of zero, and there is a separate line \ufb01t to the right. These lines are the least squares \ufb01tted values of the running variable, where the running variable was allowed to take on higher-order terms. By including higher-order terms in the regression itself, the \ufb01tted values are allowed to more \ufb02exibly track the central tendencies of the data itself. But the thing I really want to focus\nyour attention on is that there are two lines, not one. He \ufb01t the lines separately to the left and right of the cutoff.\nFinally, and probably the most vivid piece of information in this picture\u2014the gigantic jump in the dots at zero on the recentered run- ning variable. What is going on here? Well, I think you probably know, but let me spell it out. The probability of enrolling at the \ufb02agship state university jumps discontinuously when the student just barely hits the minimum SAT score required by the school. Let\u2019s say that the score was 1250. That means a student with 1240 had a lower chance of get- ting in than a student with 1250. Ten measly points and they have to go a different path.\nImagine two students\u2014the \ufb01rst student got a 1240, and the sec- ond got a 1250. Are these two students really so different from one another? Well, sure: those two individual students are likely very dif- ferent. But what if we had hundreds of students who made 1240 and hundreds more who made 1250. Don\u2019t you think those two groups are probably pretty similar to one another on observable and unobservable characteristics? After all, why would there be suddenly at 1250 a major difference in the characteristics of the students in a large sample? That\u2019s the question you should re\ufb02ect on. If the university is arbitrar- ily picking a reasonable cutoff, are there reasons to believe they are also picking a cutoff where the natural ability of students jumps at that exact spot?\nBut I said Hoekstra is evaluating the effect of attending the state \ufb02agship university on future earnings. Here\u2019s where the study gets even more intriguing. States collect data on workers in a variety of ways. One is through unemployment insurance tax reports. Hoekstra\u2019s part- ner, the state \ufb02agship university, sent the university admissions data directly to a state o\ufb03ce in which employers submit unemployment insurance tax reports. The university had social security numbers, so the matching of student to future worker worked quite well since a social security number uniquely identi\ufb01es a worker. The social secu- rity numbers were used to match quarterly earnings records from 1998 through the second quarter of 2005 to the university records. He then estimated:\nln(Earnings) = \u03c8Year + \u03c9Experience + \u03b8Cohort + \u03b5\nwhere \u03c8 is a vector of year dummies, \u03c9 is a dummy for years after high school that earnings were observed, and \u03b8 is a vector of dum- mies controlling for the cohort in which the student applied to the university (e.g., 1988). The residuals from this regression were then averaged for each applicant, with the resulting average residual earn- ings measure being used to implement a partialled out future earnings variable according to the Frisch-Waugh-Lovell theorem. Hoekstra then takes each students\u2019 residuals from the natural log of earnings regres- sion and collapses them into conditional averages for bins along the recentered running variable. Let\u2019s look at that in Figure 21.\nIn this picture, we see many of the same elements we saw in Figure 20. For instance, we see the recentered running variable along the horizontal axis, the little hollow dots representing conditional means, the curvy lines which were \ufb01t left and right of the cutoff at zero, and a helpful vertical line at zero. But now we also have an interesting title: \u201cEstimated Discontinuity = 0.095 (z = 3.01).\u201d What is this exactly?", "start_char_idx": 14623, "end_char_idx": 18789, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b5bd2545-bd5c-4c98-92d9-7543ca302bb5": {"__data__": {"id_": "b5bd2545-bd5c-4c98-92d9-7543ca302bb5", "embedding": null, "metadata": {"page number": "263 - 274", "chapter": "Regression Discontinuity"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3d8b6c2b-0a22-45a8-bbb1-03011dbcff45", "node_type": "4", "metadata": {"page number": "263 - 274", "chapter": "Regression Discontinuity"}, "hash": "ea2ad24aafa5620f6f3f6b9022fda32021a06594845cd8bc20ef2423f02119e6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "305a7aac-41cd-437d-9711-aab926ecee25", "node_type": "1", "metadata": {"page number": "263 - 274", "chapter": "Regression Discontinuity"}, "hash": "658528689b84976919e5e99e421b249cedf985eeb68234d1ccc25237a00a380d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "51f7e9f5-47c0-41cf-a990-091928260203", "node_type": "1", "metadata": {}, "hash": "6b122b62f82a32e57d4384c868fa970b133359012d90ee833d33e2c43fe529e8", "class_name": "RelatedNodeInfo"}}, "text": "The residuals from this regression were then averaged for each applicant, with the resulting average residual earn- ings measure being used to implement a partialled out future earnings variable according to the Frisch-Waugh-Lovell theorem. Hoekstra then takes each students\u2019 residuals from the natural log of earnings regres- sion and collapses them into conditional averages for bins along the recentered running variable. Let\u2019s look at that in Figure 21.\nIn this picture, we see many of the same elements we saw in Figure 20. For instance, we see the recentered running variable along the horizontal axis, the little hollow dots representing conditional means, the curvy lines which were \ufb01t left and right of the cutoff at zero, and a helpful vertical line at zero. But now we also have an interesting title: \u201cEstimated Discontinuity = 0.095 (z = 3.01).\u201d What is this exactly?\nReprinted from Mark Hoekstra, \u201cThe Effect of Attending the Flagship State University on Earnings: A Discontinuity-Based Approach,\u201d The Review of Eco- nomics and Statistics, 91:4 (November, 2009), pp. 717\u2013724. \u00a9 2009 by the President and Fellows of Harvard College and the Massachusetts Institute\nThe visualization of a discontinuous jump at zero in earnings isn\u2019t as compelling as the prior \ufb01gure, so Hoekstra conducts hypothesis tests to determine if the mean between the groups just below and just above are the same. He \ufb01nds that they are not: those just above the cut- off earn 9.5% higher wages in the long term than do those just below. In his paper, he experiments with a variety of binning of the data (what he calls the \u201cbandwidth\u201d), and his estimates when he does so range from 7.4% to 11.1%.\nNow let\u2019s think for a second about what Hoekstra is \ufb01nding. Hoek- stra is \ufb01nding that at exactly the point where workers experienced a jump in the probability of enrolling at the state \ufb02agship university, there is, ten to \ufb01fteen years later, a separate jump in logged earnings of around 10%. Those individuals who just barely made it in to the state \ufb02agship university made around 10% more in long-term earnings than those individuals who just barely missed the cutoff.\nThis, again, is the heart and soul of the RDD. By exploiting insti- tutional knowledge about how students were accepted (and subse- quently enrolled) into the state \ufb02agship university, Hoekstra was able to craft an ingenious natural experiment. And insofar as the two groups of applicants right around the cutoff have comparable future earnings in a world where neither attended the state \ufb02agship university, then there is no selection bias confounding his comparison. And we see this result in powerful, yet simple graphs. This study was an early one to show that not only does college matter for long-term earnings, but the sort of college you attend\u2014even among public universities\u2014matters as well.\nData requirements for RDD. RDD is all about \ufb01nding \u201cjumps\u201d in the prob- ability of treatment as we move along some running variable X. So where do we \ufb01nd these jumps? Where do we \ufb01nd these discontinu- ities? The answer is that humans often embed jumps into rules. And sometimes, if we are lucky, someone gives us the data that allows us to use these rules for our study.\nI am convinced that \ufb01rms and government agencies are unknow- ingly sitting atop a mountain of potential RDD-based projects. Stu- dents looking for thesis and dissertation ideas might try to \ufb01nd them. I encourage you to \ufb01nd a topic you are interested in and begin build- ing relationships with local employers and government administrators\nfor whom that topic is a priority. Take them out for coffee, get to know them, learn about their job, and ask them how treatment assign- ment works. Pay close attention to precisely how individual units get assigned to the program. Is it random? Is it via a rule? Oftentimes they will describe a process whereby a running variable is used for treatment assignment, but they won\u2019t call it that. While I can\u2019t promise this will yield pay dirt, my hunch, based in part on experience, is that they will end up describing to you some running variable that when it exceeds a threshold, people switch into some intervention. Build- ing alliances with local \ufb01rms and agencies can pay when trying to \ufb01nd good research ideas.", "start_char_idx": 17910, "end_char_idx": 22189, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "51f7e9f5-47c0-41cf-a990-091928260203": {"__data__": {"id_": "51f7e9f5-47c0-41cf-a990-091928260203", "embedding": null, "metadata": {"page number": "263 - 274", "chapter": "Regression Discontinuity"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3d8b6c2b-0a22-45a8-bbb1-03011dbcff45", "node_type": "4", "metadata": {"page number": "263 - 274", "chapter": "Regression Discontinuity"}, "hash": "ea2ad24aafa5620f6f3f6b9022fda32021a06594845cd8bc20ef2423f02119e6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b5bd2545-bd5c-4c98-92d9-7543ca302bb5", "node_type": "1", "metadata": {"page number": "263 - 274", "chapter": "Regression Discontinuity"}, "hash": "e815688188853a6746b905f9572a20e15bb5e6bac8a95b3d1c0252ed8c435e87", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d4e3a577-aac6-4049-b886-2d6f85cc7ff3", "node_type": "1", "metadata": {}, "hash": "191c8fbd6ea814b6c4a0f10ed557ac8f6a29b3af1b13ab760c41660e8e775dfc", "class_name": "RelatedNodeInfo"}}, "text": "I encourage you to \ufb01nd a topic you are interested in and begin build- ing relationships with local employers and government administrators\nfor whom that topic is a priority. Take them out for coffee, get to know them, learn about their job, and ask them how treatment assign- ment works. Pay close attention to precisely how individual units get assigned to the program. Is it random? Is it via a rule? Oftentimes they will describe a process whereby a running variable is used for treatment assignment, but they won\u2019t call it that. While I can\u2019t promise this will yield pay dirt, my hunch, based in part on experience, is that they will end up describing to you some running variable that when it exceeds a threshold, people switch into some intervention. Build- ing alliances with local \ufb01rms and agencies can pay when trying to \ufb01nd good research ideas.\nThe validity of an RDD doesn\u2019t require that the assignment rule be arbitrary. It only requires that it be known, precise and free of manipu- lation. The most effective RDD studies involve programs where X has a \u201chair trigger\u201d that is not tightly related to the outcome being studied. Examples include the probability of being arrested for DWI jumping at greater than 0.08 blood-alcohol content [Hansen, 2015]; the probabil- ity of receiving health-care insurance jumping at age 65, [Card et al., 2008]; the probability of receiving medical attention jumping when birthweight falls below 1,500 grams [Almond et al., 2010; Barreca et al., 2011]; the probability of attending summer school when grades fall below some minimum level [Jacob and Lefgen, 2004], and as we just saw, the probability of attending the state \ufb02agship university jumping when the applicant\u2019s test scores exceed some minimum requirement [Hoekstra, 2009].\nIn all these kinds of studies, we need data. But speci\ufb01cally, we need a lot of data around the discontinuities, which itself implies that the data sets useful for RDD are likely very large. In fact, large sample sizes are characteristic features of the RDD. This is also because in the face of strong trends in the running variable, sample-size requirements get even larger. Researchers are typically using administrative data or settings such as birth records where there are many observations.", "start_char_idx": 21335, "end_char_idx": 23609, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d4e3a577-aac6-4049-b886-2d6f85cc7ff3": {"__data__": {"id_": "d4e3a577-aac6-4049-b886-2d6f85cc7ff3", "embedding": null, "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3895c9fb-cef4-4c38-bd5f-7c8889fe5b0a", "node_type": "4", "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "hash": "a242e6854b86b04962a421846f45199e3bc402a0895f18154ad132cbfcff54b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "51f7e9f5-47c0-41cf-a990-091928260203", "node_type": "1", "metadata": {"page number": "263 - 274", "chapter": "Regression Discontinuity"}, "hash": "3a8b0a700d981906ee0e6d63062f928f4c9b172cce2a056b29f5403864277354", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9de6615f-8624-4a84-a47f-cd10b2a4281b", "node_type": "1", "metadata": {}, "hash": "a6222a52d2ddf3562b5d6426e408917e5bd94f3788e8d92648214974707c99df", "class_name": "RelatedNodeInfo"}}, "text": "Estimation Using an RDD:\nThe Sharp RD Design. There are generally accepted two kinds of RDD studies. There are designs where the probability of treatment goes\nfor whom that topic is a priority. Take them out for coffee, get to know them, learn about their job, and ask them how treatment assign- ment works. Pay close attention to precisely how individual units get assigned to the program. Is it random? Is it via a rule? Oftentimes they will describe a process whereby a running variable is used for treatment assignment, but they won\u2019t call it that. While I can\u2019t promise this will yield pay dirt, my hunch, based in part on experience, is that they will end up describing to you some running variable that when it exceeds a threshold, people switch into some intervention. Build- ing alliances with local \ufb01rms and agencies can pay when trying to \ufb01nd good research ideas.\nThe validity of an RDD doesn\u2019t require that the assignment rule be arbitrary. It only requires that it be known, precise and free of manipu- lation. The most effective RDD studies involve programs where X has a \u201chair trigger\u201d that is not tightly related to the outcome being studied. Examples include the probability of being arrested for DWI jumping at greater than 0.08 blood-alcohol content [Hansen, 2015]; the probabil- ity of receiving health-care insurance jumping at age 65, [Card et al., 2008]; the probability of receiving medical attention jumping when birthweight falls below 1,500 grams [Almond et al., 2010; Barreca et al., 2011]; the probability of attending summer school when grades fall below some minimum level [Jacob and Lefgen, 2004], and as we just saw, the probability of attending the state \ufb02agship university jumping when the applicant\u2019s test scores exceed some minimum requirement [Hoekstra, 2009].\nIn all these kinds of studies, we need data. But speci\ufb01cally, we need a lot of data around the discontinuities, which itself implies that the data sets useful for RDD are likely very large. In fact, large sample sizes are characteristic features of the RDD. This is also because in the face of strong trends in the running variable, sample-size requirements get even larger. Researchers are typically using administrative data or settings such as birth records where there are many observations.\nThe Sharp RD Design. There are generally accepted two kinds of RDD studies. There are designs where the probability of treatment goes\nfrom 0 to 1 at the cutoff, or what is called a \u201csharp\u201d design. And there are designs where the probability of treatment discontinuously increases at the cutoff. These are often called \u201cfuzzy\u201d designs. In all of these, though, there is some running variable X that, upon reaching a cutoff c0, the likelihood of receiving some treatment \ufb02ips. Let\u2019s look at the diagram in Figure 22, which illustrates the similarities and differences between the two designs.\nSharp RDD is where treatment is a deterministic function of the running variable X.6 An example might be Medicare enrollment, which happens sharply at age 65, excluding disability situations. A fuzzy RDD represents a discontinuous \u201cjump\u201d in the probability of treatment when X > c0. In these fuzzy designs, the cutoff is used as an instrumental variable for treatment, like Angrist and Lavy [1999], who instrument for class size with a class-size function they created from the rules used by Israeli schools to construct class sizes.\n6 Van der Klaauw [2002] called the running variable the \u201cselection variable.\u201d This\nis because Van der Klaauw [2002] is an early paper in the new literature, and the\nterminology hadn\u2019t yet been hammered out. But here they mean the same thing.\nMore formally, in a sharp RDD, treatment status is a deterministic\nand discontinuous function of a running variable Xi, where\nwhere c0 is a known threshold or cutoff. If you know the value of Xi for unit i, then you know treatment assignment for unit i with certainty. But, if for every value of X you can perfectly predict the treatment assign- ment, then it necessarily means that there are no overlap along the running variable.", "start_char_idx": 0, "end_char_idx": 4093, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9de6615f-8624-4a84-a47f-cd10b2a4281b": {"__data__": {"id_": "9de6615f-8624-4a84-a47f-cd10b2a4281b", "embedding": null, "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3895c9fb-cef4-4c38-bd5f-7c8889fe5b0a", "node_type": "4", "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "hash": "a242e6854b86b04962a421846f45199e3bc402a0895f18154ad132cbfcff54b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d4e3a577-aac6-4049-b886-2d6f85cc7ff3", "node_type": "1", "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "hash": "7cc878bf8d5461809e526de079b9947c7b500ae46985482fd5bc78694e4f8bef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb7920af-060a-40f5-8cb5-0c95ea6f0475", "node_type": "1", "metadata": {}, "hash": "e65b3ffce992bc9858862194c6589b5e937217385a2b7588c680f511c55e974e", "class_name": "RelatedNodeInfo"}}, "text": "6 Van der Klaauw [2002] called the running variable the \u201cselection variable.\u201d This\nis because Van der Klaauw [2002] is an early paper in the new literature, and the\nterminology hadn\u2019t yet been hammered out. But here they mean the same thing.\nMore formally, in a sharp RDD, treatment status is a deterministic\nand discontinuous function of a running variable Xi, where\nwhere c0 is a known threshold or cutoff. If you know the value of Xi for unit i, then you know treatment assignment for unit i with certainty. But, if for every value of X you can perfectly predict the treatment assign- ment, then it necessarily means that there are no overlap along the running variable.\nIf we assume constant treatment effects, then in potential out-\ncomes terms, we get\nY0 i Y1 i\nUsing the switching equation, we get\nYi = Y0 )Di + (Y1 i i Yi = \u03b1 + \u03b2Xi + \u03b4Di + \u03b5i\nis the discontinuity in the\n\u03b4 = lim Xi\u2192X0 = lim Xi\u2192X0\nThe sharp RDD estimation is interpreted as an average causal effect of the treatment as the running variable approaches the cutoff in the limit, for it is only in the limit that we have overlap. This average causal effect is the local average treatment effect (LATE). We discuss LATE in greater detail in the instrumental variables, but I will say one thing about it here. Since identi\ufb01cation in an RDD is a limiting case, we are technically only identifying an average causal effect for those units at the cutoff. Insofar as those units have treatment effects that dif- fer from units along the rest of the running variable, then we have only estimated an average treatment effect that is local to the range around the cutoff. We de\ufb01ne this local average treatment effect as follows:\nFigure 23. Simulated data representing observed data points along a running variable\nbelow and above some binding cutoff. Note: Dashed lines are extrapolations.\nNotice the role that extrapolation plays in estimating treatment effects with sharp RDD. If unit i is just below c0, then Di = 0. But if unit i is just above c0, then the Di = 1. But for any value of Xi, there are either units in the treatment group or the control group, but not both. Therefore, the RDD does not have common support, which is one of the reasons we rely on extrapolation for our estimation. See Figure 23.\nContinuity assumption. The key identifying assumption in an RDD is | X = c0] and called the continuity assumption. It states that E[Y0 i | X = c0] are continuous (smooth) functions of X even across the E[Y1 i c0 threshold. Absent the treatment, in other words, the expected poten- tial outcomes wouldn\u2019t have jumped; they would\u2019ve remained smooth functions of X. But think about what that means for a moment. If the expected potential outcomes are not jumping at c0, then there nec- essarily are no competing interventions occurring at c0. Continuity, in other words, explicitly rules out omitted variable bias at the cut- off itself. All other unobserved determinants of Y are continuously\nFigure 24. Mortality rates along age running variable [Carpenter and Dobkin, 2009].\nrelated to the running variable X. Does there exist some omitted vari- able wherein the outcome, would jump at c0 even if we disregarded the treatment altogether? If so, then the continuity assumption is violated and our methods do not require the LATE.\nI apologize if I\u2019m beating a dead horse, but continuity is a sub- tle assumption and merits a little more discussion. The continuity assumption means that E[Y1 | X] wouldn\u2019t have jumped at c0. If it had jumped, then it means something other than the treatment caused it to jump because Y1 is already under treatment. So an example might be a study \ufb01nding a large increase in motor vehicle accidents at age 21. I\u2019ve reproduced a \ufb01gure from and interesting study on mortality rates for different types of causes [Carpenter and Dobkin, 2009]. I have reproduced one of the key \ufb01gures in Figure 24. Notice the large discon- tinuous jump in motor vehicle death rates at age 21. The most likely explanation is that age 21 causes people to drink more, and sometimes even while they are driving.\nBut this is only a causal effect if motor vehicle accidents don\u2019t jump at age 21 for other reasons.", "start_char_idx": 3420, "end_char_idx": 7609, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bb7920af-060a-40f5-8cb5-0c95ea6f0475": {"__data__": {"id_": "bb7920af-060a-40f5-8cb5-0c95ea6f0475", "embedding": null, "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3895c9fb-cef4-4c38-bd5f-7c8889fe5b0a", "node_type": "4", "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "hash": "a242e6854b86b04962a421846f45199e3bc402a0895f18154ad132cbfcff54b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9de6615f-8624-4a84-a47f-cd10b2a4281b", "node_type": "1", "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "hash": "5fa5190536fbf39debbf2bcea6b79ed06416ef1df8578bd18a35afc0ce105412", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e644237-3be4-4fdc-90e7-940ce46568d8", "node_type": "1", "metadata": {}, "hash": "fe15ef0e5bbe9ad24d1e8393a2acc29eb57d96a5ac0568d88111179b1c6d4017", "class_name": "RelatedNodeInfo"}}, "text": "The continuity assumption means that E[Y1 | X] wouldn\u2019t have jumped at c0. If it had jumped, then it means something other than the treatment caused it to jump because Y1 is already under treatment. So an example might be a study \ufb01nding a large increase in motor vehicle accidents at age 21. I\u2019ve reproduced a \ufb01gure from and interesting study on mortality rates for different types of causes [Carpenter and Dobkin, 2009]. I have reproduced one of the key \ufb01gures in Figure 24. Notice the large discon- tinuous jump in motor vehicle death rates at age 21. The most likely explanation is that age 21 causes people to drink more, and sometimes even while they are driving.\nBut this is only a causal effect if motor vehicle accidents don\u2019t jump at age 21 for other reasons. Formally, this is exactly what is implied\nby continuity\u2014the absence of simultaneous treatments at the cutoff. For instance, perhaps there is something biological that happens to 21-year-olds that causes them to suddenly become bad drivers. Or maybe 21-year-olds are all graduating from college at age 21, and dur- ing celebrations, they get into wrecks. To test this, we might replicate Carpenter and Dobkin [2009] using data from Uruguay, where the drink- ing age is 18. If we saw a jump in motor vehicle accidents at age 21 in Uruguay, then we might have reason to believe the continuity assump- tion does not hold in the United States. Reasonably de\ufb01ned placebos can help make the case that the continuity assumption holds, even if it is not a direct test per se.\nSometimes these abstract ideas become much easier to under- stand with data, so here is an example of what we mean using a simulation.\n1 clear 2 capture log close 3 set obs 1000 4 set seed 1234567 5 6 * Generate running variable 7 gen x = rnormal(50, 25) 8 replace x=0 if x < 0 9 drop if x > 100 10 sum x, det 11 12 * Set the cutoff at X=50. Treated if X > 50 13 gen D = 0 14 replace D = 1 if x > 50 15 gen y1 = 25 + 0*D + 1.5*x + rnormal(0, 20) 16 17 * Potential outcome Y1 not jumping at cutoff (continuity) 18 twoway (scatter y1 x if D==0, msize(vsmall) msymbol(circle_hollow)) (scatter y1 x if D==1, sort mcolor(blue) msize(vsmall) msymbol(circle_hollow)) (lfit y1 x if D==0, lcolor(red) msize(small) lwidth(medthin) lpattern(solid)) (lfit y1 x, lcolor(dknavy) msize(small) lwidth(medthin) lpattern(solid)), xtitle(Test score (X)) xline(50) legend(off)\n1 library(tidyverse) 2 3 # simulate the data 4 dat <- tibble( 5 6 ) %>% 7 mutate( 8 9 10 11 12 # cutoff at x = 50 13 dat <- dat %>% 14 mutate( 15 16 17 18 19 ggplot(aes(x, y1, colour = factor(D)), data = dat) + 20 21 22 23\ngeom_point(alpha = 0.5) + geom_vline(xintercept = 50, colour = \"grey\", linetype = 2)+ stat_smooth(method = \"lm\", se = F) + labs(x = \"Test score (X)\", y = \"Potential Outcome (Y1)\")\nFigure 25 shows the results from this simulation. Notice that the value of E[Y1 | X] is changing continuously over X and through c0. This is an example of the continuity assumption. It means absent the treatment itself, the expected potential outcomes would\u2019ve remained a smooth function of X even as passing c0. Therefore, if continuity held, then only the treatment, triggered at c0, could be responsible for discrete jumps in E[Y | X].\nThe nice thing about simulations is that we actually observe the potential outcomes since we made them ourselves. But in the real world, we don\u2019t have data on potential outcomes. If we did, we could test the continuity assumption directly. But remember\u2014by the switch- ing equation, we only observe actual outcomes, never potential out- comes.", "start_char_idx": 6841, "end_char_idx": 10418, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3e644237-3be4-4fdc-90e7-940ce46568d8": {"__data__": {"id_": "3e644237-3be4-4fdc-90e7-940ce46568d8", "embedding": null, "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3895c9fb-cef4-4c38-bd5f-7c8889fe5b0a", "node_type": "4", "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "hash": "a242e6854b86b04962a421846f45199e3bc402a0895f18154ad132cbfcff54b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bb7920af-060a-40f5-8cb5-0c95ea6f0475", "node_type": "1", "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "hash": "766913fc6fbb017560faf58bde49eade0c9a2b0cc53793356faeade005257b7d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e52a776-c8be-488d-bdd1-190b802a1ac8", "node_type": "1", "metadata": {}, "hash": "14beec4135829dbdc668cdd9e21f727ecd586dd74a6ae6d5c1f4000a8de0e3da", "class_name": "RelatedNodeInfo"}}, "text": "Notice that the value of E[Y1 | X] is changing continuously over X and through c0. This is an example of the continuity assumption. It means absent the treatment itself, the expected potential outcomes would\u2019ve remained a smooth function of X even as passing c0. Therefore, if continuity held, then only the treatment, triggered at c0, could be responsible for discrete jumps in E[Y | X].\nThe nice thing about simulations is that we actually observe the potential outcomes since we made them ourselves. But in the real world, we don\u2019t have data on potential outcomes. If we did, we could test the continuity assumption directly. But remember\u2014by the switch- ing equation, we only observe actual outcomes, never potential out- comes. Thus, since units switch from Y0 to Y1 at c0, we actually\nFigure 25. Smoothness of Y1 across the cutoff illustrated using simulated data.\ncan\u2019t directly evaluate the continuity assumption. This is where insti- tutional knowledge goes a long way, because it can help build the case that nothing else is changing at the cutoff that would otherwise shift potential outcomes.\nLet\u2019s illustrate this using simulated data. Notice that while Y1 by construction had not jumped at 50 on the X running variable, Y will. Let\u2019s look at the output in Figure 26. Notice the jump at the discon- tinuity in the outcome, which I\u2019ve labeled the LATE, or local average treatment effect.\n1 * Actual outcome jumping 2 gen y = 25 + 40*D + 1.5*x + rnormal(0, 20) 3 scatter y x if D==0, msize(vsmall) || scatter y x if D==1, msize(vsmall) legend(off)\nxline(50, lstyle(foreground)) || lfit y x if D ==0, color(red) || lfit y x if D ==1, color(red) ytitle(\"Outcome (Y)\") xtitle(\"Test Score (X)\")\n1 # simulate the discontinuity 2 dat <- dat %>% 3 mutate( 4 5 6 7 # figure 36 8 ggplot(aes(x, y2, colour = factor(D)), data = dat) + 9 10 11 12\ngeom_point(alpha = 0.5) + geom_vline(xintercept = 50, colour = \"grey\", linetype = 2) + stat_smooth(method = \"lm\", se = F) + labs(x = \"Test score (X)\", y = \"Potential Outcome (Y)\")\nEstimation using local and global least squares regressions. I\u2019d like to now dig into the actual regression model you would use to estimate the LATE parameter in an RDD. We will \ufb01rst discuss some basic modeling choices that researchers often make\u2014some trivial, some important. This section will focus primarily on regression-based estimation.\nFigure 26. Estimated LATE using simulated data.\nWhile not necessary, it is nonetheless quite common for authors\nto transform the running variable X by recentering at c0:\nYi = \u03b1 + \u03b2(Xi \u2212 c0) + \u03b4Di + \u03b5i\nThis doesn\u2019t change the interpretation of the treatment effect\u2014only the interpretation of the intercept. Let\u2019s use Card et al. [2008] as an example. Medicare is triggered when a person turns 65. So recenter the running variable (age) by subtracting 65:\nY = \u03b20 + \u03b21(Age \u2212 65) + \u03b22Edu + \u03b5 = \u03b20 + \u03b21Age \u2212 \u03b2165 + \u03b22Edu + \u03b5 = (\u03b20 \u2212 \u03b2165) + \u03b21Age + \u03b22Edu + \u03b5 = \u03b1 + \u03b21Age + \u03b22Edu + \u03b5\nwhere \u03b1 = \u03b20 + \u03b2165. All other coe\ufb03cients, notice, have the same interpretation except for the intercept.\nAnother practical question relates to nonlinear data-generating processes. A nonlinear data-generating process could easily yield false positives if we do not handle the speci\ufb01cation carefully. Because sometimes we are \ufb01tting local linear regressions around the cutoff, we could spuriously pick up an effect simply for no other reason than that we imposed linearity on the model. But if the underlying data-generating process is nonlinear, then it may be a spurious result due to misspeci\ufb01cation of the model. Consider an example of this nonlinearity in Figure 27.", "start_char_idx": 9687, "end_char_idx": 13303, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e52a776-c8be-488d-bdd1-190b802a1ac8": {"__data__": {"id_": "9e52a776-c8be-488d-bdd1-190b802a1ac8", "embedding": null, "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3895c9fb-cef4-4c38-bd5f-7c8889fe5b0a", "node_type": "4", "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "hash": "a242e6854b86b04962a421846f45199e3bc402a0895f18154ad132cbfcff54b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3e644237-3be4-4fdc-90e7-940ce46568d8", "node_type": "1", "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "hash": "530d451570a5a9cf17f00c636d5783cc7fb5d683f4cfc0a0111201af9a680051", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e32eecd1-7222-4cf8-887b-901e4cbc0038", "node_type": "1", "metadata": {}, "hash": "b1a648dfdf096298b031d6217ae2c17c934f2586c8d77f0b555d0ef9075f0e85", "class_name": "RelatedNodeInfo"}}, "text": "All other coe\ufb03cients, notice, have the same interpretation except for the intercept.\nAnother practical question relates to nonlinear data-generating processes. A nonlinear data-generating process could easily yield false positives if we do not handle the speci\ufb01cation carefully. Because sometimes we are \ufb01tting local linear regressions around the cutoff, we could spuriously pick up an effect simply for no other reason than that we imposed linearity on the model. But if the underlying data-generating process is nonlinear, then it may be a spurious result due to misspeci\ufb01cation of the model. Consider an example of this nonlinearity in Figure 27.\n1 * Nonlinear data generating process 2 drop y y1 x* D 3 set obs 1000 4 gen x = rnormal(100, 50) 5 replace x=0 if x < 0 6 drop if x > 280 7 sum x, det 8 9 * Set the cutoff at X=140. Treated if X > 140 10 gen D = 0\n(continued)\nSTATA (continued)\n11 replace D = 1 if x > 140 12 gen x2 = x*x 13 gen x3 = x*x*x 14 gen y = 10000 + 0*D - 100*x +x2 + rnormal(0, 1000) 15 reg y D x 16 17 scatter y x if D==0, msize(vsmall) || scatter y x /// 18 if D==1, msize(vsmall) legend(off) xline(140, /// 19 lstyle(foreground)) ylabel(none) || lfit y x /// 20 if D ==0, color(red) || lfit y x if D ==1, /// 21 color(red) xtitle(\"Test Score (X)\") /// 22 ytitle(\"Outcome (Y)\") 23 24 * Polynomial estimation 25 capture drop y 26 gen y = 10000 + 0*D - 100*x +x2 + rnormal(0, 1000) 27 reg y D x x2 x3 28 predict yhat 29 30 scatter y x if D==0, msize(vsmall) || scatter y x /// 31 if D==1, msize(vsmall) legend(off) xline(140, /// 32 lstyle(foreground)) ylabel(none) || line yhat x /// 33 if D ==0, color(red) sort || line yhat x if D==1, /// 34 sort color(red) xtitle(\"Test Score (X)\") /// 35 ytitle(\"Outcome (Y)\")\n(continued)\nR (continued)\n13 14 15 ggplot(aes(x, y3, colour = factor(D)), data = dat) + 16 17 18 19 20 21 ggplot(aes(x, y3, colour = factor(D)), data = dat) + 22 23 24 25 26\ngeom_point(alpha = 0.2) + geom_vline(xintercept = 140, colour = \"grey\", linetype = 2) + stat_smooth(method = \"lm\", se = F) + labs(x = \"Test score (X)\", y = \"Potential Outcome (Y)\")\ngeom_point(alpha = 0.2) + geom_vline(xintercept = 140, colour = \"grey\", linetype = 2) + stat_smooth(method = \"loess\", se = F) + labs(x = \"Test score (X)\", y = \"Potential Outcome (Y)\")\nI show this both visually and with a regression. As you can see in Figure 27, the data-generating process was nonlinear, but when with straight lines to the left and right of the cutoff, the trends in the running variable generate a spurious discontinuity at the cutoff. This shows up in a regression as well. When we \ufb01t the model using a least squares regression controlling for the running variable, we estimate a causal effect though there isn\u2019t one. In Table 40, the estimated effect of D on Y is large and highly signi\ufb01cant, even though the true effect is zero. In this situation, we would need some way to model the nonlinearity below and above the cutoff to check whether, even given the nonlinearity, there had been a jump in the outcome at the discontinuity.\nSuppose that the nonlinear relationships is\nfor some reasonably smooth function f(Xi).", "start_char_idx": 12654, "end_char_idx": 15788, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e32eecd1-7222-4cf8-887b-901e4cbc0038": {"__data__": {"id_": "e32eecd1-7222-4cf8-887b-901e4cbc0038", "embedding": null, "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3895c9fb-cef4-4c38-bd5f-7c8889fe5b0a", "node_type": "4", "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "hash": "a242e6854b86b04962a421846f45199e3bc402a0895f18154ad132cbfcff54b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e52a776-c8be-488d-bdd1-190b802a1ac8", "node_type": "1", "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "hash": "bb71b32d0eff32e44793780ce240717bf2ead209952a02cefcd6f838776d6363", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b97a5047-0c37-42bc-80ef-75cd77e75c2f", "node_type": "1", "metadata": {}, "hash": "0ccba13d636c6d5bac72259de30652c3e380e3f82fd6e8af4f4eb2371ef1ad6f", "class_name": "RelatedNodeInfo"}}, "text": "As you can see in Figure 27, the data-generating process was nonlinear, but when with straight lines to the left and right of the cutoff, the trends in the running variable generate a spurious discontinuity at the cutoff. This shows up in a regression as well. When we \ufb01t the model using a least squares regression controlling for the running variable, we estimate a causal effect though there isn\u2019t one. In Table 40, the estimated effect of D on Y is large and highly signi\ufb01cant, even though the true effect is zero. In this situation, we would need some way to model the nonlinearity below and above the cutoff to check whether, even given the nonlinearity, there had been a jump in the outcome at the discontinuity.\nSuppose that the nonlinear relationships is\nfor some reasonably smooth function f(Xi). In that case, we\u2019d \ufb01t the regression model:\nYi = f(Xi) + \u03b4Di + \u03b7i\nSince f(Xi) is counterfactual for values of Xi > c0, how will we model the nonlinearity? There are two ways of approximating f(Xi). The traditional approaches let f(XBei) equal a pth-order polynomial:\nTable 40. Estimated effect of D on Y using OLS controlling for linear running variable.\nHigher-order polynomials can lead to over\ufb01tting and have been found to introduce bias [Gelman and Imbens, 2019]. Those authors recom- mend using local linear regressions with linear and quadratic forms only. Another way of approximating f(Xi) is to use a nonparametric kernel, which I discuss later.\nThough Gelman and Imbens [2019] warn us about higher-order polynomials, I\u2019d like to use an example with pth-order polynomials, mainly because it\u2019s not uncommon to see this done today. I\u2019d also like you to know some of the history of this method and understand better what old papers were doing. We can generate this function, f(Xi), by allowing the Xi terms to differ on both sides of the cutoff by including them both individually and interacting them with Di. In that case, we\nwhere (cid:27)Xi is the recentered running variable (i.e., Xi \u2212 c0). Centering at c0 ensures that the treatment effect at Xi = X0 is the coe\ufb03cient on Di in a regression model with interaction terms. As Lee and Lemieux [2010]\nnote, allowing different functions on both sides of the discontinuity should be the main results in an RDD paper.\nTo derive a regression model, \ufb01rst note that the observed values\nmust be used in place of the potential outcomes:\nYour regression model then is\nwhere \u03b2 \u2217 = \u03b21p \u2212 \u03b20p. The equation we looked at 1 earlier was just a special case of the above equation with \u03b2 \u2217 = 0. 1 The treatment effect at c0 is \u03b4. And the treatment effect at Xi \u2212 c0 > 0 is \u03b4 + \u03b2 \u2217 p cp. Let\u2019s see this in action with another simulation.\n1 * Polynomial modeling 2 capture drop y 3 gen y = 10000 + 0*D - 100*x +x2 + rnormal(0, 1000) 4 reg y D##c.(x x2 x3) 5 predict yhat 6 7 scatter y x if D==0, msize(vsmall) || scatter y x /// 8 if D==1, msize(vsmall) legend(off) xline(140, /// 9 lstyle(foreground)) ylabel(none) || line yhat x /// 10 if D ==0, color(red) sort || line yhat x if D==1, /// 11 sort color(red) xtitle(\"Test Score (X)\") /// 12 ytitle(\"Outcome (Y)\")\nTable 41. Estimated effect of D on Y using OLS controlling for linear and quadratic running variable.", "start_char_idx": 14983, "end_char_idx": 18194, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b97a5047-0c37-42bc-80ef-75cd77e75c2f": {"__data__": {"id_": "b97a5047-0c37-42bc-80ef-75cd77e75c2f", "embedding": null, "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3895c9fb-cef4-4c38-bd5f-7c8889fe5b0a", "node_type": "4", "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "hash": "a242e6854b86b04962a421846f45199e3bc402a0895f18154ad132cbfcff54b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e32eecd1-7222-4cf8-887b-901e4cbc0038", "node_type": "1", "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "hash": "8375192f73b4a41d714aeb3b1dcd6bead877be1ea277d79a9525629f0039227b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "85f49d42-1483-439c-843b-18182d6d83de", "node_type": "1", "metadata": {}, "hash": "c8e2bb327ff4a29f7345f809595c3c9ba561a8cbdcc2628e7734a313ec24a34c", "class_name": "RelatedNodeInfo"}}, "text": "Let\u2019s see this in action with another simulation.\n1 * Polynomial modeling 2 capture drop y 3 gen y = 10000 + 0*D - 100*x +x2 + rnormal(0, 1000) 4 reg y D##c.(x x2 x3) 5 predict yhat 6 7 scatter y x if D==0, msize(vsmall) || scatter y x /// 8 if D==1, msize(vsmall) legend(off) xline(140, /// 9 lstyle(foreground)) ylabel(none) || line yhat x /// 10 if D ==0, color(red) sort || line yhat x if D==1, /// 11 sort color(red) xtitle(\"Test Score (X)\") /// 12 ytitle(\"Outcome (Y)\")\nTable 41. Estimated effect of D on Y using OLS controlling for linear and quadratic running variable.\n1 library(stargazer) 2 3 dat <- tibble( 4 5 ) %>% 6 mutate( 7 8 9 10 11 12 13 14 15 regression <- lm(y3 ~ D*., data = dat) 16 17 stargazer(regression, type = \"text\") 18 19 ggplot(aes(x, y3, colour = factor(D)), data = dat) + 20 21 22 23\ngeom_point(alpha = 0.2) + geom_vline(xintercept = 140, colour = \"grey\", linetype = 2) + stat_smooth(method = \"loess\", se = F) + labs(x = \"Test score (X)\", y = \"Potential Outcome (Y)\")\nLet\u2019s look at the output from this exercise in Figure 28 and Table 41.\nAs you can see, once we model the data using a quadratic (the cubic\nultimately was unnecessary), there is no estimated treatment effect at\nthe cutoff. There is also no effect in our least squares regression.\nNonparametric kernels. But, as we mentioned earlier, Gelman and Imbens [2019] have discouraged the use of higher-order polynomials when estimating local linear regressions. An alternative is to use kernel regression. The nonparametric kernel method has problems because you are trying to estimate regressions at the cutoff point, which can result in a boundary problem (see Figure 29). In this picture, the bias is caused by strong trends in expected potential outcomes throughout the running variable.\nWhile the true effect in this diagram is AB, with a certain band- width a rectangular kernel would estimate the effect as A(cid:12)B(cid:12), which is as you can see a biased estimator. There is systematic bias with the kernel method if the underlying nonlinear function, f(X), is upwards-or downwards-xsloping.\nThe standard solution to this problem is to run local linear non- parametric regression [Hahn et al., 2001]. In the case described above, this would substantially reduce the bias. So what is that? Think of ker- nel regression as a weighted regression restricted to a window (hence\n\u201clocal\u201d). The kernel provides the weights to that regression.7 A rectan- gular kernel would give the same result as taking E[Y] at a given bin on X. The triangular kernel gives more importance to the observations closest to the center.\nThe model is some version of: (cid:2)\nyi \u2212 a \u2212 b(xi \u2212 c0)\nWhile estimating this in a given window of width h around the cut- off is straightforward, what\u2019s not straightforward is knowing how large or small to make the bandwidth. This method is sensitive to the choice of bandwidth, but more recent work allows the researcher to estimate optimal bandwidths [Calonico et al., 2014; Imbens and Kalyanaraman, 2011]. These may even allow for bandwidths to vary left and right of the cutoff.\n7 Stata\u2019s poly command estimates kernel-weighted local polynomial regression.\nMedicare and universal health care. Card et al. [2008] is an example of a sharp RDD, because it focuses on the provision of universal health- care insurance for the elderly\u2014Medicare at age 65. What makes this a policy-relevant question? Universal insurance has become highly rele- vant because of the debates surrounding the Affordable Care Act, as well as several Democratic senators supporting Medicare for All. But it is also important for its sheer size. In 2014, Medicare was 14% of the federal budget at $505 billion.\nApproximately 20% of non-elderly adults in the United States lacked insurance in 2005. Most were from lower-income families, and nearly half were African American or Hispanic.", "start_char_idx": 17617, "end_char_idx": 21490, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "85f49d42-1483-439c-843b-18182d6d83de": {"__data__": {"id_": "85f49d42-1483-439c-843b-18182d6d83de", "embedding": null, "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3895c9fb-cef4-4c38-bd5f-7c8889fe5b0a", "node_type": "4", "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "hash": "a242e6854b86b04962a421846f45199e3bc402a0895f18154ad132cbfcff54b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b97a5047-0c37-42bc-80ef-75cd77e75c2f", "node_type": "1", "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "hash": "eb487dfd49d4c8abf857a3b099ed233bf56e7e96f7336b05b77b3926d60faeb8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "21e6acb9-5275-4094-afff-afca52fa7d76", "node_type": "1", "metadata": {}, "hash": "dcbcc53f0c00484f66536f482242686e27e9e88c4f1d814c9d666499235ff1c1", "class_name": "RelatedNodeInfo"}}, "text": "These may even allow for bandwidths to vary left and right of the cutoff.\n7 Stata\u2019s poly command estimates kernel-weighted local polynomial regression.\nMedicare and universal health care. Card et al. [2008] is an example of a sharp RDD, because it focuses on the provision of universal health- care insurance for the elderly\u2014Medicare at age 65. What makes this a policy-relevant question? Universal insurance has become highly rele- vant because of the debates surrounding the Affordable Care Act, as well as several Democratic senators supporting Medicare for All. But it is also important for its sheer size. In 2014, Medicare was 14% of the federal budget at $505 billion.\nApproximately 20% of non-elderly adults in the United States lacked insurance in 2005. Most were from lower-income families, and nearly half were African American or Hispanic. Many analysts have argued that unequal insurance coverage contributes to disparities in health-care utilization and health outcomes across socioeconomic status. But, even among the policies, there is heterogeneity in the form of different copays, deductibles, and other features that affect use. Evi- dence that better insurance causes better health outcomes is limited because health insurance suffers from deep selection bias. Both sup- ply and demand for insurance depend on health status, confounding observational comparisons between people with different insurance characteristics.\nThe situation for elderly looks very different, though. Less than 1% of the elderly population is uninsured. Most have fee-for-service Medi- care coverage. And that transition to Medicare occurs sharply at age 65\u2014the threshold for Medicare eligibility.\nThe authors estimate a reduced form model measuring the causal\nwhere i indexes a socioeconomic group, a indexes age, uija indexes the unobserved error, yija health care usage, Xija a set of covariates (e.g., gender and region), fj(\u03b1; \u03b2) a smooth func- tion representing the age pro\ufb01le of outcome y for group j, and Ck ija (k = 1, 2, . . . , K) are characteristics of the insurance coverage held by the individual such as copayment rates. The problem with estimat- is that insurance coverage is endogenous: ing this model, though, cov(u, C) (cid:9)= 0. So the authors use as identi\ufb01cation of the age threshold\nfor Medicare eligibility at 65, which they argue is credibly exogenous variation in insurance status.\ndummy variables: C1 ija (any coverage) and C2 Card et al. [2008] estimate the following linear probability models:\nSuppose health insurance coverage can be summarized by two ija (generous insurance).\nwhere \u03b2 1 (a) are j are group-speci\ufb01c coe\ufb03cients, g1 smooth age pro\ufb01les for group j, and Da is a dummy if the respondent is equal to or over age 65. Recall the reduced form model:\nCombining the Cija equations, and rewriting the reduced form model, we get:\nwhere h(a) = fj(a) + \u03b41g1 (a) + \u03b42g2 (a) is the reduced form age pro\ufb01le for j j \u03b42 and vy group j, \u03c0 y \u03b41 + \u03c0 2 \u03b42 is the error term. \u03b41 + v2 = uija + v1 j ija ija ija j Assuming that the pro\ufb01les fj(a), gj(a), and g2 (a) are continuous at age j 65 (i.e., the continuity assumption necessary for identi\ufb01cation), then any discontinuity in y is due to insurance. The magnitudes will depend on the size of the insurance changes at age 65 (\u03c0 1 j ) and on the associated causal effects (\u03b41 and \u03b42).\nFor some basic health-care services, such as routine doctor visits, it may be that the only thing that matters is insurance. But, in those situations, the implied discontinuity in Y at age 65 for group j will be proportional to the change in insurance status experienced by that group. For more expensive or elective services, the generosity of the coverage may matter\u2014for instance, if patients are unwilling to cover the required copay or if the managed care program won\u2019t cover the service. This creates a potential identi\ufb01cation problem in interpreting the discontinuity in y for any one group.", "start_char_idx": 20639, "end_char_idx": 24589, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "21e6acb9-5275-4094-afff-afca52fa7d76": {"__data__": {"id_": "21e6acb9-5275-4094-afff-afca52fa7d76", "embedding": null, "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3895c9fb-cef4-4c38-bd5f-7c8889fe5b0a", "node_type": "4", "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "hash": "a242e6854b86b04962a421846f45199e3bc402a0895f18154ad132cbfcff54b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "85f49d42-1483-439c-843b-18182d6d83de", "node_type": "1", "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "hash": "ddcb79a03b9b230532ab2e953faf0e385d84fb6688314e505a798297552fd1f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "63ac8d68-383b-4152-92df-05c654f60bf9", "node_type": "1", "metadata": {}, "hash": "1ed277f373575222f368a3ea2f8ae1db45c4c10d06e8e7fdad113c002754ff43", "class_name": "RelatedNodeInfo"}}, "text": "The magnitudes will depend on the size of the insurance changes at age 65 (\u03c0 1 j ) and on the associated causal effects (\u03b41 and \u03b42).\nFor some basic health-care services, such as routine doctor visits, it may be that the only thing that matters is insurance. But, in those situations, the implied discontinuity in Y at age 65 for group j will be proportional to the change in insurance status experienced by that group. For more expensive or elective services, the generosity of the coverage may matter\u2014for instance, if patients are unwilling to cover the required copay or if the managed care program won\u2019t cover the service. This creates a potential identi\ufb01cation problem in interpreting the discontinuity in y for any one group. Since \u03c0 y is a linear combina- j tion of the discontinuities in coverage and generosity, \u03b41 and \u03b42 can be\nestimated by a regression across groups:\nwhere ej is an error term re\ufb02ecting a combination of the sampling errors in \u03c0 y\nj , \u03c0 1\nCard et al. [2008] use a couple of different data sets\u2014one a stan- dard survey and the other administrative records from hospitals in three states. First, they use the 1992\u20132003 National Health Interview Survey (NHIS). The NHIS reports respondents\u2019 birth year, birth month, and calendar quarter of the interview. Authors used this to construct an estimate of age in quarters at date of interview. A person who reaches 65 in the interview quarter is coded as age 65 and 0 quarters. Assum- ing a uniform distribution of interview dates, one-half of these people will be 0\u20136 weeks younger than 65 and one-half will be 0\u20136 weeks older. Analysis is limited to people between 55 and 75. The \ufb01nal sample has 160,821 observations.\nThe second data set is hospital discharge records for California, Florida, and New York. These records represent a complete census of discharges from all hospitals in the three states except for feder- ally regulated institutions. The data \ufb01les include information on age in months at the time of admission. Their sample selection criteria is to drop records for people admitted as transfers from other institutions and limit people between 60 and 70 years of age at admission. Sam- ple sizes are 4,017,325 (California), 2,793,547 (Florida), and 3,121,721 (New York).\nSome institutional details about the Medicare program may be helpful. Medicare is available to people who are at least 65 and have worked forty quarters or more in covered employment or have a spouse who did. Coverage is available to younger people with severe kidney disease and recipients of Social Security Disability Insurance. Eligible individuals can obtain Medicare hospital insurance (Part A) free of charge and medical insurance (Part B) for a modest monthly premium. Individuals receive notice of their impending eligibility for Medicare shortly before they turn 65 and are informed they have to enroll in it and choose whether to accept Part B coverage. Coverage begins on the \ufb01rst day of the month in which they turn 65.\nThere are \ufb01ve insurance-related variables: probability of Medicare coverage, any health insurance coverage, private coverage, two or more forms of coverage, and individual\u2019s primary health insurance is managed care. Data are drawn from the 1999\u20132003 NHIS, and for each characteristic, authors show the incidence rate at age 63\u201364 and the change at age 65 based on a version of the CK equations that include a quadratic in age, fully interacted with a post-65 dummy as well as controls for gender, education, race/ethnicity, region, and sample year. Alternative speci\ufb01cations were also used, such as a parametric model \ufb01t to a narrower age window (age 63\u201367) and a local linear regression speci\ufb01cation using a chosen bandwidth. Both show similar estimates of the change at age 65.\nThe authors present their \ufb01ndings in Table 42. The way that you read this table is each cell shows the average treatment effect for the 65-year-old population that complies with the treatment. We can see, not surprisingly, that the effect of receiving Medicare is to cause a very large increase of being on Medicare, as well as reducing coverage on private and managed care.\nFormal identi\ufb01cation in an RDD relating to some outcome (insur- ance coverage) to a treatment (Medicare age-eligibility) that itself depends on some running variable, age, relies on the continuity assumptions that we discussed earlier.", "start_char_idx": 23859, "end_char_idx": 28239, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "63ac8d68-383b-4152-92df-05c654f60bf9": {"__data__": {"id_": "63ac8d68-383b-4152-92df-05c654f60bf9", "embedding": null, "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3895c9fb-cef4-4c38-bd5f-7c8889fe5b0a", "node_type": "4", "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "hash": "a242e6854b86b04962a421846f45199e3bc402a0895f18154ad132cbfcff54b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "21e6acb9-5275-4094-afff-afca52fa7d76", "node_type": "1", "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "hash": "0f05989d918bb9ad2efffe988eeb247ac4d28c3af4506ff1b95bba745247becb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "adf69572-0d0c-4089-a81c-3fd8fe96f1c1", "node_type": "1", "metadata": {}, "hash": "88ce60c4b914fb9d1fdfaa94a50d0c709700ce6fb56a2ad5594c4438e51bd43c", "class_name": "RelatedNodeInfo"}}, "text": "Alternative speci\ufb01cations were also used, such as a parametric model \ufb01t to a narrower age window (age 63\u201367) and a local linear regression speci\ufb01cation using a chosen bandwidth. Both show similar estimates of the change at age 65.\nThe authors present their \ufb01ndings in Table 42. The way that you read this table is each cell shows the average treatment effect for the 65-year-old population that complies with the treatment. We can see, not surprisingly, that the effect of receiving Medicare is to cause a very large increase of being on Medicare, as well as reducing coverage on private and managed care.\nFormal identi\ufb01cation in an RDD relating to some outcome (insur- ance coverage) to a treatment (Medicare age-eligibility) that itself depends on some running variable, age, relies on the continuity assumptions that we discussed earlier. That is, we must assume that the conditional expectation functions for both potential outcomes is continuous at age= 65. This means that both E[Y0 | a] and E[Y1 | a] are continuous through age of 65. If that assumption is plausible, then the average treatment effect at age 65 is identi\ufb01ed as:\nThe continuity assumption requires that all other factors, observed and unobserved, that affect insurance coverage are trending smoothly at the cutoff, in other words. But what else changes at age 65 other than Medicare eligibility? Employment changes. Typically, 65 is the tradi- tional age when people retire from the labor force. Any abrupt change in employment could lead to differences in health-care utilization if nonworkers have more time to visit doctors.\ninvestigate this possible con- founder. They do this by testing for any potential discontinuities at age\nThe authors need to, therefore,\nTable 42. Insurance characteristics just before age 65 and estimated discontinuities at age 65.\nNote: Entries in each cell are estimated regression discontinuities at age 65 from quadratics in age interacted with a dummy for 65 and older. Other controls such as gender, race, education, region, and sample year are also included. Data is from the pooled 1999\u20132003 NHIS.\nFigure 30. Investigating the CPS for discontinuities at age 65 [Card et al., 2008].\n65 for confounding variables using a third data set\u2014the March CPS 1996\u20132004. And they ultimately \ufb01nd no evidence for discontinuities in employment at age 65 (Figure 30).\nNext the authors investigate the impact that Medicare had on access to care and utilization using the NHIS data. Since 1997, NHIS has asked four questions. They are:\n\u201cDuring the past 12 months has medical care been delayed for\n\u201cDuring the past 12 months was there any time when this per- son needed medical care but did not get it because [this person] could not afford it?\u201d\n\u201cDid the individual have at least one doctor visit in the past\n\u201cDid the individual have one or more overnight hospital stays in\nEstimates from this analysis are presented in Table 43. Each cell\nFigure 31. Changes in hospitalizations [Card et al., 2008].\nat the discontinuity. Standard errors are in parentheses. There are a few encouraging \ufb01ndings from this table. First, the share of the rele- vant population who delayed care the previous year fell 1.8 points, and similar for the share who did not get care at all in the previous year. The share who saw a doctor went up slightly, as did the share who stayed at a hospital. These are not very large effects in magnitude, it is important to note, but they are relatively precisely estimated. Note that these effects differed considerably by race and ethnicity as well as education.\nHaving shown modest effects on care and utilization, the authors turn to examining the kinds of care they received by examining speci\ufb01c changes in hospitalizations. Figure 31 shows the effect of Medicare on hip and knee replacements by race. The effects are largest for whites. In conclusion, the authors \ufb01nd that universal health-care coverage for the elderly increases care and utilization as well as coverage. In a subsequent study [Card et al., 2009], the authors examined the impact of Medicare on mortality and found slight decreases in mortality rates (see Table 44).\nTable 43. Measures of access to care just before 65 and estimated discontinuities at age 65.", "start_char_idx": 27398, "end_char_idx": 31636, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "adf69572-0d0c-4089-a81c-3fd8fe96f1c1": {"__data__": {"id_": "adf69572-0d0c-4089-a81c-3fd8fe96f1c1", "embedding": null, "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3895c9fb-cef4-4c38-bd5f-7c8889fe5b0a", "node_type": "4", "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "hash": "a242e6854b86b04962a421846f45199e3bc402a0895f18154ad132cbfcff54b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "63ac8d68-383b-4152-92df-05c654f60bf9", "node_type": "1", "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "hash": "0ff246dca7464e91d312cef3072b2bac8e27733564fef0727608a8620e0e0711", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "085e07b6-61bd-403a-8481-9bab8ade3fb8", "node_type": "1", "metadata": {}, "hash": "dea934018d547dc6b5554a011a551d5ffad01ca462f8784cf5617b2eae4ce693", "class_name": "RelatedNodeInfo"}}, "text": "These are not very large effects in magnitude, it is important to note, but they are relatively precisely estimated. Note that these effects differed considerably by race and ethnicity as well as education.\nHaving shown modest effects on care and utilization, the authors turn to examining the kinds of care they received by examining speci\ufb01c changes in hospitalizations. Figure 31 shows the effect of Medicare on hip and knee replacements by race. The effects are largest for whites. In conclusion, the authors \ufb01nd that universal health-care coverage for the elderly increases care and utilization as well as coverage. In a subsequent study [Card et al., 2009], the authors examined the impact of Medicare on mortality and found slight decreases in mortality rates (see Table 44).\nTable 43. Measures of access to care just before 65 and estimated discontinuities at age 65.\nDelayed last year\nDid not get care last year\nNote: Entries in each cell are estimated regression discontinuities at age 65 from quadratics in age interacted with a dummy for 65 and older. Other controls such as gender, race, education, region and sample year are also included. First two columns are from 1997\u20132003 NHIS and last\ntwo columns are from 1992\u20132003 NHIS.\n\u22121.1 Quadratic (0.2) no controls \u22121.0 Quadratic 0.2) plus controls \u22120.7 Cubic plus controls (0.3) Local OLS with ad \u22120.8 (0.2) hoc bandwidths\nNote: Dependent variable for death within interval shown in the column heading. Regres- sion estimates at the discontinuity of age 65 for \ufb02exible regression models. Standard\nInference. As we\u2019ve mentioned, it\u2019s standard practice in the RDD to estimate causal effects using local polynomial regressions. In its sim- plest form, this amounts to nothing more complicated than \ufb01tting a linear speci\ufb01cation separately on each side of the cutoff using a least squares regression. But when this is done, you are using only the obser- vations within some pre-speci\ufb01ed window (hence \u201clocal\u201d). As the true conditional expectation function is probably not linear at this window, the resulting estimator likely suffers from speci\ufb01cation bias. But if you can get the window narrow enough, then the bias of the estimator is probably small relative to its standard deviation.\nBut what if the window cannot be narrowed enough? This can happen if the running variable only takes on a few values, or if the gap between values closest to the cutoff are large. The result could be you simply do not have enough observations close to the cut- off for the local polynomial regression. This also can lead to the heteroskedasticity-robust con\ufb01dence intervals to undercover the aver- age causal effect because it is not centered. And here\u2019s the really bad news\u2014this probably is happening a lot in practice.\nIn a widely cited and very in\ufb02uential study, Lee and Card [2008] suggested that researchers should cluster their standard errors by\nthe running variable. This advice has since become common prac- tice in the empirical literature. Lee and Lemieux [2010], in a survey article on proper RDD methodology, recommend this practice, just to name one example. But in a recent study, Koles\u00e1r and Rothe [2018] provide extensive theoretical and simulation-based evidence that clus- tering on the running variable is perhaps one of the worst approaches you could take. In fact, clustering on the running variable can actually be substantially worse than heteroskedastic-robust standard errors.\nAs an alternative to clustering and robust standard errors, the authors propose two alternative con\ufb01dence intervals that have guaran- teed coverage properties under various restrictions on the conditional expectation function. Both con\ufb01dence intervals are \u201chonest,\u201d which means they achieve correct coverage uniformly over all conditional expectation functions in large samples. These con\ufb01dence intervals are currently unavailable in Stata as of the time of this writing, but they can be implemented in R with the RDHonest package.8 R users are encour- aged to use these con\ufb01dence intervals. Stata users are encouraged to switch (grudgingly) to R so as to use these con\ufb01dence intervals. Bar- ring that, Stata users should use the heteroskedastic robust standard errors. But whatever you do, don\u2019t cluster on the running variable, as that is nearly an unambiguously bad idea.\nA separate approach may be to use randomization inference. As we noted, Hahn et al.", "start_char_idx": 30762, "end_char_idx": 35166, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "085e07b6-61bd-403a-8481-9bab8ade3fb8": {"__data__": {"id_": "085e07b6-61bd-403a-8481-9bab8ade3fb8", "embedding": null, "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3895c9fb-cef4-4c38-bd5f-7c8889fe5b0a", "node_type": "4", "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "hash": "a242e6854b86b04962a421846f45199e3bc402a0895f18154ad132cbfcff54b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "adf69572-0d0c-4089-a81c-3fd8fe96f1c1", "node_type": "1", "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "hash": "b3f12ba10095c802d213912c69b34bdf6265f39380e71be4f77515997c17d2cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4404019b-30d8-4d16-9da4-c398059583b9", "node_type": "1", "metadata": {}, "hash": "eae8987393dcd9c04847ce9618f042fd32cee4dfeae88ef21deddda936863f53", "class_name": "RelatedNodeInfo"}}, "text": "Both con\ufb01dence intervals are \u201chonest,\u201d which means they achieve correct coverage uniformly over all conditional expectation functions in large samples. These con\ufb01dence intervals are currently unavailable in Stata as of the time of this writing, but they can be implemented in R with the RDHonest package.8 R users are encour- aged to use these con\ufb01dence intervals. Stata users are encouraged to switch (grudgingly) to R so as to use these con\ufb01dence intervals. Bar- ring that, Stata users should use the heteroskedastic robust standard errors. But whatever you do, don\u2019t cluster on the running variable, as that is nearly an unambiguously bad idea.\nA separate approach may be to use randomization inference. As we noted, Hahn et al. [2001] emphasized that the conditional expected potential outcomes must be continuous across the cutoff for a regres- sion discontinuity design to identify the local average treatment effect. But Cattaneo et al. [2015] suggest an alternative assumption which has implications for inference. They ask us to consider that per- haps around the cutoff, in a short enough window, the treatment was assigned to units randomly. It was effectively a coin \ufb02ip which side of the cutoff someone would be for a small enough window around the cutoff. Assuming there exists a neighborhood around the cutoff where this randomization-type condition holds, then this assumption may be viewed as an approximation of a randomized experiment around the cutoff. Assuming this is plausible, we can proceed as if only those observations closest to the discontinuity were randomly assigned,\n8 RDHonest is available at https://github.com/kolesarm/RDHonest.\nFigure 32. Vertical axis is the probability of treatment for each value of the running\nwhich leads naturally to randomization inference as a methodology for conducting exact or approximate p-values.\nThe Fuzzy RD Design. In the sharp RDD, treatment was determined when Xi \u2265 c0. But that kind of deterministic assignment does not always happen. Sometimes there is a discontinuity, but it\u2019s not entirely deterministic, though it nonetheless is associated with a discontinuity in treatment assignment. When there is an increase in the probability of treatment assignment, we have a fuzzy RDD. The earlier paper by Hoekstra [2009] had this feature, as did Angrist and Lavy [1999]. The formal de\ufb01nition of a probabilistic treatment assignment is (cid:5)\n(cid:9)= lim c0\u2190Xi\nIn other words, the conditional probability is discontinuous as X approaches c0 in the limit. A visualization of this is presented from Imbens and Lemieux [2008] in Figure 32.\nThe identifying assumptions are the same under fuzzy designs as they are under sharp designs: they are the continuity assumptions. For identi\ufb01cation, we must assume that the conditional expectation of the potential outcomes (e.g., E[Y0|X < c0]) is changing smoothly through c0. What changes at c0 is the treatment assignment probability. An illustration of this identifying assumption is in Figure 33.\nEstimating some average treatment effect under a fuzzy RDD is very similar to how we estimate a local average treatment effect with\ninstrumental variables. I will cover instrumental variables in more detail later in the book, but for now let me tell you about estimation under fuzzy designs using IV. One can estimate several ways. One simple way is a type of Wald estimator, where you estimate some causal effect as the ratio of a reduced form difference in mean outcomes around the cutoff and a reduced form difference in mean treatment assignment around the cutoff.\nThe assumptions for identi\ufb01cation here are the same as with any instrumental variables design: all the caveats about exclusion restric- tions, monotonicity, SUTVA, and the strength of the \ufb01rst stage.9\nBut one can also estimate the effect using a two-stage least squares model or similar appropriate model such as limited- information maximum likelihood. Recall that there are now two events: the \ufb01rst event is when the running variable exceeds the cutoff, and the second event is when a unit is placed in the treatment. Let Zi be an indicator for when X exceeds c0. One can use both Zi and the interac- tion terms as instruments for the treatment Di. If one uses only Zi as an instrumental variable, then it is a \u201cjust identi\ufb01ed\u201d model, which usually has good \ufb01nite sample properties.\n9 I discuss these assumptions and diagnostics in greater detail later in the chapter\nLet\u2019s look at a few of the regressions that are involved in this instru- mental variables approach.", "start_char_idx": 34435, "end_char_idx": 38982, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4404019b-30d8-4d16-9da4-c398059583b9": {"__data__": {"id_": "4404019b-30d8-4d16-9da4-c398059583b9", "embedding": null, "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3895c9fb-cef4-4c38-bd5f-7c8889fe5b0a", "node_type": "4", "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "hash": "a242e6854b86b04962a421846f45199e3bc402a0895f18154ad132cbfcff54b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "085e07b6-61bd-403a-8481-9bab8ade3fb8", "node_type": "1", "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "hash": "91497fa7e9f66d49460326160dfe70364e17d75a6a3e667a66e0a5c65f1bc6e6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5e1fa5c4-248d-4abc-9c06-5d66e0a5615c", "node_type": "1", "metadata": {}, "hash": "5d812c0ac265368f79785ec3592e49cbd7a550efbdb095242c3abe48ee3ce01b", "class_name": "RelatedNodeInfo"}}, "text": "Recall that there are now two events: the \ufb01rst event is when the running variable exceeds the cutoff, and the second event is when a unit is placed in the treatment. Let Zi be an indicator for when X exceeds c0. One can use both Zi and the interac- tion terms as instruments for the treatment Di. If one uses only Zi as an instrumental variable, then it is a \u201cjust identi\ufb01ed\u201d model, which usually has good \ufb01nite sample properties.\n9 I discuss these assumptions and diagnostics in greater detail later in the chapter\nLet\u2019s look at a few of the regressions that are involved in this instru- mental variables approach. There are three possible regressions: the \ufb01rst stage, the reduced form, and the second stage. Let\u2019s look at them in order. In the case just identi\ufb01ed (meaning only one instrument for one endogenous variable), the \ufb01rst stage would be:\nwhere \u03c0 is the causal effect of Zi on the conditional probability of treat- ment. The \ufb01tted values from this regression would then be used in a second stage. We can also use both Zi and the interaction terms as instruments for Di. If we used Zi and all its interactions, the estimated \ufb01rst stage would be:\nWe would also construct analogous \ufb01rst stages for \u02dcXiDi, . . . , \u02dcXp\nIf we wanted to forgo estimating the full IV model, we might esti- mate the reduced form only. You\u2019d be surprised how many applied people prefer to simply report the reduced form and not the fully spec- i\ufb01ed instrumental variables model. If you read Hoekstra [2009], for instance, he favored presenting the reduced form\u2014that second \ufb01gure, in fact, was a picture of the reduced form. The reduced form would regress the outcome Y onto the instrument and the running variable. The form of this fuzzy RDD reduced form is:\nAs in the sharp RDD case, one can allow the smooth function to be different on both sides of the discontinuity by interacting Zi with the running variable. The reduced form for this regression is:\nBut let\u2019s say you wanted to present the estimated effect of the treatment on some outcome. That requires estimating a \ufb01rst stage, using \ufb01tted values from that regression, and then estimating a second stage on those \ufb01tted values. This, and only this, will identify the causal\neffect of the treatment on the outcome of interest. The reduced form only estimates the causal effect of the instrument on the outcome. The second-stage model with interaction terms would be the same as before:\n\u02dcx2 i \u02dcxi + \u03b2 \u2217\nWhere \u02dcx are now not only normalized with respect to c0 but are also \ufb01tted values obtained from the \ufb01rst-stage regressions.\nAs Hahn et al. [2001] point out, one needs the same assumptions for identi\ufb01cation as one needs with IV. As with other binary instrumen- tal variables, the fuzzy RDD is estimating the local average treatment effect (LATE) [Imbens and Angrist, 1994], which is the average treat- ment effect for the compliers. In RDD, the compliers are those whose treatment status changed as we moved the value of xi from just to the left of c0 to just to the right of c0.\nThe requirement for RDD to estimate a causal effect are the con- tinuity assumptions. That is, the expected potential outcomes change smoothly as a function of the running variable through the cutoff. In words, this means that the only thing that causes the outcome to change abruptly at c0 is the treatment. But, this can be violated in practice if any of the following is true:\n1. The assignment rule is known in advance. 2. Agents are interested in adjusting. 3. Agents have time to adjust. 4. The cutoff is endogenous to factors that independently cause\npotential outcomes to shift.\n5. There is nonrandom heaping along the running variable.\nExamples include retaking an exam, self-reporting income, and so on. But some other unobservable characteristic change could happen at\neffect of the treatment on the outcome of interest. The reduced form only estimates the causal effect of the instrument on the outcome. The second-stage model with interaction terms would be the same as before:\n\u02dcx2 i \u02dcxi + \u03b2 \u2217\nWhere \u02dcx are now not only normalized with respect to c0 but are also \ufb01tted values obtained from the \ufb01rst-stage regressions.\nAs Hahn et al.", "start_char_idx": 38367, "end_char_idx": 42533, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5e1fa5c4-248d-4abc-9c06-5d66e0a5615c": {"__data__": {"id_": "5e1fa5c4-248d-4abc-9c06-5d66e0a5615c", "embedding": null, "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3895c9fb-cef4-4c38-bd5f-7c8889fe5b0a", "node_type": "4", "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "hash": "a242e6854b86b04962a421846f45199e3bc402a0895f18154ad132cbfcff54b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4404019b-30d8-4d16-9da4-c398059583b9", "node_type": "1", "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "hash": "950a2d8098ee35263a7ca12ec308cd5de4af4c56c60c6ed45a01656e05565f27", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c1837d1d-a042-4678-8007-3202df1d5158", "node_type": "1", "metadata": {}, "hash": "d8b1601d4969609e7302c4617d28a5a92e8a110a624d9b8dd4cd707062871fef", "class_name": "RelatedNodeInfo"}}, "text": "The assignment rule is known in advance. 2. Agents are interested in adjusting. 3. Agents have time to adjust. 4. The cutoff is endogenous to factors that independently cause\npotential outcomes to shift.\n5. There is nonrandom heaping along the running variable.\nExamples include retaking an exam, self-reporting income, and so on. But some other unobservable characteristic change could happen at\neffect of the treatment on the outcome of interest. The reduced form only estimates the causal effect of the instrument on the outcome. The second-stage model with interaction terms would be the same as before:\n\u02dcx2 i \u02dcxi + \u03b2 \u2217\nWhere \u02dcx are now not only normalized with respect to c0 but are also \ufb01tted values obtained from the \ufb01rst-stage regressions.\nAs Hahn et al. [2001] point out, one needs the same assumptions for identi\ufb01cation as one needs with IV. As with other binary instrumen- tal variables, the fuzzy RDD is estimating the local average treatment effect (LATE) [Imbens and Angrist, 1994], which is the average treat- ment effect for the compliers. In RDD, the compliers are those whose treatment status changed as we moved the value of xi from just to the left of c0 to just to the right of c0.\nThe requirement for RDD to estimate a causal effect are the con- tinuity assumptions. That is, the expected potential outcomes change smoothly as a function of the running variable through the cutoff. In words, this means that the only thing that causes the outcome to change abruptly at c0 is the treatment. But, this can be violated in practice if any of the following is true:\n1. The assignment rule is known in advance. 2. Agents are interested in adjusting. 3. Agents have time to adjust. 4. The cutoff is endogenous to factors that independently cause\npotential outcomes to shift.\n5. There is nonrandom heaping along the running variable.\nExamples include retaking an exam, self-reporting income, and so on. But some other unobservable characteristic change could happen at\nthe threshold, and this has a direct effect on the outcome. In other words, the cutoff is endogenous. An example would be age thresh- olds used for policy, such as when a person turns 18 years old and faces more severe penalties for crime. This age threshold triggers the treatment (i.e., higher penalties for crime), but is also correlated with variables that affect the outcomes, such as graduating from high school and voting rights. Let\u2019s tackle these problems separately.\nMcCrary\u2019s density test. Because of these challenges to identi\ufb01cation, a lot of work by econometricians and applied microeconomists has gone toward trying to \ufb01gure out solutions to these problems. The most in\ufb02uential is a density test by Justin McCrary, now called the McCrary density test [2008]. The McCrary density test is used to check whether units are sorting on the running variable. Imagine that there are two rooms with patients in line for some life-saving treatment. Patients in room A will receive the life-saving treatment, and patients in room B will knowingly receive nothing. What would you do if you were in room B? Like me, you\u2019d probably stand up, open the door, and walk across the hall to room A. There are natural incentives for the people in room B to get into room A, and the only thing that would keep people in room B from sorting into room A is if doing so were impossible.\nBut, let\u2019s imagine that the people in room B had successfully sorted themselves into room A. What would that look like to an outsider? If they were successful, then room A would have more patients than room B. In fact, in the extreme, room A is crowded and room B is empty. This is the heart of the McCrary density test, and when we see such things at the cutoff, we have some suggestive evidence that people are sorting on the running variable. This is sometimes called manipulation. Remember earlier when I said we should think of continuity as the null because nature doesn\u2019t make jumps? If you see a turtle on a fencepost, it probably didn\u2019t get there itself. Well, the same goes for the density. If the null is a continuous density through the cutoff, then bunching in the density at the cutoff is a sign that someone is moving over to the cutoff\u2014probably to take advantage of the rewards that await there. Sorting on the sorting variable is a testable prediction under the null of a continuous density.", "start_char_idx": 41771, "end_char_idx": 46139, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c1837d1d-a042-4678-8007-3202df1d5158": {"__data__": {"id_": "c1837d1d-a042-4678-8007-3202df1d5158", "embedding": null, "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3895c9fb-cef4-4c38-bd5f-7c8889fe5b0a", "node_type": "4", "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "hash": "a242e6854b86b04962a421846f45199e3bc402a0895f18154ad132cbfcff54b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5e1fa5c4-248d-4abc-9c06-5d66e0a5615c", "node_type": "1", "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "hash": "2b2e221951766dff5c06c468a00ea9dc04f23370572661fac8fdca550c1104db", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d6371224-5c5b-4442-8a5b-ddcb321b33f4", "node_type": "1", "metadata": {}, "hash": "062d7e58ae17595ab5c51bbb0546e6f138597c79ba0a43bcebc8cd2ac85de208", "class_name": "RelatedNodeInfo"}}, "text": "If they were successful, then room A would have more patients than room B. In fact, in the extreme, room A is crowded and room B is empty. This is the heart of the McCrary density test, and when we see such things at the cutoff, we have some suggestive evidence that people are sorting on the running variable. This is sometimes called manipulation. Remember earlier when I said we should think of continuity as the null because nature doesn\u2019t make jumps? If you see a turtle on a fencepost, it probably didn\u2019t get there itself. Well, the same goes for the density. If the null is a continuous density through the cutoff, then bunching in the density at the cutoff is a sign that someone is moving over to the cutoff\u2014probably to take advantage of the rewards that await there. Sorting on the sorting variable is a testable prediction under the null of a continuous density. Assuming a continuous distri- bution of units, sorting on the running variable means that units are\nmoving just on the other side of the cutoff. Formally, if we assume a desirable treatment D and an assignment rule X \u2265 c0, then we expect individuals will sort into D by choosing X such that X \u2265 c0\u2014so long as they\u2019re able. If they do, then it could imply selection bias insofar as their sorting is a function of potential outcomes.\nThe kind of test needed to investigate whether manipulation is occurring is a test that checks whether there is bunching of units at the cutoff. In other words, we need a density test. McCrary [2008] suggests a formal test where under the null, the density should be continuous at the cutoff point. Under the alternative hypothesis, the density should increase at the kink.10 I\u2019ve always liked this test because it\u2019s a really simple statistical test based on a theory that human beings are opti- mizing under constraints. And if they are optimizing, that makes for testable predictions\u2014like a discontinuous jump in the density at the cutoff. Statistics built on behavioral theory can take us further.\nTo implement the McCrary density test, partition the assignment variable into bins and calculate frequencies (i.e., the number of obser- vations) in each bin. Treat the frequency counts as the dependent variable in a local linear regression. If you can estimate the conditional expectations, then you have the data on the running variable, so in principle you can always do a density test. I recommend the package rddensity,11 which you can install for R as well.12 These packages are based on Cattaneo et al. [2019], which is based on local polynomial regressions that have less bias in the border regions.\nThis is a high-powered test. You need a lot of observations at c0 to distinguish a discontinuity in the density from noise. Let me illustrate in Figure 34 with a picture from McCrary [2008] that shows a situation with and without manipulation.\nCovariate balance and other placebos. It has become common in this literature to provide evidence for the credibility of the underlying iden- tifying assumptions, at least to some degree. While the assumptions cannot be directly tested, indirect evidence may be persuasive. I\u2019ve\n10 In those situations, anyway, where the treatment is desirable to the units.\nFigure 34. A picture with and without a discontinuity in the density. Reprinted from Jour- nal of Econometrics, 142, J. McCrary, \u201cManipulation of the Running Variable in the Regression Discontinuity Design: A Design Test,\u201d 698\u2013714. \u00a9 2008, with permission from Elsevier.\nalready mentioned one such test\u2014the McCrary density test. A sec- ond test is a covariate balance test. For RDD to be valid in your study, there must not be an observable discontinuous change in the average values of reasonably chosen covariates around the cutoff. As these are pretreatment characteristics, they should be invariant to change in treatment assignment. An example of this is from Lee et al. [2004], who evaluated the impact of Democratic vote share just at 50%, on various demographic factors (Figure 35).\nThis test is basically what is sometimes called a placebo test. That is, you are looking for there to be no effects where there shouldn\u2019t be any. So a third kind of test is an extension of that\u2014just as there shouldn\u2019t be effects at the cutoff on pretreatment values, there shouldn\u2019t be effects on the outcome of interest at arbitrarily chosen cutoffs.", "start_char_idx": 45266, "end_char_idx": 49625, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d6371224-5c5b-4442-8a5b-ddcb321b33f4": {"__data__": {"id_": "d6371224-5c5b-4442-8a5b-ddcb321b33f4", "embedding": null, "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3895c9fb-cef4-4c38-bd5f-7c8889fe5b0a", "node_type": "4", "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "hash": "a242e6854b86b04962a421846f45199e3bc402a0895f18154ad132cbfcff54b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c1837d1d-a042-4678-8007-3202df1d5158", "node_type": "1", "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "hash": "e4421d25b64ce862cdbb339f7f05e577f6176832521d4ff30015cb98d79c972f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "94cf0326-16f6-4a44-9abc-93ac34e29e2d", "node_type": "1", "metadata": {}, "hash": "9f12b70849edb411df5ad58b93b55c1cf53573422a1cfe8019f01bf519136834", "class_name": "RelatedNodeInfo"}}, "text": "already mentioned one such test\u2014the McCrary density test. A sec- ond test is a covariate balance test. For RDD to be valid in your study, there must not be an observable discontinuous change in the average values of reasonably chosen covariates around the cutoff. As these are pretreatment characteristics, they should be invariant to change in treatment assignment. An example of this is from Lee et al. [2004], who evaluated the impact of Democratic vote share just at 50%, on various demographic factors (Figure 35).\nThis test is basically what is sometimes called a placebo test. That is, you are looking for there to be no effects where there shouldn\u2019t be any. So a third kind of test is an extension of that\u2014just as there shouldn\u2019t be effects at the cutoff on pretreatment values, there shouldn\u2019t be effects on the outcome of interest at arbitrarily chosen cutoffs. Imbens and Lemieux [2008] suggest looking at one side of the discontinuity, taking the median value of the running variable in that section, and pretending it was a discontinuity, c(cid:12) 0. Then test whether there is a discontinuity in the outcome at c(cid:12) 0. You do not want to \ufb01nd anything.\nNonrandom heaping on the running variable. Almond et al. [2010] is a fascinating study. The authors are interested in estimating the causal effect of medical expenditures on health outcomes, in part because many medical technologies, while effective, may not justify the costs associated with their use. Determining their effectiveness is challeng- ing given that medical resources are, we hope, optimally assigned to\nFigure 35. Panels refer to (top left to bottom right) district characteristics: real income,\nto vote. Circles represent the average characteristic within intervals of 0.01\nin Democratic vote share. The continuous line represents the predicted val-\nues from a fourth-order polynomial in vote share \ufb01tted separately for points\nabove and below the 50% threshold. The dotted line represents the 95% con-\n\ufb01dence interval. Reprinted from Lee, D. S., Moretti, E., and Butler, M. J. (2004). \u201cDo Voters Affect or Elect Policies: Evidence from the U.S. House.\u201d Quarterly Journal of Economics, 119(3):807\u2013859. Permission from Oxford University Press.\npatients based on patient potential outcomes. To put it a different way, if the physician perceives that an intervention will have the best out- come, then that is likely a treatment that will be assigned to the patient. This violates independence, and more than likely, if the endogeneity of the treatment is deep enough, controlling for selection directly will be tough, if not impossible. As we saw with our earlier example of the per- fect doctor, such nonrandom assignment of interventions can lead to confusing correlations. Counterintuitive correlations may be nothing more than selection bias.\nBut Almond et al. [2010] had an ingenious insight\u2014in the United States, it is typically the case that babies with a very low birth weight\nreceive heightened medical attention. This categorization is called the \u201cvery low birth weight\u201d range, and such low birth weight is quite danger- ous for the child. Using administrative hospital records linked to mor- tality data, the authors \ufb01nd that the 1-year infant mortality decreases by around 1 percentage point when the child\u2019s birth weight is just below the 1,500-gram threshold compared to those born just above. Given the mean 1-year mortality of 5.5%, this estimate is sizable, suggesting that the medical interventions triggered by the very-low-birth-weight classi\ufb01cation have bene\ufb01ts that far exceed their costs.\nBarreca et al. [2011] and Barreca et al. [2016] highlight some of econometric issues related to what they call \u201cheaping\u201d on the running variable. Heaping is when there is an excess number of units at cer- tain points along the running variable. In this case, it appeared to be at regular 100-gram intervals and was likely caused by a tendency for hospitals to round to the nearest integer. A visualization of this problem can be seen in the original Almond et al. [2010], which I reproduce here in Figure 36. The long black lines appearing regularly across the birth- weight distribution are excess mass of children born at those numbers. This sort of event is unlikely to occur naturally in nature, and it is almost certainly caused by either sorting or rounding.", "start_char_idx": 48754, "end_char_idx": 53115, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "94cf0326-16f6-4a44-9abc-93ac34e29e2d": {"__data__": {"id_": "94cf0326-16f6-4a44-9abc-93ac34e29e2d", "embedding": null, "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3895c9fb-cef4-4c38-bd5f-7c8889fe5b0a", "node_type": "4", "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "hash": "a242e6854b86b04962a421846f45199e3bc402a0895f18154ad132cbfcff54b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d6371224-5c5b-4442-8a5b-ddcb321b33f4", "node_type": "1", "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "hash": "91cf92e15e9b3f6297439ac73f083d05543efa394aa3722351a1527b92c96d1f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "86c248d8-643f-40a0-9121-4226ec37da1f", "node_type": "1", "metadata": {}, "hash": "c149af2473a6eeaa1088566668be2f31ffe3b0768692195b3ee5f8458ebe8d0b", "class_name": "RelatedNodeInfo"}}, "text": "Barreca et al. [2011] and Barreca et al. [2016] highlight some of econometric issues related to what they call \u201cheaping\u201d on the running variable. Heaping is when there is an excess number of units at cer- tain points along the running variable. In this case, it appeared to be at regular 100-gram intervals and was likely caused by a tendency for hospitals to round to the nearest integer. A visualization of this problem can be seen in the original Almond et al. [2010], which I reproduce here in Figure 36. The long black lines appearing regularly across the birth- weight distribution are excess mass of children born at those numbers. This sort of event is unlikely to occur naturally in nature, and it is almost certainly caused by either sorting or rounding. It could be due to less sophisticated scales or, more troubling, to staff rounding a child\u2019s birth weight to 1,500 grams in order to make the child eligible for increased medical attention.\nAlmond et al. [2010] attempt to study this more carefully using the conventional McCrary density test and \ufb01nd no clear, statistically signi\ufb01cant evidence for sorting on the running variable at the 1,500- gram cutoff. Satis\ufb01ed, they conduct their main analysis, in which they \ufb01nd a causal effect of around a 1-percentage-point reduction in 1-year mortality.\nThe focus of Barreca et al. [2011] and Barreca et al. [2016] is very much on the heaping phenomenon shown in Figure 36. Part of the strength of their work, though, is their illustration of some of the short- comings of a conventional McCrary density test. In this case, the data heap at 1,500 grams appears to be babies whose mortality rates are unusually high. These children are outliers compared to units to both the immediate left and the immediate right. It is important to note that such events would not occur naturally; there is no reason to believe that\nFigure 36. Distribution of births by gram. Reprinted from Almond, D., Doyle, J. J.,\nKowalski, A., and Williams, H. (2010). \u201cEstimating Returns to Medical Care: Evidence from at-risk Newborns.\u201d The Quarterly Journal of Economics, 125(2):591\u2013634. Permission from Oxford University Press.\nnature would produce heaps of children born with outlier health defects every 100 grams. The authors comment on what might be going on:\nThis [heaping at 1,500 grams] may be a signal that poor-quality hos- pitals have relatively high propensities to round birth weights but is also consistent with manipulation of recorded birth weights by doctors, nurses, or parents to obtain favorable treatment for their children. Barreca et al. [2011] show that this nonrandom heaping leads one to conclude that it is \u201cgood\u201d to be strictly less than any 100-g cutoff between 1,000 and 3,000 grams.\nSince estimation in an RDD compares means as we approach the threshold from either side, the estimates should not be sensitive to the observations at the thresholds itself. Their solution is a so-called \u201cdonut hole\u201d RDD, wherein they remove units in the vicinity of 1,500 grams and reestimate the model. Insofar as units are dropped, the parameter we are estimating at the cutoff has become an even more unusual type of local average treatment effect that may be even less informative about the average treatment effects that policy makers are desperate to know. But the strength of this rule is that it allows for the possibility that units at the heap differ markedly due to selection bias from those in the surrounding area. Dropping these units reduces the\nsample size by around 2% but has very large effects on 1-year mortal- ity, which is approximately 50% lower than what was found by Almond et al. [2010].\nThese companion papers help us better understand some of the ways in which selection bias can creep into the RDD. Heaping is not the end of the world, which is good news for researchers facing such a problem. The donut hole RDD can be used to circumvent some of the problems. But ultimately this solution involves dropping observations, and insofar as your sample size is small relative to the number of heap- ing units, the donut hole approach could be infeasible. It also changes the parameter of interest to be estimated in ways that may be di\ufb03cult to understand or explain. Caution with nonrandom heaping along the running variable is probably a good thing.", "start_char_idx": 52351, "end_char_idx": 56672, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "86c248d8-643f-40a0-9121-4226ec37da1f": {"__data__": {"id_": "86c248d8-643f-40a0-9121-4226ec37da1f", "embedding": null, "metadata": {"page number": "313 - 314", "chapter": "Regression Discontinuity"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4f33b2b1-e0b1-4436-80fb-33767c564527", "node_type": "4", "metadata": {"page number": "313 - 314", "chapter": "Regression Discontinuity"}, "hash": "4995d52cc6901a1a2ac99242bafaa1407ee42aa3d95db67311b1857bb7d2d125", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "94cf0326-16f6-4a44-9abc-93ac34e29e2d", "node_type": "1", "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}, "hash": "eebe30dba468dc5a2695de2f6e4aad9089aaaf36db7c6fef901725a91de46762", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8dc5e05a-6302-47ce-a15e-1e1c5e0ac640", "node_type": "1", "metadata": {}, "hash": "3b0968d9f8a2e11516ae2e29226105fa77ec1b7c74b9b78f7198a0cd25d4f079", "class_name": "RelatedNodeInfo"}}, "text": "Replicating a Popular Design: The Close Election:\nWithin RDD, there is a particular kind of design that has become quite popular, this design the close-election design. Essentially, exploits a feature of American democracies wherein winners in polit- ical races are declared when a candidate gets the minimum needed share of votes. Insofar as very close races represent exogenous assignments of a party\u2019s victory, which I\u2019ll discuss below, then we can use these close elections to identify the causal effect of the winner on a variety of outcomes. We may also be able to test political economy theories that are otherwise nearly impossible to evaluate.\nThe following section has two goals. First, to discuss in detail the close election design using the classic Lee et al. [2004]. Second, to show how to implement the close-election design by replicating several parts of Lee et al. [2004].\nDo Politicians or Voters Pick Policies? The big question motivating Lee et al. (2004) has to do with whether and in which way voters affect pol- icy. There are two fundamentally different views of the role of elections in a representative democracy: convergence theory and divergence theory.\nsample size by around 2% but has very large effects on 1-year mortal- ity, which is approximately 50% lower than what was found by Almond et al. [2010].\nThese companion papers help us better understand some of the ways in which selection bias can creep into the RDD. Heaping is not the end of the world, which is good news for researchers facing such a problem. The donut hole RDD can be used to circumvent some of the problems. But ultimately this solution involves dropping observations, and insofar as your sample size is small relative to the number of heap- ing units, the donut hole approach could be infeasible. It also changes the parameter of interest to be estimated in ways that may be di\ufb03cult to understand or explain. Caution with nonrandom heaping along the running variable is probably a good thing.", "start_char_idx": 0, "end_char_idx": 1997, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8dc5e05a-6302-47ce-a15e-1e1c5e0ac640": {"__data__": {"id_": "8dc5e05a-6302-47ce-a15e-1e1c5e0ac640", "embedding": null, "metadata": {"page number": "314 - 341"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb1a07a4-6944-47b1-832e-5dff9fe341c4", "node_type": "4", "metadata": {"page number": "314 - 341"}, "hash": "3290a0b169dcd40ad1992e9a9432a980fba7e092425e59e964f2ddc137429db9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "86c248d8-643f-40a0-9121-4226ec37da1f", "node_type": "1", "metadata": {"page number": "313 - 314", "chapter": "Regression Discontinuity"}, "hash": "802292a7de1222706e800b33f99532e8c4297f5017c1eabceba424c559dcc489", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9cb2841a-3c5d-44b8-98bd-5f3b318f2c07", "node_type": "1", "metadata": {}, "hash": "cd9805439b237eee87179bce508a6edcd332569636d9b7151031cb7df6b9c721", "class_name": "RelatedNodeInfo"}}, "text": "Replicating a Popular Design: The Close Election:\nWithin RDD, there is a particular kind of design that has become quite popular, this design the close-election design. Essentially, exploits a feature of American democracies wherein winners in polit- ical races are declared when a candidate gets the minimum needed share of votes. Insofar as very close races represent exogenous assignments of a party\u2019s victory, which I\u2019ll discuss below, then we can use these close elections to identify the causal effect of the winner on a variety of outcomes. We may also be able to test political economy theories that are otherwise nearly impossible to evaluate.\nThe following section has two goals. First, to discuss in detail the close election design using the classic Lee et al. [2004]. Second, to show how to implement the close-election design by replicating several parts of Lee et al. [2004].\nDo Politicians or Voters Pick Policies? The big question motivating Lee et al. (2004) has to do with whether and in which way voters affect pol- icy. There are two fundamentally different views of the role of elections in a representative democracy: convergence theory and divergence theory.\nThe convergence theory states that heterogeneous voter ideology forces each candidate to moderate his or her position (e.g., similar to the median voter theorem):\nCompetition for votes can force even the most partisan Repub- licans and Democrats to moderate their policy choices. In the extreme case, competition may be so strong that it leads to \u201cfull policy convergence\u201d: opposing parties are forced to adopt identical policies. [Lee et al. 2004, 807]\nDivergence theory is a slightly more commonsense view of politi- cal actors. When partisan politicians cannot credibly commit to certain policies, then convergence is undermined and the result can be full policy \u201cdivergence.\u201d Divergence is when the winning candidate, after taking o\ufb03ce, simply pursues her most-preferred policy. In this extreme case, voters are unable to compel candidates to reach any kind of pol- icy compromise, and this is expressed as two opposing candidates choosing very different policies under different counterfactual victory scenarios.\nLee et al. [2004] present a model, which I\u2019ve simpli\ufb01ed. Let R and D be candidates in a congressional race. The policy space is a sin- gle dimension where D\u2019s and R\u2019s policy preferences in a period are quadratic loss functions, u(l) and v(l), and l is the policy variable. Each player has some bliss point, which is his or her most preferred location along the unidimensional policy range. For Democrats, it\u2019s l\u2217 = c(> 0), and for Republicans it\u2019s l\u2217 =0. Here\u2019s what this means.\nEx ante, voters expect the candidate to choose some policy and they expect the candidate to win with probability P(xe, ye), where xe and ye are the policies chosen by Democrats and Republicans, respectively. When x>ye, then\nP\u2217 represents the underlying popularity of the Democratic Party, or put differently, the probability that D would win if the policy chosen x equaled the Democrat\u2019s bliss point c.\nThe solution to this game has multiple Nash equilibria, which I\n2. Complete divergence: Voters elect politicians with \ufb01xed policies\n1. Partial/complete convergence: Voters affect policies.\nwho do whatever they want to do.13 (cid:129) Key result is that more popularity has no effect on policies.\n(cid:129) An exogenous shock to P\u2217 (i.e., dropping Democrats into the district) does nothing to equilibrium policies. Voters elect politicians who then do whatever they want because of their \ufb01xed policy preferences.\nThat is,\nThe potential roll-call voting record outcomes of the candidate follow- ing some election is\nwhere Dt indicates whether a Democrat won the election. That is, only the winning candidate\u2019s policy is observed. This expression can be transformed into regression equations:\nwhere \u03b10 and \u03b20 are constants.\nThis equation can\u2019t be directly estimated because we never observe P\u2217. But suppose we could randomize Dt. Then Dt would be independent of P\u2217 t and \u03b5t. Then taking conditional expectations with respect to Dt, we get:\nIt takes what it wants. See https://www\nThe \u201celect\u201d component is \u03c01[PD ] and is estimated as the differ- \u2212 PR t+1 t+1 ence in mean voting records between the parties at time t. The fraction of districts won by Democrats in t + 1 is an estimate of [PD ]. t+1 Because we can estimate the total effect, \u03b3 , of a Democrat victory in t on RCt+1, we can net out the elect component to implicitly get the \u201ceffect\u201d component.", "start_char_idx": 0, "end_char_idx": 4537, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9cb2841a-3c5d-44b8-98bd-5f3b318f2c07": {"__data__": {"id_": "9cb2841a-3c5d-44b8-98bd-5f3b318f2c07", "embedding": null, "metadata": {"page number": "314 - 341"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb1a07a4-6944-47b1-832e-5dff9fe341c4", "node_type": "4", "metadata": {"page number": "314 - 341"}, "hash": "3290a0b169dcd40ad1992e9a9432a980fba7e092425e59e964f2ddc137429db9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8dc5e05a-6302-47ce-a15e-1e1c5e0ac640", "node_type": "1", "metadata": {"page number": "314 - 341"}, "hash": "0a46a550b721d7e77fbbd08789ddc07c221d753bf7274a83d98aaca8f874542f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9edad27-d956-4d95-9c9f-e2bddd52e1e8", "node_type": "1", "metadata": {}, "hash": "2eb05dace0d15dc64b0811ff4b3e5878d5cfc554b7bfe0ff81375ad93d708593", "class_name": "RelatedNodeInfo"}}, "text": "That is, only the winning candidate\u2019s policy is observed. This expression can be transformed into regression equations:\nwhere \u03b10 and \u03b20 are constants.\nThis equation can\u2019t be directly estimated because we never observe P\u2217. But suppose we could randomize Dt. Then Dt would be independent of P\u2217 t and \u03b5t. Then taking conditional expectations with respect to Dt, we get:\nIt takes what it wants. See https://www\nThe \u201celect\u201d component is \u03c01[PD ] and is estimated as the differ- \u2212 PR t+1 t+1 ence in mean voting records between the parties at time t. The fraction of districts won by Democrats in t + 1 is an estimate of [PD ]. t+1 Because we can estimate the total effect, \u03b3 , of a Democrat victory in t on RCt+1, we can net out the elect component to implicitly get the \u201ceffect\u201d component.\nBut random assignment of Dt is crucial. For without it, this equation would re\ufb02ect \u03c01 and selection (i.e., Democratic districts have more lib- eral bliss points). So the authors aim to randomize Dt using a RDD, which I\u2019ll now discuss in detail.\nReplication exercise. There are two main data sets in this project. The \ufb01rst is a measure of how liberal an o\ufb03cial voted. This is collected from the Americans for Democratic Action (ADA) linked with House of Representatives election results for 1946\u20131995. Authors use the ADA score for all US House representatives from 1946 to 1995 as their vot- ing record index. For each Congress, the ADA chose about twenty-\ufb01ve high-pro\ufb01le roll-call votes and created an index varying from 0 to 100 for each representative. Higher scores correspond to a more \u201cliberal\u201d voting record. The running variable in this study is the vote share. That is the share of all votes that went to a Democrat. ADA scores are then linked to election returns data during that period.\nRecall that we need randomization of Dt. The authors have a clever solution. They will use arguably exogenous variation in Democratic\nTable 45. Original results based on ADA scores\u2014close elections sample.\nEstimated gap\nNote: Standard errors in parentheses. The unit of observation is a district-congressional session. The sample includes only observations where the Democrat vote share at time t is strictly between 48% and 52%. The estimated gap is the difference in the average of the relevant variable for observations for which the Democrat vote share at time t is strictly between 50% and 52% and observations for which the Democrat vote share at time t is strictly between 48% and 50%. Time t and t + 1 refer to congressional sessions. ADAt is the adjusted ADA voting score. Higher ADA scores correspond to more liberal roll-call voting records. Sample size is 915.\nwins to check whether convergence or divergence is correct. If con- vergence is true, then Republicans and Democrats who just barely won should vote almost identically, whereas if divergence is true, they should vote differently at the margins of a close race. This \u201cat the mar- gins of a close race\u201d is crucial because the idea is that it is at the margins of a close race that the distribution of voter preferences is the same. And if voter preferences are the same, but policies diverge at the cutoff, then it suggests politicians and not voters are driving policy making.\nThe exogenous shock comes from the discontinuity in the running variable. At a vote share of just above 0.5, the Democratic candidate wins. They argue that just around that cutoff, random chance deter- mined the Democratic win\u2014hence the random assignment of Dt [Cat- taneo et al., 2015]. Table 45 is a reproduction of Cattaneo et al.\u2019s main results. The effect of a Democratic victory increases liberal voting by 21 points in the next period, 48 points in the current period, and the probability of reelection by 48%. The authors \ufb01nd evidence for both divergence and incumbency advantage using this design. Let\u2019s dig into the data ourselves now and see if we can \ufb01nd where the authors are getting these results. We will examine the results around Table 45 by playing around with the data and different speci\ufb01cations.", "start_char_idx": 3753, "end_char_idx": 7800, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b9edad27-d956-4d95-9c9f-e2bddd52e1e8": {"__data__": {"id_": "b9edad27-d956-4d95-9c9f-e2bddd52e1e8", "embedding": null, "metadata": {"page number": "314 - 341"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb1a07a4-6944-47b1-832e-5dff9fe341c4", "node_type": "4", "metadata": {"page number": "314 - 341"}, "hash": "3290a0b169dcd40ad1992e9a9432a980fba7e092425e59e964f2ddc137429db9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9cb2841a-3c5d-44b8-98bd-5f3b318f2c07", "node_type": "1", "metadata": {"page number": "314 - 341"}, "hash": "04a156074a6fd08dbc22ce02bd0e3a292c575eb2b59a5f1917ef6cbf195fd46c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e11147b7-2ec9-4185-9f3e-1eb4d17ec8c2", "node_type": "1", "metadata": {}, "hash": "88eb74bd25d7a61696a0815715a0e4d146853d8839e27e70586b4a8b4a91b0cf", "class_name": "RelatedNodeInfo"}}, "text": "The exogenous shock comes from the discontinuity in the running variable. At a vote share of just above 0.5, the Democratic candidate wins. They argue that just around that cutoff, random chance deter- mined the Democratic win\u2014hence the random assignment of Dt [Cat- taneo et al., 2015]. Table 45 is a reproduction of Cattaneo et al.\u2019s main results. The effect of a Democratic victory increases liberal voting by 21 points in the next period, 48 points in the current period, and the probability of reelection by 48%. The authors \ufb01nd evidence for both divergence and incumbency advantage using this design. Let\u2019s dig into the data ourselves now and see if we can \ufb01nd where the authors are getting these results. We will examine the results around Table 45 by playing around with the data and different speci\ufb01cations.\n1 use https://github.com/scunning1975/mixtape/raw/master/lmb-data.dta, clear 2 3 * Replicating Table 1 of Lee, Moretti and Butler (2004) 4 reg score lagdemocrat cluster(id)\n1 library(tidyverse) 2 library(haven) 3 library(estimatr) 4 5 read_data <- function(df) 6 { 7 8 9 10 11 } 12 13 lmb_data <- read_data(\"lmb-data.dta\") 14 15 lmb_subset <- lmb_data %>% 16 17 18 lm_1 <- lm_robust(score ~ lagdemocrat, data = lmb_subset, clusters = id) 19 lm_2 <- lm_robust(score ~ democrat, data = lmb_subset, clusters = id) 20 lm_3 <- lm_robust(democrat ~ lagdemocrat, data = lmb_subset, clusters = id) 21 22 summary(lm_1) 23 summary(lm_2) 24 summary(lm_3)\ndf, sep = \"\")\nTable 46. Replicated results based on ADA scores\u2014close elections sample.\nEstimated gap\nNote: Cluster robust standard errors in parentheses. * p<0.10, ** p<0.05, *** p<0.01\nWe reproduce regression results from Lee, Moretti, and Butler in Table 46. While the results are close to Lee, Moretti, and Butler\u2019s origi- nal table, they are slightly different. But ignore that for now. The main thing to see is that we used regressions limited to the window right around the cutoff to estimate the effect. These are local regressions in the sense that they use data close to the cutoff. Notice the win- dow we chose\u2014we are only using observations between 0.48 and 0.52 vote share. So this regression is estimating the coe\ufb03cient on Dt right around the cutoff. What happens if we use all the data?\n1 #using all data (note data used is lmb_data, not lmb_subset) 2 3 lm_1 <- lm_robust(score ~ lagdemocrat, data = lmb_data, clusters = id) 4 lm_2 <- lm_robust(score ~ democrat, data = lmb_data, clusters = id) 5 lm_3 <- lm_robust(democrat ~ lagdemocrat, data = lmb_data, clusters = id) 6 7 summary(lm_1) 8 summary(lm_2) 9 summary(lm_3)\nTable 47. Results based on ADA scores\u2014full sample.\nEstimated gap\nNote: Cluster robust standard errors in parentheses. * p<0.10, ** p<0.05, *** p<0.01\nNotice that when we use all of the data, we get somewhat differ- ent effects (Table 47). The effect on future ADA scores gets larger by 10 points, but the contemporaneous effect gets smaller. The effect on incumbency, though, increases considerably. So here we see that sim- ply running the regression yields different estimates when we include data far from the cutoff itself.\nNeither of these regressions included controls for the running vari- able though. It also doesn\u2019t use the recentering of the running variable. So let\u2019s do both. We will simply subtract 0.5 from the running variable so that values of 0 are where the vote share equals 0.5, negative val- ues are Democratic vote shares less than 0.5, and positive values are Democratic vote shares above 0.5. To do this, type in the following lines:\nclusters = id)\nclusters = id)\nWe report our analysis from the programming in Table 48.", "start_char_idx": 6984, "end_char_idx": 10625, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e11147b7-2ec9-4185-9f3e-1eb4d17ec8c2": {"__data__": {"id_": "e11147b7-2ec9-4185-9f3e-1eb4d17ec8c2", "embedding": null, "metadata": {"page number": "314 - 341"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb1a07a4-6944-47b1-832e-5dff9fe341c4", "node_type": "4", "metadata": {"page number": "314 - 341"}, "hash": "3290a0b169dcd40ad1992e9a9432a980fba7e092425e59e964f2ddc137429db9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9edad27-d956-4d95-9c9f-e2bddd52e1e8", "node_type": "1", "metadata": {"page number": "314 - 341"}, "hash": "d8c110dfdc50c213fa1bc44f231e6e2fe1048df116fd8074ce21dc690548759a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "284e461c-18c7-4b67-a865-ce74fc01354e", "node_type": "1", "metadata": {}, "hash": "eb8fe0b018cb10eaa9fcc954948b338777612a7a1a5f92168fe2335d7e5333d4", "class_name": "RelatedNodeInfo"}}, "text": "The effect on future ADA scores gets larger by 10 points, but the contemporaneous effect gets smaller. The effect on incumbency, though, increases considerably. So here we see that sim- ply running the regression yields different estimates when we include data far from the cutoff itself.\nNeither of these regressions included controls for the running vari- able though. It also doesn\u2019t use the recentering of the running variable. So let\u2019s do both. We will simply subtract 0.5 from the running variable so that values of 0 are where the vote share equals 0.5, negative val- ues are Democratic vote shares less than 0.5, and positive values are Democratic vote shares above 0.5. To do this, type in the following lines:\nclusters = id)\nclusters = id)\nWe report our analysis from the programming in Table 48. While the incumbency effect falls closer to what Lee et al. [2004] \ufb01nd, the effects are still quite different.\nIt is common, though, to allow the running variable to vary on either side of the discontinuity, but how exactly do we implement that? Think of it\u2014we need for a regression line to be on either side, which means necessarily that we have two lines left and right of the discontinuity. To do this, we need an interaction\u2014speci\ufb01cally an interaction of the running variable with the treatment variable. To implement this in Stata, we can use the code shown in lmb_4.do.\nTable 48. Results based on ADA scores\u2014full sample.\nEstimated gap\nNote: Cluster robust standard errors in parentheses. *p < 0.10. **p < 0.05. ***p < 0.01.\n1 * Use all the data but interact the treatment variable with the running variable 2 xi: reg score i.lagdemocrat*demvoteshare_c, cluster(id) 3 xi: reg score i.democrat*demvoteshare_c, cluster(id) 4 xi: reg democrat i.lagdemocrat*demvoteshare_c, cluster(id)\n1 lm_1 <- lm_robust(score ~ lagdemocrat*demvoteshare_c, 2 data = lmb_data, clusters = id) 3 lm_2 <- lm_robust(score ~ democrat*demvoteshare_c, 4 data = lmb_data, clusters = id) 5 lm_3 <- lm_robust(democrat ~ lagdemocrat*demvoteshare_c, 6 7 8 summary(lm_1) 9 summary(lm_2) 10 summary(lm_3) 11\ndata = lmb_data, clusters = id)\nIn Table 49, we report the global regression analysis with the run-\nning variable interacted with the treatment variable. This pulled down\nthe coe\ufb03cients somewhat, but they remain larger than what was found\nwhen we used only those observations within 0.02 points of the 0.5.\nFinally, let\u2019s estimate the model with a quadratic.\n1 * Use all the data but interact the treatment variable with the running variable and\n2 gen demvoteshare_sq = demvoteshare_c^2 3 xi: reg score lagdemocrat##c.(demvoteshare_c demvoteshare_sq), cluster(id) 4 xi: reg score democrat##c.(demvoteshare_c demvoteshare_sq), cluster(id) 5 xi: reg democrat lagdemocrat##c.(demvoteshare_c demvoteshare_sq),\n5 data = lmb_data, clusters = id) 6 lm_2 <- lm_robust(score ~ democrat*demvoteshare_c +\ndata = lmb_data, clusters = id)\ndata = lmb_data, clusters = id)\nIncluding the quadratic causes the estimated effect of a demo- cratic victory on future voting to fall considerably (see Table 50). The effect on contemporaneous voting is smaller than what Lee et al. [2004] \ufb01nd, as is the incumbency effect. But the purpose here is simply to illustrate the standard steps using global regressions.\nBut notice, we are still estimating global regressions. And it is for that reason that the coe\ufb03cient is larger. This suggests that there exist strong outliers in the data that are causing the distance at c0 to spread more widely. So a natural solution is to again limit our analysis to a smaller window. What this does is drop the observations far away from c0 and omit the in\ufb02uence of outliers from our estimation at the cutoff.", "start_char_idx": 9819, "end_char_idx": 13524, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "284e461c-18c7-4b67-a865-ce74fc01354e": {"__data__": {"id_": "284e461c-18c7-4b67-a865-ce74fc01354e", "embedding": null, "metadata": {"page number": "314 - 341"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb1a07a4-6944-47b1-832e-5dff9fe341c4", "node_type": "4", "metadata": {"page number": "314 - 341"}, "hash": "3290a0b169dcd40ad1992e9a9432a980fba7e092425e59e964f2ddc137429db9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e11147b7-2ec9-4185-9f3e-1eb4d17ec8c2", "node_type": "1", "metadata": {"page number": "314 - 341"}, "hash": "c0b36efd1eaca3979cd2148f7983b8f709a9bb1bba6c542c27ba2efc27233c0a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6684648b-0ac7-483d-9989-ddb0dbd51658", "node_type": "1", "metadata": {}, "hash": "4c1c125528949c67e0669c61e39f65b5bdd92e04bbe1a9bb789914337f8b4040", "class_name": "RelatedNodeInfo"}}, "text": "The effect on contemporaneous voting is smaller than what Lee et al. [2004] \ufb01nd, as is the incumbency effect. But the purpose here is simply to illustrate the standard steps using global regressions.\nBut notice, we are still estimating global regressions. And it is for that reason that the coe\ufb03cient is larger. This suggests that there exist strong outliers in the data that are causing the distance at c0 to spread more widely. So a natural solution is to again limit our analysis to a smaller window. What this does is drop the observations far away from c0 and omit the in\ufb02uence of outliers from our estimation at the cutoff. Since we used +/\u2212 \u22120.02 last time, we\u2019ll use +/\u2212 \u22120.05 this time just to mix things up.\nTable 49. Results based on ADA scores\u2014full sample with linear interactions.\nEstimated gap\nNote: Cluster robust standard errors in parentheses. *p < 0.10, **p < 0.05, ***p < 0.01\nTable 50. Results based on ADA scores\u2014full sample with linear and quadratic\nEstimated gap\nNote: Cluster robust standard errors in parentheses. *p < 0.10, **p < 0.05, ***p < 0.01\n1 * Use 5 points from the cutoff 2 xi: reg score lagdemocrat##c.(demvoteshare_c demvoteshare_sq) if lagdemvoteshare>.45 & lagdemvoteshare<.55, cluster(id)\n3 xi: reg score democrat##c.(demvoteshare_c demvoteshare_sq) if\nlagdemvoteshare>.45 & lagdemvoteshare<.55, cluster(id) 4 xi: reg democrat lagdemocrat##c.(demvoteshare_c demvoteshare_sq) if lagdemvoteshare>.45 & lagdemvoteshare<.55, cluster(id)\n6 data = lmb_data, clusters = id) 7 lm_2 <- lm_robust(score ~ democrat*demvoteshare_c +\ndata = lmb_data, clusters = id)\ndata = lmb_data, clusters = id)\nTable 51. Results based on ADA scores\u2014close election sample with linear and\nEstimated gap\nNote: Cluster robust standard errors in parentheses. *p < 0.10, **p < 0.05, ***p < 0.01\nAs can be seen in Table 51, when we limit our analysis to +/\u2212 0.05 around the cutoff, we are using more observations away from the cutoff than we used in our initial analysis. That\u2019s why we only have 2,441 observations for analysis as opposed to the 915 we had in our original analysis. But we also see that including the quadratic interac- tion pulled the estimated size on future voting down considerably, even when using the smaller sample.\nBut putting that aside, let\u2019s talk about all that we just did. First we \ufb01t a model without controlling for the running variable. But then we included the running variable, introduced in a variety of ways. For instance, we interacted the variable of Democratic vote share with the democratic dummy, as well as including a quadratic. In all this analy- sis, we extrapolated trends lines from the running variable beyond the support of the data to estimate local average treatment effects right at the cutoff.\nBut we also saw that the inclusion of the running variable in any form tended to reduce the effect of a victory for Democrats on future Democratic voting patterns, which was interesting. Lee et al. [2004] original estimate of around 21 is attenuated considerably when we include controls for the running variable, even when we go back to estimating very local \ufb02exible regressions. While the effect remains signi\ufb01cant, it is considerably smaller, whereas the immediate effect remains quite large.\nBut there are still other ways to explore the impact of the treatment at the cutoff. For instance, while Hahn et al. [2001] clari\ufb01ed assump- tions about RDD\u2014speci\ufb01cally, continuity of the conditional expected\npotential outcomes\u2014they also framed estimation as a nonparamet- ric problem and emphasized using local polynomial regressions. What exactly does this mean though in practice?\nNonparametric methods mean a lot of different things to different people in statistics, but in RDD contexts, the idea is to estimate a model that doesn\u2019t assume a functional form for the relationship between the outcome variable (Y) and the running variable (X).", "start_char_idx": 12895, "end_char_idx": 16791, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6684648b-0ac7-483d-9989-ddb0dbd51658": {"__data__": {"id_": "6684648b-0ac7-483d-9989-ddb0dbd51658", "embedding": null, "metadata": {"page number": "314 - 341"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb1a07a4-6944-47b1-832e-5dff9fe341c4", "node_type": "4", "metadata": {"page number": "314 - 341"}, "hash": "3290a0b169dcd40ad1992e9a9432a980fba7e092425e59e964f2ddc137429db9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "284e461c-18c7-4b67-a865-ce74fc01354e", "node_type": "1", "metadata": {"page number": "314 - 341"}, "hash": "6c200d7d1f6cc392b40d43189708fd88e8c277aa29d4ae01a363f3a0ffafe148", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fea6bc01-b9d3-4ea3-a067-b890b1418966", "node_type": "1", "metadata": {}, "hash": "7e1212662367da16465badd717ae35a6268068514baabcf21706ce7fc36c452d", "class_name": "RelatedNodeInfo"}}, "text": "While the effect remains signi\ufb01cant, it is considerably smaller, whereas the immediate effect remains quite large.\nBut there are still other ways to explore the impact of the treatment at the cutoff. For instance, while Hahn et al. [2001] clari\ufb01ed assump- tions about RDD\u2014speci\ufb01cally, continuity of the conditional expected\npotential outcomes\u2014they also framed estimation as a nonparamet- ric problem and emphasized using local polynomial regressions. What exactly does this mean though in practice?\nNonparametric methods mean a lot of different things to different people in statistics, but in RDD contexts, the idea is to estimate a model that doesn\u2019t assume a functional form for the relationship between the outcome variable (Y) and the running variable (X). The model would be something like this:\nY = f(X) + \u03b5\nA very basic method would be to calculate E[Y] for each bin on X, like a histogram. And Stata has an option to do this called cmogram, cre- ated by Christopher Robert. The program has a lot of useful options, and we can re-create important \ufb01gures from Lee et al. [2004]. Figure 37 shows the relationship between the Democratic win (as a function of the running variable, Democratic vote share) and the candidates, second-period ADA score.\nFigure 37. Showing total effect of initial win on future ADA scores. Reprinted from Lee,\nD. S., Moretti, E., and Butler, M. J. (2004). \u201cDo Voters Affect or Elect Policies: Evidence from the U.S. House.\u201d Quarterly Journal of Economics, 119(3):807\u2013 859. Permission from Oxford University Press.\nTo reproduce this, there are a few options. You could manually cre- ate this \ufb01gure yourself using either the \u201ctwoway\u201d command in Stata or \u201cggplot\u201d in R. But I\u2019m going to show you using the canned cmo- gram routine that was created, as it\u2019s a quick-and-dirty way to get some information about the data.\n1 * Nonparametric estimation graphic 2 ssc install cmogram 3 cmogram score lagdemvoteshare, cut(0.5) scatter line(0.5) qfitci 4 cmogram score lagdemvoteshare, cut(0.5) scatter line(0.5) lfit 5 cmogram score lagdemvoteshare, cut(0.5) scatter line(0.5) lowess\n1 #aggregating the data 2 categories <- lmb_data$lagdemvoteshare 3 4 demmeans <- split(lmb_data$score, cut(lmb_data$lagdemvoteshare, 100)) %>% 5 6 7 8 agg_lmb_data <- data.frame(score = demmeans, lagdemvoteshare = seq(0.01,1,\n9 10 #plotting 11 lmb_data <- lmb_data %>% 12 mutate(gg_group = case_when(lagdemvoteshare > 0.5 ~ 1, TRUE ~ 0)) 13 14 ggplot(lmb_data, aes(lagdemvoteshare, score)) + 15 16 17 18 19 20\ngeom_point(aes(x = lagdemvoteshare, y = score), data = agg_lmb_data) + stat_smooth(aes(lagdemvoteshare, score, group = gg_group), method = \"lm\",\n(continued)\nR (continued)\ngeom_point(aes(x = lagdemvoteshare, y = score), data = agg_lmb_data) + stat_smooth(aes(lagdemvoteshare, score, group = gg_group), method = \"loess\") (cid:12)\u2192\ngeom_point(aes(x = lagdemvoteshare, y = score), data = agg_lmb_data) + stat_smooth(aes(lagdemvoteshare, score, group = gg_group), method = \"lm\") + xlim(0,1) + ylim(0,100) + geom_vline(xintercept = 0.5)\nFigure 38 shows the output from this program. Notice the sim- ilarities between what we produced here and what Lee et al. [2004] produced in their \ufb01gure. The only differences are subtle changes in the binning used for the two \ufb01gures.\nFigure 38. Using cmogram with quadratic \ufb01t and con\ufb01dence intervals. Reprinted from\nLee, D. S., Moretti, E., and Butler, M. J.", "start_char_idx": 16030, "end_char_idx": 19437, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fea6bc01-b9d3-4ea3-a067-b890b1418966": {"__data__": {"id_": "fea6bc01-b9d3-4ea3-a067-b890b1418966", "embedding": null, "metadata": {"page number": "314 - 341"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb1a07a4-6944-47b1-832e-5dff9fe341c4", "node_type": "4", "metadata": {"page number": "314 - 341"}, "hash": "3290a0b169dcd40ad1992e9a9432a980fba7e092425e59e964f2ddc137429db9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6684648b-0ac7-483d-9989-ddb0dbd51658", "node_type": "1", "metadata": {"page number": "314 - 341"}, "hash": "7f301f48a11e48d3a82db11a67b735231649466327d1b9cc8c28510987b8206e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "01a25409-b87f-4db3-bc38-2a4e4922b9d0", "node_type": "1", "metadata": {}, "hash": "8c5a8080a83a95e2d1a5175bb1f81c19fb0b9bced68c17ab72e3b63217134b10", "class_name": "RelatedNodeInfo"}}, "text": "Notice the sim- ilarities between what we produced here and what Lee et al. [2004] produced in their \ufb01gure. The only differences are subtle changes in the binning used for the two \ufb01gures.\nFigure 38. Using cmogram with quadratic \ufb01t and con\ufb01dence intervals. Reprinted from\nLee, D. S., Moretti, E., and Butler, M. J. (2004). \u201cDo Voters Affect or Elect Policies: Evidence from the U.S. House.\u201d Quarterly Journal of Economics, 119(3):807\u2013859.\nFigure 39. Using cmogram with linear \ufb01t. Reprinted from Lee, D. S., Moretti, E., and But-\nler, M. J. (2004). \u201cDo Voters Affect or Elect Policies: Evidence from the U.S. House.\u201d Quarterly Journal of Economics, 119(3):807\u2013859.\nWe have options other than a quadratic \ufb01t, though, and it\u2019s useful to compare this graph with one in which we only \ufb01t a linear model. Now, because there are strong trends in the running variable, we probably just want to use the quadratic, but let\u2019s see what we get when we use simpler straight lines.\nFigure 39 shows what we get when we only use a linear \ufb01t of the data left and right of the cutoff. Notice the in\ufb02uence that outliers far from the actual cutoff play in the estimate of the causal effect at the cutoff. Some of this would go away if we restricted the bandwidth to be shorter distances to and from the cutoff, but I leave it to you to do that.\nFinally, we can use a lowess \ufb01t. A lowess \ufb01t more or less crawls through the data and runs small regression on small cuts of data. This can give the \ufb01gure a zigzag appearance. We nonetheless show it in Figure 40.\nFigure 40. Using cmogram with lowess \ufb01t. Reprinted from Lee, D. S., Moretti, E., and But-\nler, M. J. (2004). \u201cDo Voters Affect or Elect Policies: Evidence from the U.S. House.\u201d Quarterly Journal of Economics, 119(3):807\u2013859.\nIf there don\u2019t appear to be any trends in the running variable, then the polynomials aren\u2019t going to buy you much. Some very good papers only report a linear \ufb01t because there weren\u2019t very strong trends to begin with. For instance, consider Carrell et al. [2011]. Those authors are interested in the causal effect of drinking on academic test outcomes for students at the Air Force Academy. Their running variable is the precise age of the student, which they have because they know the student\u2019s date of birth and they know the date of every exam taken at the Air Force Academy. Because the Air Force Academy restricts stu- dents\u2019 social life, there is a starker increase in drinking at age 21 on its campus than might be the case for a more a typical university campus. They examined the causal effect of drinking age on normalized grades using RDD, but because there weren\u2019t strong trends in the data, they presented a graph with only a linear \ufb01t. Your choice should be in large part based on what, to your eyeball, is the best \ufb01t of the data.\nHahn et al. [2001] have shown that one-sided kernel estimation such as lowess may suffer from poor properties because the point of interest is at the boundary (i.e., the discontinuity). This is called the \u201cboundary problem.\u201d They propose using local linear nonparametric regressions instead. In these regressions, more weight is given to the observations at the center.\nYou can also estimate kernel-weighted local polynomial regres- sions. Think of it as a weighted regression restricted to a window like we\u2019ve been doing (hence the word \u201clocal\u201d) where the chosen kernel pro- vides the weights. A rectangular kernel would give the same results as E[Y] at a given bin on X, but a triangular kernel would give more importance to observations closest to the center. This method will be sensitive to the size of the bandwidth chosen. But in that sense, it\u2019s similar to what we\u2019ve been doing. Figure 41 shows this visually.\n1 * Note kernel-weighted local polynomial regression is a smoothing method.", "start_char_idx": 19124, "end_char_idx": 22924, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "01a25409-b87f-4db3-bc38-2a4e4922b9d0": {"__data__": {"id_": "01a25409-b87f-4db3-bc38-2a4e4922b9d0", "embedding": null, "metadata": {"page number": "314 - 341"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb1a07a4-6944-47b1-832e-5dff9fe341c4", "node_type": "4", "metadata": {"page number": "314 - 341"}, "hash": "3290a0b169dcd40ad1992e9a9432a980fba7e092425e59e964f2ddc137429db9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fea6bc01-b9d3-4ea3-a067-b890b1418966", "node_type": "1", "metadata": {"page number": "314 - 341"}, "hash": "f2198865f6f499c092e98cf3634adc0bfd592918fbd2285197a11817a5427311", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dfc3fa80-0798-4afc-918a-8d253923e66d", "node_type": "1", "metadata": {}, "hash": "a6e45ff5ebaa2ef37f8e2bedab4f9302079b0be68dd09397bb116887aab3756b", "class_name": "RelatedNodeInfo"}}, "text": "This is called the \u201cboundary problem.\u201d They propose using local linear nonparametric regressions instead. In these regressions, more weight is given to the observations at the center.\nYou can also estimate kernel-weighted local polynomial regres- sions. Think of it as a weighted regression restricted to a window like we\u2019ve been doing (hence the word \u201clocal\u201d) where the chosen kernel pro- vides the weights. A rectangular kernel would give the same results as E[Y] at a given bin on X, but a triangular kernel would give more importance to observations closest to the center. This method will be sensitive to the size of the bandwidth chosen. But in that sense, it\u2019s similar to what we\u2019ve been doing. Figure 41 shows this visually.\n1 * Note kernel-weighted local polynomial regression is a smoothing method. 2 capture drop sdem* x1 x0 3 lpoly score demvoteshare if democrat == 0, nograph kernel(triangle) gen(x0\n5 scatter sdem1 x1, color(red) msize(small) || scatter sdem0 x0, msize(small)\ncolor(red) xline(0.5,lstyle(dot)) legend(off) xtitle(\"Democratic vote share\") ytitle(\"ADA score\")\nkernel = \"box\", bandwidth = 0.1))\n(continued)\nR (continued)\ngeom_smooth(aes(x, y), data = smooth_dem0) + geom_smooth(aes(x, y), data = smooth_dem1) + geom_vline(xintercept = 0.5)\nkernel = \"box\", bandwidth = 0.1))\nA couple of \ufb01nal things. First, recall the continuity assumption. Because the continuity assumption speci\ufb01cally involves continuous conditional expectation functions of the potential outcomes through- out the cutoff, it therefore is untestable. That\u2019s right\u2014it\u2019s an untestable assumption. But, what we can do is check for whether there are changes in the conditional expectation functions for other exogenous covariates that cannot or should not be changing as a result of the cut- off. So it\u2019s very common to look at things like race or gender around the cutoff. You can use these same methods to do that, but I do not do them here. Any RDD paper will always involve such placebos; even though they are not direct tests of the continuity assumption, they are indirect tests. Remember, when you are publishing, your readers aren\u2019t as familiar with this thing you\u2019re studying, so your task is explain to readers what you know. Anticipate their objections and the sources of their skepticism. Think like them. Try to put yourself in a stranger\u2019s shoes. And then test those skepticisms to the best of your ability.\nSecond, we saw the importance of bandwidth selection, or win- dow, for estimating the causal effect using this method, as well as the\nimportance of selection of polynomial length. There\u2019s always a trade- off when choosing the bandwidth between bias and variance\u2014the shorter the window, the lower the bias, but because you have less data, the variance in your estimate increases. Recent work has been focused on optimal bandwidth selection, such as Imbens and Kalyanaraman [2011] and Calonico et al. [2014]. The latter can be implemented with the user-created rdrobust command. These methods ultimately choose optimal bandwidths that may differ left and right of the cutoff based on some bias-variance trade-off. Let\u2019s repeat our analysis using this nonparametric method. The coe\ufb03cient is 46.48 with a standard error of 1.24.\n1 * Local polynomial point estimators with bias correction 2 ssc install rdrobust, replace 3 rdrobust score demvoteshare, c(0.5)\n1 library(tidyverse) 2 library(rdrobust) 3 4 rdr <- rdrobust(y = lmb_data$score, 5 6 summary(rdr)\nx = lmb_data$demvoteshare, c = 0.5)\nThis method, as we\u2019ve repeatedly said, is data-greedy because it gobbles up data at the discontinuity. So ideally these kinds of meth- ods will be used when you have large numbers of observations in the sample so that you have a sizable number of observations at the discontinuity. When that is the case, there should be some harmony in your \ufb01ndings across results. If there isn\u2019t, then you may not have su\ufb03cient power to pick up this effect.\nFinally, we look at the implementation of the McCrary density test.", "start_char_idx": 22116, "end_char_idx": 26128, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dfc3fa80-0798-4afc-918a-8d253923e66d": {"__data__": {"id_": "dfc3fa80-0798-4afc-918a-8d253923e66d", "embedding": null, "metadata": {"page number": "314 - 341"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb1a07a4-6944-47b1-832e-5dff9fe341c4", "node_type": "4", "metadata": {"page number": "314 - 341"}, "hash": "3290a0b169dcd40ad1992e9a9432a980fba7e092425e59e964f2ddc137429db9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "01a25409-b87f-4db3-bc38-2a4e4922b9d0", "node_type": "1", "metadata": {"page number": "314 - 341"}, "hash": "95c03fdedb6d3fe4589c9e04ecf230171932b42d2924cfa52a4596915fd8cedd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c2935f8-9dbd-44ed-bf7b-fd7cc811004d", "node_type": "1", "metadata": {}, "hash": "466254a726d1c40d422af88c51992bd410869b8d5375631ba5d47066d21f6087", "class_name": "RelatedNodeInfo"}}, "text": "So ideally these kinds of meth- ods will be used when you have large numbers of observations in the sample so that you have a sizable number of observations at the discontinuity. When that is the case, there should be some harmony in your \ufb01ndings across results. If there isn\u2019t, then you may not have su\ufb03cient power to pick up this effect.\nFinally, we look at the implementation of the McCrary density test. We will implement this test using local polynomial density estimation [Cattaneo et al., 2019]. This requires installing two \ufb01les in Stata. Visu- ally inspecting the graph in Figure 42, we see no signs that there was manipulation in the running variable at the cutoff.\nFigure 42. McCrary density test using local linear nonparametric regressions.\nfrom(https://sites.google.com/site/rdpackages/rddensity/stata) replace\nfrom(https://sites.google.com/site/nppackages/lpdensity/stata) replace\n1 library(tidyverse) 2 library(rddensity) 3 library(rdd) 4 5 DCdensity(lmb_data$demvoteshare, cutpoint = 0.5) 6 7 density <- rddensity(lmb_data$demvoteshare, c = 0.5) 8 rdplotdensity(density, lmb_data$demvoteshare)\nConcluding remarks about close-election designs. Let\u2019s circle back to the close-election design. The design has since become practically a cottage industry within economics and political science. It has been extended to other types of elections and outcomes. One paper I like a lot used close gubernatorial elections to examine the effect of Demo- cratic governors on the wage gap between workers of different races [Beland, 2015]. There are dozens more.\nBut a critique from Caughey and Sekhon [2011] called into question the validity of Lee\u2019s analysis on the House elections. They found that bare winners and bare losers in US House elections differed consid- erably on pretreatment covariates, which had not been formally evalu- ated by Lee et al. [2004]. And that covariate imbalance got even worse in the closest elections. Their conclusion is that the sorting problems got more severe, not less, in the closest of House races, suggesting that these races could not be used for an RDD.\nit appeared that this criticism by Caughey and Sekhon [2011] threw cold water on the entire close-election design, but\nwe since know that is not the case. It appears that the Caughey and Sekhon [2011] criticism may have been only relevant for a subset of House races but did not characterize other time periods or other types of races. Eggers et al. [2014] evaluated 40,000 close elections, includ- ing the House in other time periods, mayoral races, and other types of races for political o\ufb03ces in the US and nine other countries. No other case that they encountered exhibited the type of pattern described by Caughey and Sekhon [2011]. Eggers et al. (2014) conclude that the assumptions behind RDD in the close-election design are likely to be met in a wide variety of electoral settings and is perhaps one of the best RD designs we have going forward.\nMany times, the concept of a running variable shifting a unit into treatment and in turn causing a jump in some outcome is su\ufb03cient. But there are some instances in which the idea of a \u201cjump\u201d doesn\u2019t describe what happens. A couple of papers by David Card and coauthors have extended the regression discontinuity design in order to handle these different types of situations. The most notable is Card et al. [2015], which introduced a new method called regression kink design, or RKD. The intuition is rather simple. Rather than the cutoff causing a discon- tinuous jump in the treatment variable at the cutoff, it changes the \ufb01rst derivative, which is known as a kink. Kinks are often embedded in pol- icy rules, and thanks to Card et al. [2015], we can use kinks to identify the causal effect of a policy by exploiting the jump in the \ufb01rst derivative. Card et al.\u2019s [2015] paper applies the design to answer the question of whether the level of unemployment bene\ufb01ts affects the length of time spent unemployed in Austria. Unemployment bene\ufb01ts are based on income in a base period. There is then a minimum bene\ufb01t level that isn\u2019t binding for people with low earnings. Then bene\ufb01ts are 55% of the earnings in the base period.", "start_char_idx": 25721, "end_char_idx": 29902, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c2935f8-9dbd-44ed-bf7b-fd7cc811004d": {"__data__": {"id_": "3c2935f8-9dbd-44ed-bf7b-fd7cc811004d", "embedding": null, "metadata": {"page number": "314 - 341"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb1a07a4-6944-47b1-832e-5dff9fe341c4", "node_type": "4", "metadata": {"page number": "314 - 341"}, "hash": "3290a0b169dcd40ad1992e9a9432a980fba7e092425e59e964f2ddc137429db9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dfc3fa80-0798-4afc-918a-8d253923e66d", "node_type": "1", "metadata": {"page number": "314 - 341"}, "hash": "fc2ab6cb4fb49db3642865e17bc5af55cc062266ab2788c7936f7b897edbee35", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "56c18120-5518-4f31-bab7-50846be95689", "node_type": "1", "metadata": {}, "hash": "9dd34650ac16ac00c3df0f1b0adb59a0889caafe9e9da5257b0e62405f91bac6", "class_name": "RelatedNodeInfo"}}, "text": "The intuition is rather simple. Rather than the cutoff causing a discon- tinuous jump in the treatment variable at the cutoff, it changes the \ufb01rst derivative, which is known as a kink. Kinks are often embedded in pol- icy rules, and thanks to Card et al. [2015], we can use kinks to identify the causal effect of a policy by exploiting the jump in the \ufb01rst derivative. Card et al.\u2019s [2015] paper applies the design to answer the question of whether the level of unemployment bene\ufb01ts affects the length of time spent unemployed in Austria. Unemployment bene\ufb01ts are based on income in a base period. There is then a minimum bene\ufb01t level that isn\u2019t binding for people with low earnings. Then bene\ufb01ts are 55% of the earnings in the base period. There is a maximum bene\ufb01t level that is then adjusted every year, which creates a discontinuity in the schedule. Figure 43 shows the relationship between base earnings and unemployment bene\ufb01ts around the discontinuity. There\u2019s a visible kink in the empirical relationship between average bene\ufb01ts and base\nwe since know that is not the case. It appears that the Caughey and Sekhon [2011] criticism may have been only relevant for a subset of House races but did not characterize other time periods or other types of races. Eggers et al. [2014] evaluated 40,000 close elections, includ- ing the House in other time periods, mayoral races, and other types of races for political o\ufb03ces in the US and nine other countries. No other case that they encountered exhibited the type of pattern described by Caughey and Sekhon [2011]. Eggers et al. (2014) conclude that the assumptions behind RDD in the close-election design are likely to be met in a wide variety of electoral settings and is perhaps one of the best RD designs we have going forward.\nMany times, the concept of a running variable shifting a unit into treatment and in turn causing a jump in some outcome is su\ufb03cient. But there are some instances in which the idea of a \u201cjump\u201d doesn\u2019t describe what happens. A couple of papers by David Card and coauthors have extended the regression discontinuity design in order to handle these different types of situations. The most notable is Card et al. [2015], which introduced a new method called regression kink design, or RKD. The intuition is rather simple. Rather than the cutoff causing a discon- tinuous jump in the treatment variable at the cutoff, it changes the \ufb01rst derivative, which is known as a kink. Kinks are often embedded in pol- icy rules, and thanks to Card et al. [2015], we can use kinks to identify the causal effect of a policy by exploiting the jump in the \ufb01rst derivative. Card et al.\u2019s [2015] paper applies the design to answer the question of whether the level of unemployment bene\ufb01ts affects the length of time spent unemployed in Austria. Unemployment bene\ufb01ts are based on income in a base period. There is then a minimum bene\ufb01t level that isn\u2019t binding for people with low earnings. Then bene\ufb01ts are 55% of the earnings in the base period. There is a maximum bene\ufb01t level that is then adjusted every year, which creates a discontinuity in the schedule. Figure 43 shows the relationship between base earnings and unemployment bene\ufb01ts around the discontinuity. There\u2019s a visible kink in the empirical relationship between average bene\ufb01ts and base\n\u201cInference on Causal Effects in a Generalized Regression Kink Design.\u201d Econometrica, 84(6):2453\u20132483. Copyright \u00a9 2015 Wiley. Used with permis- sion from John Wiley and Sons.\nearnings. You can see this in the sharp decline in the slope of the func- tion as base-year earnings pass the threshold. Figure 44 presents a similar picture, but this time of unemployment duration. Again, there is a clear kink as base earnings pass the threshold. The authors con- clude that increases in unemployment bene\ufb01ts in the Austrian context exert relatively large effects on unemployment duration.\nThe regression discontinuity design is often considered a winning design because of its upside in credibly identifying causal effects. As with all designs, its credibility only comes from deep institutional knowledge, particularly surrounding the relationship between the run- ning variable, the cutoff, treatment assignment, and the outcomes themselves.", "start_char_idx": 29162, "end_char_idx": 33412, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "56c18120-5518-4f31-bab7-50846be95689": {"__data__": {"id_": "56c18120-5518-4f31-bab7-50846be95689", "embedding": null, "metadata": {"page number": "314 - 341"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb1a07a4-6944-47b1-832e-5dff9fe341c4", "node_type": "4", "metadata": {"page number": "314 - 341"}, "hash": "3290a0b169dcd40ad1992e9a9432a980fba7e092425e59e964f2ddc137429db9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c2935f8-9dbd-44ed-bf7b-fd7cc811004d", "node_type": "1", "metadata": {"page number": "314 - 341"}, "hash": "1525d2834c8d67d8406022788858e313b1edb9be0740e7a143d3794713616d8e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "205c3c80-2b85-4754-a117-aa5468378f88", "node_type": "1", "metadata": {}, "hash": "b4cf4b84e7c3a998147319ae43c61bfe5ae21f3bbcd6c3b6c11f1853ab814d0f", "class_name": "RelatedNodeInfo"}}, "text": "Copyright \u00a9 2015 Wiley. Used with permis- sion from John Wiley and Sons.\nearnings. You can see this in the sharp decline in the slope of the func- tion as base-year earnings pass the threshold. Figure 44 presents a similar picture, but this time of unemployment duration. Again, there is a clear kink as base earnings pass the threshold. The authors con- clude that increases in unemployment bene\ufb01ts in the Austrian context exert relatively large effects on unemployment duration.\nThe regression discontinuity design is often considered a winning design because of its upside in credibly identifying causal effects. As with all designs, its credibility only comes from deep institutional knowledge, particularly surrounding the relationship between the run- ning variable, the cutoff, treatment assignment, and the outcomes themselves. Insofar as one can easily \ufb01nd a situation in which a run- ning variable passing some threshold leads to units being siphoned off\n\u201cInference on Causal Effects in a Generalized Regression Kink Design.\u201d Econometrica, 84(6):2453\u20132483. Copyright \u00a9 2015 Wiley. Used with permis- sion from John Wiley and Sons.\nearnings. You can see this in the sharp decline in the slope of the func- tion as base-year earnings pass the threshold. Figure 44 presents a similar picture, but this time of unemployment duration. Again, there is a clear kink as base earnings pass the threshold. The authors con- clude that increases in unemployment bene\ufb01ts in the Austrian context exert relatively large effects on unemployment duration.\nThe regression discontinuity design is often considered a winning design because of its upside in credibly identifying causal effects. As with all designs, its credibility only comes from deep institutional knowledge, particularly surrounding the relationship between the run- ning variable, the cutoff, treatment assignment, and the outcomes themselves. Insofar as one can easily \ufb01nd a situation in which a run- ning variable passing some threshold leads to units being siphoned off\nWeber, A. (2015). \u201cInference on Causal Effects in a Generalized Regres- sion Kink Design.\u201d Econometrica, 84(6):2453\u20132483. Copyright \u00a9 2015 Wiley. Used with permission from John Wiley and Sons.\ninto some treatment, then if continuity is believable, you\u2019re probably sit- ting on a great opportunity, assuming you can use it to do something theoretically interesting and policy relevant to others.\nRegression discontinuity design opportunities abound, particu- larly within \ufb01rms and government agencies, for no other reason than that these organizations face scarcity problems and must use some method to ration a treatment. Randomization is a fair way to do it, and that is often the method used. But a running variable is another method. Routinely, organizations will simply use a continuous score to assign treatments by arbitrarily picking a cutoff above which everyone receives the treatment. Finding these can yield a cheap yet powerfully informative natural experiment. This chapter attempted to lay out the basics of the design. But the area continues to grow at a lightning pace. So I encourage you to see this chapter as a starting point, not an ending point.", "start_char_idx": 32577, "end_char_idx": 35775, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "205c3c80-2b85-4754-a117-aa5468378f88": {"__data__": {"id_": "205c3c80-2b85-4754-a117-aa5468378f88", "embedding": null, "metadata": {"page number": "342 - 342", "chapter": "Instrumental Variables"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9783acf8-1965-4724-a3ef-0ea2dc1d39ea", "node_type": "4", "metadata": {"page number": "342 - 342", "chapter": "Instrumental Variables"}, "hash": "4210ba07fb7de748ba74cf2e8eacd276e21c8dbceadffaff39a737edafbb5777", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "56c18120-5518-4f31-bab7-50846be95689", "node_type": "1", "metadata": {"page number": "314 - 341"}, "hash": "4540adde94ccf2f1c2499e8c50c8fdfbbd6f61e4937a5a739bac54dda8fc475d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a66ee751-b150-4ace-80c6-f6287df9df64", "node_type": "1", "metadata": {}, "hash": "6edb7445eb1ee58c1314352d4423beb31e0cd8b42c816b1677d40dfd73431d3c", "class_name": "RelatedNodeInfo"}}, "text": "Instrumental Variables:\nI made \u201cSunday Candy,\u201d I\u2019m never going to hell. I met Kanye West, I\u2019m never going to fail.\nJust as Archimedes said, \u201cGive me a fulcrum, and I shall move the world,\u201d you could just as easily say that with a good-enough instru- ment, you can identify any causal effect. But, while that is hyper- bole, for reasons we will soon see, it is nonetheless the case that the instrumental variables (IV) design is potentially one of most impor- tant research designs ever devised. It is also unique because it is one of those instances that the econometric estimator was not simply ripped off from statistics (e.g., Eicker-Huber-White standard errors) or imported from some other \ufb01eld (e.g., like regression discontinuity). IV was invented by an economist, and its history is fascinating.", "start_char_idx": 0, "end_char_idx": 802, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a66ee751-b150-4ace-80c6-f6287df9df64": {"__data__": {"id_": "a66ee751-b150-4ace-80c6-f6287df9df64", "embedding": null, "metadata": {"page number": "342 - 346", "chapter": "Instrumental Variables"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3e25f122-2f91-48e3-90a1-3c135ee4e1e3", "node_type": "4", "metadata": {"page number": "342 - 346", "chapter": "Instrumental Variables"}, "hash": "ff210a997d28215f03997ae95011adffeea63a2e02fb96b8b5c79efe37bff70c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "205c3c80-2b85-4754-a117-aa5468378f88", "node_type": "1", "metadata": {"page number": "342 - 342", "chapter": "Instrumental Variables"}, "hash": "292e62c92c76664f782d6fda496a9adada4f64f4b72e97cd0bf8791d64cb72d5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "65e5b3b0-ff03-492a-ab0e-1cc4c07e7900", "node_type": "1", "metadata": {}, "hash": "e21aac5b5f87fd6709e573c8ab8c3e29b57770daee02ddbe01e8e5db4ac73482", "class_name": "RelatedNodeInfo"}}, "text": "History of Instrumental Variables: Father and Son:\nPhilip Wright was born in 1861 and died in 1934. He received his bachelor\u2019s degree from Tufts in 1884 and a master\u2019s degree from Har- vard in 1887. His son, Sewall Wright, was born in 1889 when Philip was 28. The family moved from Massachusetts to Galesburg, Illinois, where Philip took a position as professor of mathematics and eco- nomics at Lombard College. Philip published numerous articles and books in economics over his career, and he published poetry, too. You can see his vita here at https://scholar.harvard.edu/\ufb01les/stock/\ufb01les/\nwright_cv.jpg.1 Sewall attended Lombard College and took his college mathematics courses from his father.\nIn 1913, Philip took a position at Harvard, and Sewall entered there as a graduate student. Philip would later leave for the Brookings Insti- tute, and Sewall would take his \ufb01rst job in the Department of Zoology at the University of Chicago, where he would eventually be promoted to professor in 1930.\nPhilip was proli\ufb01c, which, given his sizable teaching and service requirements, is impressive. He published in top journals such as Quar- terly Journal of Economics, Journal of the American Statistical Associ- ation, Journal of Political Economy, andAmerican Economic Review . A common theme across many of his publications was the identi\ufb01cation problem. He was acutely aware of it and intent on solving it.\nIn 1928, Philip was writing a book about animal and vegetable oils. The reason? He believed that recent tariff increases were harming international relations. And he wrote about the damage from the tar- iffs, which had affected animal and vegetable oils. The book, it turns out, would become a classic\u2014not for tariffs or oils, but for being the \ufb01rst proof for the existence of an instrumental variables estimator.\nWhile his father was publishing like a \ufb01end in economics, Sewall Wright was revolutionizing the \ufb01eld of genetics. He invented path anal- ysis, a precursor to Pearl\u2019s directed acyclical graphic models, and he made important contributions to the theory of evolution and genetics. He was a genius. The decision to not follow in the family business (eco- nomics) created a bit of tension between the two men, but all evidence suggests that they found each other intellectually stimulating.\nIn his book on vegetable and oil tariffs, there is an appendix (enti- tled Appendix B) in which the calculus of an instrumental variables estimator was worked out. Elsewhere, Philip thanked his son for his valuable contributions to what he had written, referring to the path anal- ysis that Sewall had taught him. This path analysis, it turned out, played a key role in Appendix B.\nAppendix B showed a solution to the identi\ufb01cation problem. So long as the economist is willing to impose some restrictions on the\n1 Philip had a passion for poetry, and even published some in his life. He also used\nhis school\u2019s printing press to publish the \ufb01rst book of poems by the great American poet\nproblem, then the system of equations can be identi\ufb01ed. Speci\ufb01cally, if there is one instrument for supply, and the supply and demand errors are uncorrelated, then the elasticity of demand can be identi\ufb01ed.\nBut who wrote this Appendix B? Either man could\u2019ve done so. It is a chapter in an economics book, which points to Philip. But it used the path analysis, which points to Sewall. Historians have debated this, even going so far as to accuse Philip of stealing the idea from his son. If Philip stole the idea, by which I mean that when he published Appendix B, he failed to give proper attribution to his son, then it would at the very least have been a strange oversight. In come Stock and Trebbi [2003] to offer their opinions to this debate over authorship.\nStock and Trebbi [2003] tried to determine the authorship of Appendix B using \u201cstylometric analysis.\u201d Stylometric analysis had been used in other applications, such as to identify the author of the 1996 political novel Primary Colors (Joseph Klein) and the unsigned Feder- alist Papers. But Stock and Trebbi [2003] is easily the best application of stylometric analysis in economics.2\nThe method is akin to contemporary machine learning methods. The authors collected raw data containing the known original aca- demic writings of each man, plus the \ufb01rst chapter and Appendix B of the book in question.", "start_char_idx": 0, "end_char_idx": 4359, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "65e5b3b0-ff03-492a-ab0e-1cc4c07e7900": {"__data__": {"id_": "65e5b3b0-ff03-492a-ab0e-1cc4c07e7900", "embedding": null, "metadata": {"page number": "342 - 346", "chapter": "Instrumental Variables"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3e25f122-2f91-48e3-90a1-3c135ee4e1e3", "node_type": "4", "metadata": {"page number": "342 - 346", "chapter": "Instrumental Variables"}, "hash": "ff210a997d28215f03997ae95011adffeea63a2e02fb96b8b5c79efe37bff70c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a66ee751-b150-4ace-80c6-f6287df9df64", "node_type": "1", "metadata": {"page number": "342 - 346", "chapter": "Instrumental Variables"}, "hash": "668014a187f6842eddaeb3e92f54e3dfac6a0b27f34cabb82765f77d5db65fd2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "75563db9-8b98-48e8-ad0c-21d6923b93ef", "node_type": "1", "metadata": {}, "hash": "162cc02275ab43612791c15921341d5076ea755dc847740f3ea76da500fb8fba", "class_name": "RelatedNodeInfo"}}, "text": "If Philip stole the idea, by which I mean that when he published Appendix B, he failed to give proper attribution to his son, then it would at the very least have been a strange oversight. In come Stock and Trebbi [2003] to offer their opinions to this debate over authorship.\nStock and Trebbi [2003] tried to determine the authorship of Appendix B using \u201cstylometric analysis.\u201d Stylometric analysis had been used in other applications, such as to identify the author of the 1996 political novel Primary Colors (Joseph Klein) and the unsigned Feder- alist Papers. But Stock and Trebbi [2003] is easily the best application of stylometric analysis in economics.2\nThe method is akin to contemporary machine learning methods. The authors collected raw data containing the known original aca- demic writings of each man, plus the \ufb01rst chapter and Appendix B of the book in question. All footnotes, graphs, and \ufb01gures were excluded. Blocks of 1,000 words were selected from the \ufb01les. Fifty-four blocks were selected: twenty written by Sewall with certainty, twenty-\ufb01ve by Philip, six from Appendix B, and three from chapter 1. Chapter 1 has always been attributed to Philip, but Stock and Trebbi [2003] treat the three blocks as unknown to check whether their model is correctly predicting authorship when authorship is already known.\nThe stylometric indicators that they used included the frequency of occurrence in each block of 70 function words. The list was taken from a separate study. These 70 function words produced 70 numeri- cal variables, each of which is a count, per 1,000 words, of an individual function word in the block. Some words were dropped (e.g., \u201cthings\u201d because they occurred only once), leaving 69 function words.\n2 But it\u2019s easy to have the best paper on a topic in some \ufb01eld when you\u2019re also the\nThe second set of stylometric indicators, taken from another study, concerned grammatical constructions. Stock and Trebbi [2003] used 18 grammatical constructions, which were frequency counts. They included things like noun followed by an adverb, total occurrences of prepositions, coordinating conjunction followed by noun, and so on. There was one dependent variable in their analysis, and that was authorship. The independent variables were 87 covariates (69 function word counts and 18 grammatical statistics).\nThe results of this analysis are absolutely fascinating. For instance, many covariates have very large t-statistics, which would be unlikely if there really were no stylistic differences between the authors and the indicators were independently distributed.\nSo what do they \ufb01nd? Most interesting is their regression analysis.\nThey write:\nWe regressed authorship against an intercept, the \ufb01rst two prin- cipal components of the grammatical statistics and the \ufb01rst two principal components of the function word counts, and we attribute authorship depending on whether the predicted value is greater or less than 0.5. [191]\nAnd what did they \ufb01nd? That all of the Appendix B and chapter 1 blocks were assigned to Philip, not Sewall. They did other robustness checks, and all of them still pointed to Philip as the author.\nWriting Appendix B and solving the problem that became Appendix B are technically distinct. But I nonetheless love this story for many rea- sons. First, I love the idea that an econometric estimator as important as instrumental variables has its roots in economics. I\u2019m so accus- tomed to stories in which the actual econometric estimator was lifted from statistics (Huber-White standard errors) or educational psychol- ogy (regression discontinuity) that it\u2019s nice to know economists have added their own designs to the canon. But the other part of the story that I love is the father-son component. It\u2019s encouraging to know that a father and son can overcome differences through intellectual collabo- rations such as this. Such relationships are important, and tensions, when they arise, should be vigorously pursued until those tensions dissolve if possible. Relationships, and love more generally, matter after all. And Philip and Sewall give a story of that.", "start_char_idx": 3481, "end_char_idx": 7596, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "75563db9-8b98-48e8-ad0c-21d6923b93ef": {"__data__": {"id_": "75563db9-8b98-48e8-ad0c-21d6923b93ef", "embedding": null, "metadata": {"page number": "346 - 350", "chapter": "Instrumental Variables"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8d3cc16e-42ef-4617-baf2-aabf5d33ca06", "node_type": "4", "metadata": {"page number": "346 - 350", "chapter": "Instrumental Variables"}, "hash": "fe1d5e7d62bcdd9af0f143d07acf7bbaf888eb4ac8cc2187506389f5d3d1b55e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "65e5b3b0-ff03-492a-ab0e-1cc4c07e7900", "node_type": "1", "metadata": {"page number": "342 - 346", "chapter": "Instrumental Variables"}, "hash": "b503c7b87cf503273c31fbfe5a575e2aeddfad35858cf82bdb0d2eae5c34e1ec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "25a42784-36aa-4ec9-801c-b721ddce1a8e", "node_type": "1", "metadata": {}, "hash": "982220055612be35d478ee9cc1b4cdc60f48cf1120c351bc53b7120bea8db40f", "class_name": "RelatedNodeInfo"}}, "text": "Intuition of Instrumental Variables:\nCanonical IV DAG. To understand the instrumental variables estimator, it is helpful to start with a DAG that shows a chain of causal effects that contains all the information needed to understand the instrumen- tal variables strategy. First, notice the backdoor path between D and Y: D \u2190 U \u2192 Y. Furthermore, note that U is unobserved by the econo- metrician, which causes the backdoor path to remain open. If we have this kind of selection on unobservables, then there does not exist a con- ditioning strategy that will satisfy the backdoor criterion (in our data). But, before we throw up our arms, let\u2019s look at how Z operates through these pathways.\nFirst, there is a mediated pathway from Z to Y via D. When Z varies, D varies, which causes Y to change. But, even though Y is varying when Z varies, notice that Y is only varying because D has varied. You some- times hear people describe this as the \u201conly through\u201d assumption. That is, Z affects Y \u201conly through\u201d D.\nImagine this for a moment though. Imagine D consists of people making choices. Sometimes these choices affect Y, and sometimes these choices are merely correlated with changes in Y due to unob- served changes in U. But along comes some shock, Z, which induces some but not all of the people in D to make different decisions. What will happen?\nWell, for one, when those people\u2019s decisions change, Y will change too, because of the causal effect. But all of the correlation between D and Y in that situation will re\ufb02ect the causal effect. The reason is that D is a collider along the backdoor path between Z and Y.\nBut I\u2019m not done with this metaphor. Let\u2019s assume that in this D variable, with all these people, only some of the people change their behavior because of D. What then? Well, in that situation, Z is causing a change in Y for just a subset of the population. If the instrument only changes the behavior of women, for instance, then the causal effect of D on Y will only re\ufb02ect the causal effect of women\u2019s choices, not men\u2019s choices.\nThere are two ideas inherent in the previous paragraph that I want to emphasize. First, if there are heterogeneous treatment effects (e.g., men affect Y differently than women do), then our Z shock only identi- \ufb01ed some of the causal effect of D on Y. And that piece of the causal effect may only be valid for the population of women whose behavior changed in response to Z; it may not be re\ufb02ective of how men\u2019s behav- ior would affect Y. And second, if Z is inducing some of the change in Y via only a fraction of the change in D, then it\u2019s almost as though we have less data to identify that causal effect than we really have.\nHere we see two of the di\ufb03culties in interpreting instrumental vari- ables and identifying a parameter using instrumental variables. Instru- mental variables only identify a causal effect for any group of units whose behaviors are changed as a result of the instrument. We call this the causal effect of the complier population; in our example, only women \u201ccomplied\u201d with the instrument, so we only know its effect for them. And second, instrumental variables are typically going to have larger standard errors, and as such, they will fail to reject in many instances if for no other reason than being underpowered.\nMoving along, let\u2019s return to the DAG. Notice that we drew the DAG such that Z is independent of U. You can see this because D is a collider along the Z \u2192 D \u2190 U path, which implies that Z and U are independent. This is called the \u201cexclusion restriction,\u201d which we will discuss in more detail later. But brie\ufb02y, the IV estimator assumes that Z is independent of the variables that determine Y except for D.\nSecond, Z is correlated with D, and because of its correlation with D (and D\u2019s effect on Y), Z is correlated with Y but only through its effect on D. This relationship between Z and D is called the \u201c\ufb01rst stage\u201d because of the two-stage least squares estimator, which is a kind of IV esti- mator. The reason it is only correlated with Y via D is because D is a collider along the path Z \u2192 D \u2190 U \u2192 Y.\nGood instruments should feel weird. How do you know when you have a good instrument? One, it will require prior knowledge. I\u2019d encour- age you to write down that prior knowledge into a DAG and use it to re\ufb02ect on the feasibility of your design.", "start_char_idx": 0, "end_char_idx": 4347, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "25a42784-36aa-4ec9-801c-b721ddce1a8e": {"__data__": {"id_": "25a42784-36aa-4ec9-801c-b721ddce1a8e", "embedding": null, "metadata": {"page number": "346 - 350", "chapter": "Instrumental Variables"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8d3cc16e-42ef-4617-baf2-aabf5d33ca06", "node_type": "4", "metadata": {"page number": "346 - 350", "chapter": "Instrumental Variables"}, "hash": "fe1d5e7d62bcdd9af0f143d07acf7bbaf888eb4ac8cc2187506389f5d3d1b55e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "75563db9-8b98-48e8-ad0c-21d6923b93ef", "node_type": "1", "metadata": {"page number": "346 - 350", "chapter": "Instrumental Variables"}, "hash": "bf6f133c1ae14eee1ef543870fb05b05cd6b1b10a21518160352ba6a8950f691", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "30e4dcd2-e875-4d4d-9e07-80b4a9dd61c6", "node_type": "1", "metadata": {}, "hash": "253337695b072b3842032b603808b2fdcf614001eb041ef5f06f4366f73eb0ca", "class_name": "RelatedNodeInfo"}}, "text": "But brie\ufb02y, the IV estimator assumes that Z is independent of the variables that determine Y except for D.\nSecond, Z is correlated with D, and because of its correlation with D (and D\u2019s effect on Y), Z is correlated with Y but only through its effect on D. This relationship between Z and D is called the \u201c\ufb01rst stage\u201d because of the two-stage least squares estimator, which is a kind of IV esti- mator. The reason it is only correlated with Y via D is because D is a collider along the path Z \u2192 D \u2190 U \u2192 Y.\nGood instruments should feel weird. How do you know when you have a good instrument? One, it will require prior knowledge. I\u2019d encour- age you to write down that prior knowledge into a DAG and use it to re\ufb02ect on the feasibility of your design. As a starting point, you can\ncontemplate identifying a causal effect using IV only if you can theo- retically and logically defend the exclusion restriction, since the exclu- sion restriction is an untestable assumption. That defense requires theory, and since some people aren\u2019t comfortable with theoretical argu- ments like that, they tend to eschew the use of IV. More and more applied microeconomists are skeptical of IV because they are able to tell limitless stories in which exclusion restrictions do not hold.\nBut, let\u2019s say you think you do have a good instrument. How might you defend it as such to someone else? A necessary but not su\ufb03- cient condition for having an instrument that can satisfy the exclusion restriction is if people are confused when you tell them about the instrument\u2019s relationship to the outcome. Let me explain. No one is likely to be confused when you tell them that you think family size will reduce the labor supply of women. They don\u2019t need a Becker model to convince them that women who have more children probably are employed outside the home less often than those with fewer children. But what would they think if you told them that mothers whose \ufb01rst two children were the same gender were employed outside the home less than those whose two children had a balanced sex ratio? They would probably be confused because, after all, what does the gender composition of one\u2019s \ufb01rst two children have to do with whether a woman works outside the home? That\u2019s a head scratcher. They\u2019re confused because, logically, whether the \ufb01rst two kids are the same gender versus not the same gender doesn\u2019t seem on its face to change the incentives a women has to work outside the home, which is based on reservation wages and market wages. And yet, empirically it is true that if your \ufb01rst two children are a boy, many families will have a third compared to those who had a boy and a girl \ufb01rst. So what gives?\nThe gender composition of the \ufb01rst two children matters for a fam- ily if they have preferences over diversity of gender. Families where the \ufb01rst two children were boys are more likely to try again in the hopes they\u2019ll have a girl. And the same for two girls. Insofar as parents would like to have at least one boy and one girl, then having two boys might cause them to roll the dice for a girl.\nAnd there you see the characteristics of a good instrument. It\u2019s weird to a lay person because a good instrument (two boys) only changes the outcome by \ufb01rst changing some endogenous treatment\nvariable (family size) thus allowing us to identify the causal effect of family size on some outcome (labor supply). And so without knowl- edge of the endogenous variable, relationships between the instru- ment and the outcome don\u2019t make much sense. Why? Because the instrument is irrelevant to the determinants of the outcome except for its effect on the endogenous treatment variable. You also see another quality of the instrument that we like, which is that it\u2019s quasi-random. Before moving along, I\u2019d like to illustrate this \u201cweird instrument\u201d in one more way, using two of my favorite artists: Chance the Rap- per and Kanye West. At the start of this chapter, I posted a line from Kanye West\u2019s wonderful song \u201cUltralight Beam\u201d on the underrated Life of Pablo. On that song, Chance the Rapper sings:\nI made \u201cSunday Candy,\u201d I\u2019m never going to hell. I met Kanye West, I\u2019m never going to fail.\nSeveral years before \u201cUltralight Beam,\u201d Chance made a song called \u201cSunday Candy.\u201d It\u2019s a great song and I encourage you to listen to it.", "start_char_idx": 3597, "end_char_idx": 7902, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "30e4dcd2-e875-4d4d-9e07-80b4a9dd61c6": {"__data__": {"id_": "30e4dcd2-e875-4d4d-9e07-80b4a9dd61c6", "embedding": null, "metadata": {"page number": "346 - 350", "chapter": "Instrumental Variables"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8d3cc16e-42ef-4617-baf2-aabf5d33ca06", "node_type": "4", "metadata": {"page number": "346 - 350", "chapter": "Instrumental Variables"}, "hash": "fe1d5e7d62bcdd9af0f143d07acf7bbaf888eb4ac8cc2187506389f5d3d1b55e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "25a42784-36aa-4ec9-801c-b721ddce1a8e", "node_type": "1", "metadata": {"page number": "346 - 350", "chapter": "Instrumental Variables"}, "hash": "8375143a580e37733d1616369ccfdd01231c83d8ab91eb05ffc3dc14c69c7ae8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c45a8a02-5a40-4887-bc3b-2ec373152505", "node_type": "1", "metadata": {}, "hash": "550366573d8ab44e3137a0f83bba47f11a4739a1e31859997823d0f7f3c2a33a", "class_name": "RelatedNodeInfo"}}, "text": "Why? Because the instrument is irrelevant to the determinants of the outcome except for its effect on the endogenous treatment variable. You also see another quality of the instrument that we like, which is that it\u2019s quasi-random. Before moving along, I\u2019d like to illustrate this \u201cweird instrument\u201d in one more way, using two of my favorite artists: Chance the Rap- per and Kanye West. At the start of this chapter, I posted a line from Kanye West\u2019s wonderful song \u201cUltralight Beam\u201d on the underrated Life of Pablo. On that song, Chance the Rapper sings:\nI made \u201cSunday Candy,\u201d I\u2019m never going to hell. I met Kanye West, I\u2019m never going to fail.\nSeveral years before \u201cUltralight Beam,\u201d Chance made a song called \u201cSunday Candy.\u201d It\u2019s a great song and I encourage you to listen to it. But Chance makes a strange argument here on \u201cUltralight Beam.\u201d He claims that because he made \u201cSunday Candy,\u201d therefore he won\u2019t go to hell. Now even a religious person will \ufb01nd that perplexing, as there is nothing in Christian theology of eternal damnation that would link mak- ing a song to the afterlife. This, I would argue, is a \u201cweird instrument\u201d because without knowing the endogenous variable on the mediated path SC \u2192? \u2192 H, the two phenomena don\u2019t seem to go together.\nBut let\u2019s say that I told you that after Chance made \u201cSunday Candy,\u201d he got a phone call from his old preacher. The preacher loved the song and invited Chance to come sing it at church. And while revisiting his childhood church, Chance had a religious experience that caused him to convert back to Christianity. Now, and only now, does his statement make sense. It isn\u2019t that \u201cSunday Candy\u201d itself shaped the path of his afterlife, so much as \u201cSunday Candy\u201d caused a particular event that itself caused his beliefs about the future to change. That the line makes a weird argument is what makes \u201cSunday Candy\u201d a good instrument. But let\u2019s take the second line\u2014\u201cI met Kanye West, I\u2019m never going to fail.\u201d Unlike the \ufb01rst line, this is likely not a good instrument. Why? Because I don\u2019t even need to know what variable is along the mediated\npath KW \u2192? \u2192 F to doubt the exclusion restriction. If you are a musi- cian, a relationship with Kanye West can possibly make or break your career. Kanye could make your career by collaborating with you on a song or by introducing you to highly talented producers. There is no shortage of ways in which a relationship with Kanye West can cause you to be successful, regardless of whatever unknown endogenous variable we have placed in this mediated path. And since it\u2019s easy to tell a story where knowing Kanye West directly causes one\u2019s success, knowing Kanye West is likely a bad instrument. It simply won\u2019t satisfy the exclusion restriction in this context.\nUltimately, good instruments are jarring precisely because of the exclusion restriction\u2014these two things (gender composition and work) don\u2019t seem to go together. If they did go together, it would likely mean that the exclusion restriction was violated. But if they don\u2019t, then the person is confused, and that is at minimum a possible candidate for a good instrument. This is the commonsense explanation of the \u201conly through\u201d assumption.", "start_char_idx": 7120, "end_char_idx": 10317, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c45a8a02-5a40-4887-bc3b-2ec373152505": {"__data__": {"id_": "c45a8a02-5a40-4887-bc3b-2ec373152505", "embedding": null, "metadata": {"page number": "350 - 357", "chapter": "Instrumental Variables"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c9b5eba0-8b14-4697-88d6-c8ce7cd182a9", "node_type": "4", "metadata": {"page number": "350 - 357", "chapter": "Instrumental Variables"}, "hash": "aa5b2bbbce9d1e70425880735d6aa53163f953f080c9ebcaa8492c3ad1c23c55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "30e4dcd2-e875-4d4d-9e07-80b4a9dd61c6", "node_type": "1", "metadata": {"page number": "346 - 350", "chapter": "Instrumental Variables"}, "hash": "099e1c0887717fac00e4f2f9e0e8248a1c7cfbda19f5ee809efeddc99f072dba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a9990d1-6649-4b31-ad07-fcbb9bbafb61", "node_type": "1", "metadata": {}, "hash": "af3dd58c63a51197e617af3c8d42dcc31d298e214a8caa19d73e2545934ee737", "class_name": "RelatedNodeInfo"}}, "text": "Homogeneous Treatment Effects:\nThere are two ways to discuss the instrumental variables design: one in a world where the treatment has the same causal effect for everybody (\u201chomogeneous treatment effects\u201d) and one in a world where the treatment effects can differ across the population (\u201chetero- geneous treatment effects\u201d). For homogeneous treatment effects, I will depend on a more traditional approach rather than on potential outcomes notation. When the treatment effect is constant, I don\u2019t feel we need potential outcomes notation as much.\nInstrumental variables methods are typically used to address omit- ted variable bias, measurement error, and simultaneity. For instance, quantity and price is determined by the intersection of supply and demand, so any observational correlation between price and quan- tity is uninformative about the elasticities associated with supply or demand curves. Philip Wright understood this, which was why he investigated the problem so intensely.\nI will assume a homogeneous treatment effect of \u03b4 which is the same for every person. This means that if college caused my wages\npath KW \u2192? \u2192 F to doubt the exclusion restriction. If you are a musi- cian, a relationship with Kanye West can possibly make or break your career. Kanye could make your career by collaborating with you on a song or by introducing you to highly talented producers. There is no shortage of ways in which a relationship with Kanye West can cause you to be successful, regardless of whatever unknown endogenous variable we have placed in this mediated path. And since it\u2019s easy to tell a story where knowing Kanye West directly causes one\u2019s success, knowing Kanye West is likely a bad instrument. It simply won\u2019t satisfy the exclusion restriction in this context.\nUltimately, good instruments are jarring precisely because of the exclusion restriction\u2014these two things (gender composition and work) don\u2019t seem to go together. If they did go together, it would likely mean that the exclusion restriction was violated. But if they don\u2019t, then the person is confused, and that is at minimum a possible candidate for a good instrument. This is the commonsense explanation of the \u201conly through\u201d assumption.\nThere are two ways to discuss the instrumental variables design: one in a world where the treatment has the same causal effect for everybody (\u201chomogeneous treatment effects\u201d) and one in a world where the treatment effects can differ across the population (\u201chetero- geneous treatment effects\u201d). For homogeneous treatment effects, I will depend on a more traditional approach rather than on potential outcomes notation. When the treatment effect is constant, I don\u2019t feel we need potential outcomes notation as much.\nInstrumental variables methods are typically used to address omit- ted variable bias, measurement error, and simultaneity. For instance, quantity and price is determined by the intersection of supply and demand, so any observational correlation between price and quan- tity is uninformative about the elasticities associated with supply or demand curves. Philip Wright understood this, which was why he investigated the problem so intensely.\nI will assume a homogeneous treatment effect of \u03b4 which is the same for every person. This means that if college caused my wages\nto increase by 10%, it also caused your wages to increase by 10%. Let\u2019s start by illustrating the problem of omitted variable bias. Assume the classical labor problem where we\u2019re interested in the causal effect of schooling on earnings, but schooling is endogenous because of unobserved ability. Let\u2019s draw a simple DAG to illustrate this setup.\nWe can represent this DAG with a simple regression. Let the true\nmodel of earnings be:\nwhere Y is the log of earnings, S is schooling measured in years, A is individual \u201cability,\u201d and \u03b5 is an error term uncorrelated with schooling or ability. The reason A is unobserved is simply because the surveyor either forgot to collect it or couldn\u2019t collect it and therefore it\u2019s missing from the data set.3 For instance, the CPS tells us nothing about respon- dents\u2019 family background, intelligence, motivation, or non-cognitive ability. Therefore, since ability is unobserved, we have the following equation instead:\nwhere \u03b7i is a composite error term equalling \u03b3 Ai + \u03b5i. We assume that schooling is correlated with ability, so therefore it is correlated with \u03b7i, making it endogenous in the second, shorter regression. Only \u03b5i is uncorrelated with the regressors, and that is by de\ufb01nition.\nestimated value of (cid:12)\u03b4 is:\nWe know from the derivation of the least squares operator that the\n3 Unobserved ability doesn\u2019t mean it\u2019s literally unobserved, in other words. It could\nbe just missing from your data set, and therefore is unobserved to you.", "start_char_idx": 0, "end_char_idx": 4789, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6a9990d1-6649-4b31-ad07-fcbb9bbafb61": {"__data__": {"id_": "6a9990d1-6649-4b31-ad07-fcbb9bbafb61", "embedding": null, "metadata": {"page number": "350 - 357", "chapter": "Instrumental Variables"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c9b5eba0-8b14-4697-88d6-c8ce7cd182a9", "node_type": "4", "metadata": {"page number": "350 - 357", "chapter": "Instrumental Variables"}, "hash": "aa5b2bbbce9d1e70425880735d6aa53163f953f080c9ebcaa8492c3ad1c23c55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c45a8a02-5a40-4887-bc3b-2ec373152505", "node_type": "1", "metadata": {"page number": "350 - 357", "chapter": "Instrumental Variables"}, "hash": "3cdd195c3dbad527d253eb6c9907abe994831e4b2977843fb858a0439ad41bad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c117c0d8-2ae9-49f9-aa53-4f7bfb83c2da", "node_type": "1", "metadata": {}, "hash": "c3c8c1a22efe33dc591e406ba8980e2751ab9c881492191910b59c7b0fda4bdc", "class_name": "RelatedNodeInfo"}}, "text": "Therefore, since ability is unobserved, we have the following equation instead:\nwhere \u03b7i is a composite error term equalling \u03b3 Ai + \u03b5i. We assume that schooling is correlated with ability, so therefore it is correlated with \u03b7i, making it endogenous in the second, shorter regression. Only \u03b5i is uncorrelated with the regressors, and that is by de\ufb01nition.\nestimated value of (cid:12)\u03b4 is:\nWe know from the derivation of the least squares operator that the\n3 Unobserved ability doesn\u2019t mean it\u2019s literally unobserved, in other words. It could\nbe just missing from your data set, and therefore is unobserved to you.\nPlugging in the true value of Y (from the longer model), we get the following:\n(cid:12)\u03b4 = E[\u03b1S + S2\u03b4 + \u03b3 SA + \u03b5S] \u2212 E(S)E[\u03b1 + \u03b4S + \u03b3 A + \u03b5]\nIf \u03b3 > 0 andC(A, S) >0, then (cid:12)\u03b4, the coe\ufb03cient on schooling, is upward biased. And that is probably the case given that it\u2019s likely that ability and schooling are positively correlated.\nBut let\u2019s assume that you have found a really great weird instru- ment Zi that causes people to get more schooling but that is indepen- dent of student ability and the structural error term. It is independent of ability, which means we can get around the endogeneity problem. And it\u2019s not associated with the other unobserved determinants of earn- ings, which basically makes it weird. The DAG associated with this set up would look like this:\nWe can use this variable, as I\u2019ll now show, to estimate \u03b4. First,\ncalculate the covariance of Y and Z:\nNotice that the parameter of interest, \u03b4 is on the right side. So how do we isolate it? We can estimate it with the following:\nso long as C(A, Z) = 0 and C(\u03b5, Z) = 0.\nThese zero covariances are the statistical truth contained in the IV DAG from earlier. If ability is independent of Z, then this second covari- ance is zero. And if Z is independent of the structural error term, \u03b5, then it too is zero. This, you see, is what is meant by the \u201cexclusion restriction\u201d: the instrument must be independent of both parts of the composite error term.\nBut the exclusion restriction is only a necessary condition for IV to work; it is not a su\ufb03cient condition. After all, if all we needed was exclusion, then we could use a random number generator for an instru- ment. Exclusion is not enough. We also need the instrument to be highly correlated with the endogenous variable schooling S. And the higher the better. We see that here because we are dividing by C(S, Z), so it necessarily requires that this covariance not be zero.\nThe numerator in this simple ratio is sometimes called the \u201creduced form,\u201d while the denominator is called the \u201c\ufb01rst stage.\u201d These terms are somewhat confusing, particularly the former, as \u201creduced form\u201d means different things to different people. But in the IV termi- nology, it is that relationship between the instrument and the outcome itself. The \ufb01rst stage is less confusing, as it gets its name from the two-stage least squares estimator, which we\u2019ll discuss next.\nWhen you take the probability limit of this expression, then assum- ing C(A, Z) = 0 and C(\u03b5, Z) = 0 due to the exclusion restriction, you get\np lim (cid:12)\u03b4 = \u03b4\nBut if Z is not independent of \u03b7 (either because it\u2019s correlated with A or \u03b5), and if the correlation between S and Z is weak, then (cid:12)\u03b4 becomes severely biased in \ufb01nite samples.\nTwo-stage least squares. One of the more intuitive instrumental vari- ables estimators is the two-stage least squares (2SLS). Let\u2019s review an example to illustrate why it is helpful for explaining some of the IV\nintuition. Suppose you have a sample of data on Y, S, and Z. For each observation i, we assume the data are generated according to:\nwhere C(Z, \u03b5) = 0 and \u03b2 (cid:9)= 0. The former assumption is the exclusion restriction whereas the second assumption is a non-zero \ufb01rst-stage.", "start_char_idx": 4177, "end_char_idx": 7996, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c117c0d8-2ae9-49f9-aa53-4f7bfb83c2da": {"__data__": {"id_": "c117c0d8-2ae9-49f9-aa53-4f7bfb83c2da", "embedding": null, "metadata": {"page number": "350 - 357", "chapter": "Instrumental Variables"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c9b5eba0-8b14-4697-88d6-c8ce7cd182a9", "node_type": "4", "metadata": {"page number": "350 - 357", "chapter": "Instrumental Variables"}, "hash": "aa5b2bbbce9d1e70425880735d6aa53163f953f080c9ebcaa8492c3ad1c23c55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a9990d1-6649-4b31-ad07-fcbb9bbafb61", "node_type": "1", "metadata": {"page number": "350 - 357", "chapter": "Instrumental Variables"}, "hash": "4b558b2d6ddd575f04b27a57b7c0c574a06778e4ef7b8f4e71fd12f0f6106731", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "08d057ee-01fa-4d31-acb8-61ffb19c6d2d", "node_type": "1", "metadata": {}, "hash": "14b01d54651964875abdc888644428f8d409f375a3f9f2430c473794b2ef262a", "class_name": "RelatedNodeInfo"}}, "text": "Two-stage least squares. One of the more intuitive instrumental vari- ables estimators is the two-stage least squares (2SLS). Let\u2019s review an example to illustrate why it is helpful for explaining some of the IV\nintuition. Suppose you have a sample of data on Y, S, and Z. For each observation i, we assume the data are generated according to:\nwhere C(Z, \u03b5) = 0 and \u03b2 (cid:9)= 0. The former assumption is the exclusion restriction whereas the second assumption is a non-zero \ufb01rst-stage. (xi \u2212 \u00afx) = 0, Now using our IV expression, and using the result that we can write out the IV estimator as:\nn i=1\nWhen we substitute the true model for Y, we get the following:\n= \u03b4 + \u201csmall if n is large\u201d\nSo, let\u2019s return to our \ufb01rst description of (cid:12)\u03b4 as the ratio of two covari- ances. With some simple algebraic manipulation, we get the following: (cid:12)\u03b4 = C(Y, Z) C(S, Z) C(Z, Y) V(Z) C(Z, S) V(Z)\nwhere the denominator is equal to (cid:12)\u03b2.4 We can rewrite (cid:12)\u03b2 as:\nThen we rewrite the IV estimator and make a substitution:\nNotice now what is inside the parentheses: (cid:12)\u03b2Z, which are the \ufb01tted values of schooling from the \ufb01rst-stage regression. We are no longer, in other words, using S\u2014we are using its \ufb01tted values. Recall that S = \u03b3 + \u03b2Z + (cid:2); (cid:12)\u03b4 = C((cid:12)\u03b2ZY) and let (cid:12)S = (cid:12)\u03b3 + (cid:12)\u03b2Z. Then the two-stage least V((cid:12)\u03b2Z)\nsquares (2SLS) estimator is:\nI will now show that (cid:12)\u03b2C(Y, Z) = C((cid:12)S, Y), and leave it to you to show that V((cid:12)\u03b2Z) = V((cid:12)S).\n4 That is, Si = \u03b3 + \u03b2Zi + (cid:2)i.\nNow let\u2019s return to something I said earlier\u2014learning 2SLS can help you better understand the intuition of instrumental variables more gen- erally. What does this mean exactly? First, the 2SLS estimator used only the \ufb01tted values of the endogenous regressors for estimation. These \ufb01tted values were based on all variables used in the model, including the excludable instrument. And as all of these instruments are exogenous in the structural model, what this means is that the \ufb01tted values themselves have become exogenous too. Put differently, we are using only the variation in schooling that is exogenous. So that\u2019s kind of interesting, as now we\u2019re back in a world where we are identifying causal effects from exogenous changes in schooling caused by our instrument.\nBut, now the less-exciting news. This exogenous variation in S driven by the instrument is only a subset of the total variation in school- ing. Or put differently, IV reduces the variation in the data, so there is less information available for identi\ufb01cation, and what little variation we have left comes from only those units who responded to the instru- ment in the \ufb01rst place. This, it turns out, will be critical later when we relax the homogeneous treatment effects assumption and allow for heterogeneity.", "start_char_idx": 7510, "end_char_idx": 10351, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "08d057ee-01fa-4d31-acb8-61ffb19c6d2d": {"__data__": {"id_": "08d057ee-01fa-4d31-acb8-61ffb19c6d2d", "embedding": null, "metadata": {"page number": "357 - 366", "chapter": "Instrumental Variables"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bd5b8f1f-211a-4cc1-810e-fd3efaebd20e", "node_type": "4", "metadata": {"page number": "357 - 366", "chapter": "Instrumental Variables"}, "hash": "942bbf9dfe47b404d7b679c0655f63c975191944e202fad320d59fb17808209a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c117c0d8-2ae9-49f9-aa53-4f7bfb83c2da", "node_type": "1", "metadata": {"page number": "350 - 357", "chapter": "Instrumental Variables"}, "hash": "292a94635e268d38ab175e2815ae4daeef27137d536eae7d7a47736ddfc62bcf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab93fbb1-be9c-47ad-a1eb-108ae5b3ad37", "node_type": "1", "metadata": {}, "hash": "1eb0c6230c552269ca0960bd83609250c78cf1b51d6c52682b9edbe26ba757a7", "class_name": "RelatedNodeInfo"}}, "text": "Parental Methamphetamine Abuse and Foster Care:\nIt\u2019s helpful to occasionally stop and try to think about real-world applications as much as possible; otherwise the estimators feel very opaque and unhelpful. So to illustrate, I\u2019m going to review one of my own papers with Keith Finlay that sought to estimate the effect that parental methamphetamine abuse had on child abuse and foster care admissions [Cunningham and Finlay, 2012].\nIt has been claimed that substance abuse, notably illicit drug use, has a negative impact on parenting, causing neglect, but as these all occur in equilibrium, it\u2019s possible that the correlation is simply re\ufb02ec- tive of selection bias. Maybe households with parents who abuse drugs would\u2019ve had the same negative outcomes had the parents not used drugs. After all, it\u2019s not like people are \ufb02ipping coins when decid- ing to smoke meth. So let me brie\ufb02y give you some background to the study so that you better understand the data-generating process.\nNow let\u2019s return to something I said earlier\u2014learning 2SLS can help you better understand the intuition of instrumental variables more gen- erally. What does this mean exactly? First, the 2SLS estimator used only the \ufb01tted values of the endogenous regressors for estimation. These \ufb01tted values were based on all variables used in the model, including the excludable instrument. And as all of these instruments are exogenous in the structural model, what this means is that the \ufb01tted values themselves have become exogenous too. Put differently, we are using only the variation in schooling that is exogenous. So that\u2019s kind of interesting, as now we\u2019re back in a world where we are identifying causal effects from exogenous changes in schooling caused by our instrument.\nBut, now the less-exciting news. This exogenous variation in S driven by the instrument is only a subset of the total variation in school- ing. Or put differently, IV reduces the variation in the data, so there is less information available for identi\ufb01cation, and what little variation we have left comes from only those units who responded to the instru- ment in the \ufb01rst place. This, it turns out, will be critical later when we relax the homogeneous treatment effects assumption and allow for heterogeneity.\nIt\u2019s helpful to occasionally stop and try to think about real-world applications as much as possible; otherwise the estimators feel very opaque and unhelpful. So to illustrate, I\u2019m going to review one of my own papers with Keith Finlay that sought to estimate the effect that parental methamphetamine abuse had on child abuse and foster care admissions [Cunningham and Finlay, 2012].\nIt has been claimed that substance abuse, notably illicit drug use, has a negative impact on parenting, causing neglect, but as these all occur in equilibrium, it\u2019s possible that the correlation is simply re\ufb02ec- tive of selection bias. Maybe households with parents who abuse drugs would\u2019ve had the same negative outcomes had the parents not used drugs. After all, it\u2019s not like people are \ufb02ipping coins when decid- ing to smoke meth. So let me brie\ufb02y give you some background to the study so that you better understand the data-generating process.\nFirst, methamphetamine is a toxic poison to the mind and body and highly addictive. Some of the symptoms of meth abuse are increased energy and alertness, decreased appetite, intense euphoria, impaired judgment, and psychosis. Second, the meth epidemic in the United States began on the West Coast, before gradually making its way eastward over the 1990s.\nWe were interested in the impact that this growth in meth abuse was having on children. Observers and law enforcement had com- mented, without concrete causal evidence, that the epidemic was causing a growth in foster care admissions. But how could we sep- arate correlation from causality? The solution was contained within how meth itself is produced.\nMeth is synthesized from a reduction of ephedrine or pseu- doephedrine, which is also the active ingredient in many cold medi- cations, such as Sudafed. Without one of these two precursors, it is impossible to produce the kind of meth people abuse. These precur- sors had supply chains that could be potentially disrupted because of the concentration of pharmaceutical laboratories. In 2004, nine fac- tories manufactured the bulk of the world\u2019s supply of ephedrine and pseudoephedrine.", "start_char_idx": 0, "end_char_idx": 4392, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab93fbb1-be9c-47ad-a1eb-108ae5b3ad37": {"__data__": {"id_": "ab93fbb1-be9c-47ad-a1eb-108ae5b3ad37", "embedding": null, "metadata": {"page number": "357 - 366", "chapter": "Instrumental Variables"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bd5b8f1f-211a-4cc1-810e-fd3efaebd20e", "node_type": "4", "metadata": {"page number": "357 - 366", "chapter": "Instrumental Variables"}, "hash": "942bbf9dfe47b404d7b679c0655f63c975191944e202fad320d59fb17808209a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "08d057ee-01fa-4d31-acb8-61ffb19c6d2d", "node_type": "1", "metadata": {"page number": "357 - 366", "chapter": "Instrumental Variables"}, "hash": "439465390fdfb9ceb0d61730cd7703f9404539283c42dd9160132001dfadefc5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5bb2cc83-4e15-4131-bb2a-739e188fdfa5", "node_type": "1", "metadata": {}, "hash": "6688c2758531e1af8fda522fc13b9baf474e858a6560c33c48b58a609720f11c", "class_name": "RelatedNodeInfo"}}, "text": "We were interested in the impact that this growth in meth abuse was having on children. Observers and law enforcement had com- mented, without concrete causal evidence, that the epidemic was causing a growth in foster care admissions. But how could we sep- arate correlation from causality? The solution was contained within how meth itself is produced.\nMeth is synthesized from a reduction of ephedrine or pseu- doephedrine, which is also the active ingredient in many cold medi- cations, such as Sudafed. Without one of these two precursors, it is impossible to produce the kind of meth people abuse. These precur- sors had supply chains that could be potentially disrupted because of the concentration of pharmaceutical laboratories. In 2004, nine fac- tories manufactured the bulk of the world\u2019s supply of ephedrine and pseudoephedrine. The US Drug Enforcement Agency correctly noted that if it could regulate access to ephedrine and pseudoephedrine, then it could effectively interrupt the production of methamphetamine, and in turn, hypothetically reduce meth abuse and its associated social harms.\nSo, with input from the DEA, Congress passed the Domestic Chem- ical Diversion Control Act in August 1995, which provided safeguards by regulating the distribution of products that contained ephedrine as the primary medicinal ingredient. But the new legislation\u2019s regulations applied to ephedrine, not pseudoephedrine, and since the two pre- cursors were nearly identical, tra\ufb03ckers quickly substituted. By 1996, pseudoephedrine was found to be the primary precursor in almost half of meth lab seizures.\nTherefore, the DEA went back to Congress, seeking greater con- trol over pseudoephedrine products. And the Comprehensive Metham- phetamine Control Act of 1996 went into effect between October and December 1997. This act required distributors of all forms of pseu- doephedrine to be subject to chemical registration. Dobkin and Nicosia\nFigure 45. Ratio of median monthly expected retail prices of meth, heroin, and cocaine\nrelative to their respective values in 1995, STRIDE 1995\u20131999. Reprinted from\nCunningham, S. and Finlay, K. (2012). \u201cParental Substance Abuse and Fos- ter Care: Evidence from Two Methamphetamine Supply Shocks?\u201d Economic Inquiry, 51(1):764\u2013782. Copyright \u00a9 2012 Wiley. Used with permission from John Wiley and Sons.\n[2009] argued that these precursor shocks may very well have been the largest supply shocks in the history of drug enforcement.\nWe placed a Freedom of Information Act request with the DEA requesting all of their undercover purchases and seizures of illicit drugs going back decades. The data included the price of an under- cover purchase, the drug\u2019s type, its weight and its purity, as well as the locations in which the purchases occurred. We used these data to construct a price series for meth, heroin, and cocaine. The effect of the two interventions were dramatic. The \ufb01rst supply intervention caused retail (street) prices (adjusted for purity, weight, and in\ufb02ation) to more than quadruple. The second intervention, while still quite effective at raising relative prices, did not have as large an effect as the \ufb01rst. See Figure 45.\nWe showed two other drug prices (cocaine and heroin) in addi- tion to meth because we wanted the reader to understand that the\n1995 and 1997 shocks were uniquely impacting meth markets. They did not appear to be common shocks affecting all drug markets, in other words. As a result, we felt more con\ufb01dent that our analysis would be able to isolate the effect of methamphetamine, as opposed to sub- stance abuse more generally. The two interventions simply had no effect on cocaine and heroin prices despite causing a massive short- age of meth and raising its retail price. It wouldn\u2019t have surprised me if disrupting meth markets had caused a shift in demand for cocaine or heroin and in turn caused its prices to change, yet at \ufb01rst glance in the time series, I\u2019m not \ufb01nding that. Weird.\nWe are interested in the causal effect of meth abuse on child abuse, and so our \ufb01rst stage is necessarily a proxy for meth abuse\u2014 the number of people entering treatment who listed meth as one of the substances they used in their last episode of substance abuse. As I said before, since a picture is worth a thousand words, I\u2019m going to show you pictures of both the \ufb01rst stage and the reduced form.", "start_char_idx": 3552, "end_char_idx": 7926, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5bb2cc83-4e15-4131-bb2a-739e188fdfa5": {"__data__": {"id_": "5bb2cc83-4e15-4131-bb2a-739e188fdfa5", "embedding": null, "metadata": {"page number": "357 - 366", "chapter": "Instrumental Variables"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bd5b8f1f-211a-4cc1-810e-fd3efaebd20e", "node_type": "4", "metadata": {"page number": "357 - 366", "chapter": "Instrumental Variables"}, "hash": "942bbf9dfe47b404d7b679c0655f63c975191944e202fad320d59fb17808209a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ab93fbb1-be9c-47ad-a1eb-108ae5b3ad37", "node_type": "1", "metadata": {"page number": "357 - 366", "chapter": "Instrumental Variables"}, "hash": "df81d16e575b27e4f8d9eedb48fa6b5b8028ad6e6d5d65df87228babd8cea3a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "db19e2b2-3432-41ed-9739-c1412172868b", "node_type": "1", "metadata": {}, "hash": "cc1fb7e5e1db7fff5bdc620a822a270213400af5855f05c50a4ac107df32529e", "class_name": "RelatedNodeInfo"}}, "text": "The two interventions simply had no effect on cocaine and heroin prices despite causing a massive short- age of meth and raising its retail price. It wouldn\u2019t have surprised me if disrupting meth markets had caused a shift in demand for cocaine or heroin and in turn caused its prices to change, yet at \ufb01rst glance in the time series, I\u2019m not \ufb01nding that. Weird.\nWe are interested in the causal effect of meth abuse on child abuse, and so our \ufb01rst stage is necessarily a proxy for meth abuse\u2014 the number of people entering treatment who listed meth as one of the substances they used in their last episode of substance abuse. As I said before, since a picture is worth a thousand words, I\u2019m going to show you pictures of both the \ufb01rst stage and the reduced form. Why do I do this instead of going directly to the tables of coe\ufb03cients? Because quite frankly, you are more likely to \ufb01nd those estimates believable if you can see evidence for the \ufb01rst stage and the reduced form in the raw data itself.5\nIn Figure 46, we show the \ufb01rst stage. All of these data come from the Treatment Episode Data Set (TEDS), which includes all peo- ple going into treatment for substance abuse at federally funded clin- ics. Patients list the last three substances used in the most recent \u201cepisode.\u201d We mark anyone who listed meth, cocaine, or heroin and aggregate by month and state. But \ufb01rst, let\u2019s look at the national aggre- gate in Figure 46. You can see evidence for the effect the two inter- ventions had on meth \ufb02ows, particularly the ephedrine intervention. Self-admitted meth admissions dropped signi\ufb01cantly, as did total meth admissions, but there\u2019s no effect on cocaine or heroin. The effect of the pseudoephedrine is not as dramatic, but it appears to cause a break in trend as the growth in meth admissions slows during this period of time. In summary, it appears we have a \ufb01rst stage because, during the interventions, meth admissions declines.\n5 I wouldn\u2019t go so far as to say that presenting pictures of the reduced form and\n\ufb01rst stage is mandatory in the way that many pictures in an RDD are mandatory, but it\u2019s\nawfully close. Ultimately, seeing is believing.\nFigure 46. Visual representation of the equivalent of the \ufb01rst stage. Reprinted from\nCunningham, S. and Finlay, K. (2012). \u201cParental Substance Abuse and Fos- ter Care: Evidence from Two Methamphetamine Supply Shocks?\u201d Economic Inquiry, 51(1):764\u2013782. Copyright \u00a9 2012 Wiley. Used with permission from John Wiley and Sons.\nIn Figure 47, we show the reduced form\u2014that is, the effect of the price shocks on foster care admissions. Consistent with what we found in our \ufb01rst-stage graphic, the ephedrine intervention in particular had a profoundly negative effect on foster care admissions. They fell from around 8,000 children removed per month to around 6,000, then began rising again. The second intervention also had an effect, though it appears to be milder. The reason we believe that the second interven- tion had a more modest effect than the \ufb01rst is because (1) the effect on price was about half the size of the \ufb01rst intervention, and (2) domes- tic meth production was being replaced by Mexican imports of meth over the late 1990s, and the precursor regulations were not applicable in Mexico. Thus, by the end of the 1990s, domestic meth production played a smaller role in total output, and hence the effect on price and admissions was probably smaller.\nFigure 47. Showing reduced form effect of interventions on children removed from fam-\nilies and placed into foster care. Reprinted from Cunningham, S. and Fin-\nlay, K. (2012). \u201cParental Substance Abuse and Foster Care: Evidence from Two Methamphetamine Supply Shocks?\u201d Economic Inquiry, 51(1):764\u2013782. Copyright \u00a9 2012 Wiley. Used with permission from John Wiley and Sons.\nIt\u2019s worth re\ufb02ecting for a moment on the reduced form. Why would rising retail prices of a pure gram of methamphetamine cause a child not to be placed in foster care? Prices don\u2019t cause child abuse\u2014they\u2019re just nominal pieces of information in the world. The only way in which a higher price for meth could reduce foster care admissions is if par- ents reduced their consumption of methamphetamine, which in turn caused a reduction in harm to one\u2019s child.", "start_char_idx": 7164, "end_char_idx": 11412, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "db19e2b2-3432-41ed-9739-c1412172868b": {"__data__": {"id_": "db19e2b2-3432-41ed-9739-c1412172868b", "embedding": null, "metadata": {"page number": "357 - 366", "chapter": "Instrumental Variables"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bd5b8f1f-211a-4cc1-810e-fd3efaebd20e", "node_type": "4", "metadata": {"page number": "357 - 366", "chapter": "Instrumental Variables"}, "hash": "942bbf9dfe47b404d7b679c0655f63c975191944e202fad320d59fb17808209a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5bb2cc83-4e15-4131-bb2a-739e188fdfa5", "node_type": "1", "metadata": {"page number": "357 - 366", "chapter": "Instrumental Variables"}, "hash": "209934439e1efe5c63f9493d05586e754a3406b4b985cc05f8e90323a0a9f6af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "90893272-7465-476c-9e36-352124bc9a34", "node_type": "1", "metadata": {}, "hash": "4a138944f3c9e97980a5bb6c31a1dd99384fa7ff6ae97db48a4c4b5a636745c3", "class_name": "RelatedNodeInfo"}}, "text": "Figure 47. Showing reduced form effect of interventions on children removed from fam-\nilies and placed into foster care. Reprinted from Cunningham, S. and Fin-\nlay, K. (2012). \u201cParental Substance Abuse and Foster Care: Evidence from Two Methamphetamine Supply Shocks?\u201d Economic Inquiry, 51(1):764\u2013782. Copyright \u00a9 2012 Wiley. Used with permission from John Wiley and Sons.\nIt\u2019s worth re\ufb02ecting for a moment on the reduced form. Why would rising retail prices of a pure gram of methamphetamine cause a child not to be placed in foster care? Prices don\u2019t cause child abuse\u2014they\u2019re just nominal pieces of information in the world. The only way in which a higher price for meth could reduce foster care admissions is if par- ents reduced their consumption of methamphetamine, which in turn caused a reduction in harm to one\u2019s child. This picture is a key piece of evidence for the reader that this is going on.\nIn Table 52, I reproduce our main results from my article with Keith. There are a few pieces of key information that all IV tables should have. First, there is the OLS regression. As the OLS regression suf- fers from endogeneity, we want the reader to see it so that they have something to compare the IV model to. Let\u2019s focus on column 1, where the dependent variable is total entry into foster care. We \ufb01nd no effect, interestingly, of meth on foster care when we estimate using OLS.\nMonth-of-year \ufb01xed effects State controls State \ufb01xed effects State linear time trends\nNote: Log latest entry into foster care is the natural log of the sum of all new foster care admissions by state, race, and month. Models 3 to 10 denote the \ufb02ow of children into foster care via a given route of admission denoted by the column heading. Models 11 and 12 use the natural log of\nthe sum of all foster care exits by state, race, and month. ***, **, and * denote statistical signi\ufb01cance at the 1%, 5%, and 10% levels, respectively.\nThe second piece of information that one should report in a 2SLS table is the \ufb01rst stage itself. We report the \ufb01rst stage at the bottom of each even-numbered column. As you can see, for each one-unit devia- tion in price from its long-term trend, meth admissions into treatment (our proxy) fell by \u22120.0005 log points. This is highly signi\ufb01cant at the 1% level, but we check for the strength of the instrument using the F statistic [Staiger and Stock, 1997].6 We have an F statistic of 17.6, which suggests that our instrument is strong enough for identi\ufb01cation.\nFinally, let\u2019s examine the 2SLS estimate of the treatment effect itself. Notice using only the exogenous variation in log meth admis- sions, and assuming the exclusion restriction holds in our model, we are able to isolate a causal effect of log meth admissions on log aggre- gate foster care admissions. As this is a log-log regression, we can interpret the coe\ufb03cient as an elasticity. We \ufb01nd that a 10% increase in meth admissions for treatment appears to cause around a 15% increase in children being removed from their homes and placed into foster care. This effect is both large and precise. And it was not detectable otherwise (the coe\ufb03cient was zero).\nWhy are they being removed? Our data (AFCARS) lists several channels: parental incarceration, child neglect, parental drug use, and physical abuse. Interestingly, we do not \ufb01nd any effect of parental drug use or parental incarceration, which is perhaps counterintuitive. Their signs are negative and their standard errors are large. Rather, we \ufb01nd effects of meth admissions on removals for physical abuse and neglect. Both are elastic (i.e., \u03b4 > 1).\nWhat did we learn? First, we learned how a contemporary piece of applied microeconomics goes about using instrumental variables to identify causal effects. We saw the kinds of graphical evidence mus- tered, the way in which knowledge about the natural experiment and the policies involved helped the authors argue for the exclusion restric- tion (since it cannot be tested), and the kind of evidence presented from 2SLS, including the \ufb01rst-stage tests for weak instruments. Hope- fully seeing a paper at this point was helpful. But the second thing we learned concerned the actual study itself.", "start_char_idx": 10584, "end_char_idx": 14772, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "90893272-7465-476c-9e36-352124bc9a34": {"__data__": {"id_": "90893272-7465-476c-9e36-352124bc9a34", "embedding": null, "metadata": {"page number": "357 - 366", "chapter": "Instrumental Variables"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bd5b8f1f-211a-4cc1-810e-fd3efaebd20e", "node_type": "4", "metadata": {"page number": "357 - 366", "chapter": "Instrumental Variables"}, "hash": "942bbf9dfe47b404d7b679c0655f63c975191944e202fad320d59fb17808209a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "db19e2b2-3432-41ed-9739-c1412172868b", "node_type": "1", "metadata": {"page number": "357 - 366", "chapter": "Instrumental Variables"}, "hash": "43da0de89baa5c71bbae23873b33e9baa33c36f18def66e9cbdcf35d072ec493", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4105950d-196c-429a-9876-34a66625b39b", "node_type": "1", "metadata": {}, "hash": "56023bfc6974cd9a539665d56a3adb432ca617f0ed8c69462fd5c727fb8ee9ea", "class_name": "RelatedNodeInfo"}}, "text": "Interestingly, we do not \ufb01nd any effect of parental drug use or parental incarceration, which is perhaps counterintuitive. Their signs are negative and their standard errors are large. Rather, we \ufb01nd effects of meth admissions on removals for physical abuse and neglect. Both are elastic (i.e., \u03b4 > 1).\nWhat did we learn? First, we learned how a contemporary piece of applied microeconomics goes about using instrumental variables to identify causal effects. We saw the kinds of graphical evidence mus- tered, the way in which knowledge about the natural experiment and the policies involved helped the authors argue for the exclusion restric- tion (since it cannot be tested), and the kind of evidence presented from 2SLS, including the \ufb01rst-stage tests for weak instruments. Hope- fully seeing a paper at this point was helpful. But the second thing we learned concerned the actual study itself. We learned that for the group\n6 I explain the importance of the F statistic later in this chapter, but ordinarily an F test on the excludability of the instrument from the \ufb01rst stage is calculated to check for\nof meth users whose behavior was changed as a result of rising real prices of a pure gram of methamphetamine (i.e., the complier subpop- ulation), their meth use was causing child abuse and neglect so severe that it merited removing their children and placing those children into foster care. If you were only familiar with Dobkin and Nicosia [2009], who found no effect of meth on crime using county-level data from California and only the 1997 ephedrine shock, you might incorrectly conclude that there are no social costs associated with meth abuse. But, while meth does not appear to cause crime in California, it does appear to harm the children of meth users and places strains on the foster care system.", "start_char_idx": 13875, "end_char_idx": 15693, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4105950d-196c-429a-9876-34a66625b39b": {"__data__": {"id_": "4105950d-196c-429a-9876-34a66625b39b", "embedding": null, "metadata": {"page number": "366 - 376", "chapter": "Instrumental Variables"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b1db15f2-4c85-4d62-bcb0-8f7f43a76d83", "node_type": "4", "metadata": {"page number": "366 - 376", "chapter": "Instrumental Variables"}, "hash": "9e1f6224307f70abfd0142adac26799ab4dc145eab927bfb6e95ae23d47ff177", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "90893272-7465-476c-9e36-352124bc9a34", "node_type": "1", "metadata": {"page number": "357 - 366", "chapter": "Instrumental Variables"}, "hash": "2ab2ecf7f94ca88dbb9cecdcf9591d0f6643525e7a290810dca775af74406fad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ed740d9b-30d6-4dcd-9dd4-14ea4281cbc3", "node_type": "1", "metadata": {}, "hash": "9eea655d09bde8d5ab22e3d1cb02342a154240c635b37f05b6739c1e2c3c1af6", "class_name": "RelatedNodeInfo"}}, "text": "The Problem of Weak Instruments:\nI am not trying to smother you with papers. But before we move back into the technical material itself, I\u2019d like to discuss one more paper. This paper will also help you better understand the weak instru- ment literature following its publication.\nAs we\u2019ve said since the beginning, with example after example, there is a very long tradition in labor economics of building models that can credibly identify the returns to schooling. This goes back to Becker [1994] and the workshop at Columbia that Becker ran for years with Jacob Mincer. This study of the returns to schooling has been an important task given education\u2019s growing importance in the distri- bution of income and wealth since the latter twentieth century with increasing returns to skill in the marketplace [Juhn et al., 1993].\nOne of the more seminal papers in instrumental variables for the modern period is Angrist and Krueger [1991]. Their idea is simple and clever; a quirk in the United States educational system is that a child enters a grade on the basis of his or her birthday. For a long time, that cutoff was late December. If children were born on or before December 31, then they were assigned to the \ufb01rst grade. But if their birthday was on or after January 1, they were assigned to kindergarten. Thus two people\u2014one born on December 31 and one born on January 1\u2014were exogenously assigned different grades.\nof meth users whose behavior was changed as a result of rising real prices of a pure gram of methamphetamine (i.e., the complier subpop- ulation), their meth use was causing child abuse and neglect so severe that it merited removing their children and placing those children into foster care. If you were only familiar with Dobkin and Nicosia [2009], who found no effect of meth on crime using county-level data from California and only the 1997 ephedrine shock, you might incorrectly conclude that there are no social costs associated with meth abuse. But, while meth does not appear to cause crime in California, it does appear to harm the children of meth users and places strains on the foster care system.\nI am not trying to smother you with papers. But before we move back into the technical material itself, I\u2019d like to discuss one more paper. This paper will also help you better understand the weak instru- ment literature following its publication.\nAs we\u2019ve said since the beginning, with example after example, there is a very long tradition in labor economics of building models that can credibly identify the returns to schooling. This goes back to Becker [1994] and the workshop at Columbia that Becker ran for years with Jacob Mincer. This study of the returns to schooling has been an important task given education\u2019s growing importance in the distri- bution of income and wealth since the latter twentieth century with increasing returns to skill in the marketplace [Juhn et al., 1993].\nOne of the more seminal papers in instrumental variables for the modern period is Angrist and Krueger [1991]. Their idea is simple and clever; a quirk in the United States educational system is that a child enters a grade on the basis of his or her birthday. For a long time, that cutoff was late December. If children were born on or before December 31, then they were assigned to the \ufb01rst grade. But if their birthday was on or after January 1, they were assigned to kindergarten. Thus two people\u2014one born on December 31 and one born on January 1\u2014were exogenously assigned different grades.\nFigure 48. Compulsory schooling start dates by birthdates.\nNow there\u2019s nothing necessarily relevant here because if those chil- dren always stay in school for the duration necessary to get a high school degree, then that arbitrary assignment of start date won\u2019t affect high school completion. It\u2019ll only affect when they get that high school degree. But this is where it gets interesting. For most of the twentieth century, the US had compulsory schooling laws that forced a person to remain in high school until age 16. After age 16, one could legally stop going to school. Figure 48 explains visually this instrumental variable.7 Angrist and Krueger had the insight that that small quirk was exogenously assigning more schooling to people born later in the year. The person born in December would reach age 16 with more educa- tion than the person born in January, in other words. Thus, the authors uncovered small exogenous variation in schooling. Notice how similar their idea was to regression discontinuity. That\u2019s because IV and RDD are conceptually very similar strategies.\nFigure 49 shows the \ufb01rst stage, and it is really interesting.", "start_char_idx": 0, "end_char_idx": 4659, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ed740d9b-30d6-4dcd-9dd4-14ea4281cbc3": {"__data__": {"id_": "ed740d9b-30d6-4dcd-9dd4-14ea4281cbc3", "embedding": null, "metadata": {"page number": "366 - 376", "chapter": "Instrumental Variables"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b1db15f2-4c85-4d62-bcb0-8f7f43a76d83", "node_type": "4", "metadata": {"page number": "366 - 376", "chapter": "Instrumental Variables"}, "hash": "9e1f6224307f70abfd0142adac26799ab4dc145eab927bfb6e95ae23d47ff177", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4105950d-196c-429a-9876-34a66625b39b", "node_type": "1", "metadata": {"page number": "366 - 376", "chapter": "Instrumental Variables"}, "hash": "5c533badb3ec36700a6d5d93c80b159762f71755b683b47105fd1964547ffad6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3427b9e5-8afc-4626-b3d7-6673a8e88ab4", "node_type": "1", "metadata": {}, "hash": "96f0818dcf5184235e2e837d50cabdde5f474d832d2bf8efbe1fa0c61ce20e42", "class_name": "RelatedNodeInfo"}}, "text": "It\u2019ll only affect when they get that high school degree. But this is where it gets interesting. For most of the twentieth century, the US had compulsory schooling laws that forced a person to remain in high school until age 16. After age 16, one could legally stop going to school. Figure 48 explains visually this instrumental variable.7 Angrist and Krueger had the insight that that small quirk was exogenously assigning more schooling to people born later in the year. The person born in December would reach age 16 with more educa- tion than the person born in January, in other words. Thus, the authors uncovered small exogenous variation in schooling. Notice how similar their idea was to regression discontinuity. That\u2019s because IV and RDD are conceptually very similar strategies.\nFigure 49 shows the \ufb01rst stage, and it is really interesting. Look at all those 3s and 4s at the top of the picture. There\u2019s a clear pattern\u2014 those with birthdays in the third and fourth quarter have more school- ing on average than do those with birthdays in the \ufb01rst and second quarters. That relationship gets weaker as we move into later cohorts, but that is probably because for later cohorts, the price on higher lev- els of schooling was rising so much that fewer and fewer people were dropping out before \ufb01nishing their high school degree.\n7 Angrist and Krueger always made such helpful and effective graphics for their\nstudies, and this paper is a great example.\nFigure 49. First-stage relationship between quarter of birth and schooling. Reprinted\nfrom Cunningham, S. and Finlay, K. (2012). \u201cParental Substance Abuse and Foster Care: Evidence from Two Methamphetamine Supply Shocks?\u201d Eco- nomic Inquiry, 51(1):764\u2013782. Copyright \u00a9 2012 Wiley. Used with permission from John Wiley and Sons.\nFigure 50 shows the reduced-form relationship between quarter of birth and log weekly earnings.8 You have to squint a little bit, but you can see the pattern\u2014all along the top of the jagged path are 3s and 4s, and all along the bottom of the jagged path are 1s and 2s. Not always, but it\u2019s correlated.\nRemember what I said about how instruments having a certain ridiculousness to them? That is, you know you have a good instrument if the instrument itself doesn\u2019t seem relevant for explaining the out- come of interest because that\u2019s what the exclusion restriction implies. Why would quarter of birth affect earnings? It doesn\u2019t make any obvi- ous, logical sense why it should. But, if I told you that people born later\n8 I know, I know. No one has ever accused me of being subtle. But it\u2019s an important\npoint\u2014a picture is worth a thousand words. If you can communicate your \ufb01rst stage\nand reduced form in pictures, you always should, as it will really captivate the reader\u2019s\nattention and be far more compelling than a simple table of coe\ufb03cients ever could.\nschooling. Reprinted from Angrist, J. D. and Krueger, A. B. (1991). \u201cDoes Compulsory School Attendance Affect Schooling and Earnings?\u201d Quarterly Journal of Economics, 106(4):979\u20131014. Permission from Oxford University Press.\nin the year got more schooling than those with less because of compul- sory schooling, then the relationship between the instrument and the outcome snaps into place. The only reason we can think of as to why the instrument would affect earnings is if the instrument were operat- ing through schooling. Instruments only explain the outcome, in other words, when you understand their effect on the endogenous variable.9 Angrist and Krueger use three dummies as their instruments: a dummy for \ufb01rst quarter, a dummy for second quarter, and a dummy for third quarter. Thus, the omitted category is the fourth quarter, which is the group that gets the most schooling. Now ask yourself this: if we regressed years of schooling onto those three dummies, what should\n9 Having said that, Buckles and Hungerman [2013] found that in fact there are\nsystematic differences in individual attributes that are predictors of ability by birth\nthe signs and magnitudes be? That is, what would we expect the rela- tionship between the \ufb01rst quarter (compared to the fourth quarter) and schooling? Let\u2019s look at their \ufb01rst-stage results (Table 53).", "start_char_idx": 3809, "end_char_idx": 8007, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3427b9e5-8afc-4626-b3d7-6673a8e88ab4": {"__data__": {"id_": "3427b9e5-8afc-4626-b3d7-6673a8e88ab4", "embedding": null, "metadata": {"page number": "366 - 376", "chapter": "Instrumental Variables"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b1db15f2-4c85-4d62-bcb0-8f7f43a76d83", "node_type": "4", "metadata": {"page number": "366 - 376", "chapter": "Instrumental Variables"}, "hash": "9e1f6224307f70abfd0142adac26799ab4dc145eab927bfb6e95ae23d47ff177", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ed740d9b-30d6-4dcd-9dd4-14ea4281cbc3", "node_type": "1", "metadata": {"page number": "366 - 376", "chapter": "Instrumental Variables"}, "hash": "6132487c17fb8f8f1a7800e6cc3cfa046cf677e7a386ed9c63938d64ed80c0d8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d7347f9-3a89-4981-a560-599d6e99aa72", "node_type": "1", "metadata": {}, "hash": "bcb2827c1e8ff5cf9d2e2b056cade2fcd4437cf7acdf83d163714202d220a91f", "class_name": "RelatedNodeInfo"}}, "text": "Instruments only explain the outcome, in other words, when you understand their effect on the endogenous variable.9 Angrist and Krueger use three dummies as their instruments: a dummy for \ufb01rst quarter, a dummy for second quarter, and a dummy for third quarter. Thus, the omitted category is the fourth quarter, which is the group that gets the most schooling. Now ask yourself this: if we regressed years of schooling onto those three dummies, what should\n9 Having said that, Buckles and Hungerman [2013] found that in fact there are\nsystematic differences in individual attributes that are predictors of ability by birth\nthe signs and magnitudes be? That is, what would we expect the rela- tionship between the \ufb01rst quarter (compared to the fourth quarter) and schooling? Let\u2019s look at their \ufb01rst-stage results (Table 53).\nTable 53 shows the \ufb01rst stage from a regression of the following\nwhere Zi is the dummy for the \ufb01rst three quarters, and \u03c0i is the coe\ufb03- cient on each dummy. Now we look at what they produced in Table 53. The coe\ufb03cients are all negative and signi\ufb01cant for the total years of education and the high school graduate dependent variables. Notice, too, that the relationship gets much weaker once we move beyond the groups bound by compulsory schooling: the number of years of schooling for high school students (no effect) and probability of being a college graduate (no effect).\nRegarding those college non-results. Ask yourself this question: why should we expect quarter of birth to affect the probability of being a high school graduate but not a college grad? What if we had found quarter of birth predicted high school completion, college completion, post-graduate completion, and total years of schooling beyond high\nTable 54. Effect of schooling on wages using OLS and 2SLS.\nYears of schooling\nNote: Standard errors in parentheses. First stage is quarter of birth dummies.\nschool? Wouldn\u2019t it start to seem like this compulsory schooling instru- ment was not what we thought it was? After all, this quarter of birth instrument really should only impact high school completion; since it doesn\u2019t bind anyone beyond high school, it shouldn\u2019t affect the num- ber of years beyond high school or college completion probabilities. If it did, we might be skeptical of the whole design. But here it didn\u2019t, which to me makes it even more convincing that they\u2019re identifying a compulsory high school schooling effect.10\nNow we look at the second stage for both OLS and 2SLS (which the authors label TSLS, but means the same thing). Table 54 shows these results. The authors didn\u2019t report the \ufb01rst stage in this table because they reported it in the earlier table we just reviewed. For small val- ues, the log approximates a percentage change, so they are \ufb01nding a 7.1% return for every additional year of schooling, but with 2SLS it\u2019s higher (8.9%). That\u2019s interesting, because if it was merely ability bias, then we\u2019d expect the OLS estimate to be too large, not too small. So something other than mere ability bias must be going on here.\nFor whatever it\u2019s worth, I am personally convinced at this point that quarter of birth is a valid instrument and that they\u2019ve identi\ufb01ed a causal effect of schooling on earnings, but Angrist and Krueger [1991] want to go further, probably because they want more precision in their esti- mate. And to get more precision, they load up the \ufb01rst stage with even\n10 These kinds of falsi\ufb01cations are extremely common in contemporary applied\nwork. This is because many of the identifying assumptions in any research design are\nsimply untestable. And so the burden of proof is on researchers to convince the reader,\nmore instruments. Speci\ufb01cally, they use speci\ufb01cations with 30 dum- mies (quarter of birth \u00d7 year) and 150 dummies (quarter of birth \u00d7 state) as instruments. The idea is that the quarter of birth effect may\nBut at what cost? Many of these instruments are only now weakly\ncorrelated with schooling\u2014in some locations, they have almost no cor-\nrelation, and for some cohorts as well. We got a \ufb02avor of that, in fact,\nin Table 54, where the later cohorts show less variation in schooling\nby quarter of birth than the earlier cohorts.", "start_char_idx": 7184, "end_char_idx": 11376, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1d7347f9-3a89-4981-a560-599d6e99aa72": {"__data__": {"id_": "1d7347f9-3a89-4981-a560-599d6e99aa72", "embedding": null, "metadata": {"page number": "366 - 376", "chapter": "Instrumental Variables"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b1db15f2-4c85-4d62-bcb0-8f7f43a76d83", "node_type": "4", "metadata": {"page number": "366 - 376", "chapter": "Instrumental Variables"}, "hash": "9e1f6224307f70abfd0142adac26799ab4dc145eab927bfb6e95ae23d47ff177", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3427b9e5-8afc-4626-b3d7-6673a8e88ab4", "node_type": "1", "metadata": {"page number": "366 - 376", "chapter": "Instrumental Variables"}, "hash": "6d2be959e1efad60b139bf0ddde4d20f0df0bc0fab537a5fb677d250d901ed0b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "05866629-f5a8-4208-b3bf-4c8fd629a0a0", "node_type": "1", "metadata": {}, "hash": "7beca79a708308fc7339d2db253c599422762682a5105d23284efc8d1d7c8dfb", "class_name": "RelatedNodeInfo"}}, "text": "This is because many of the identifying assumptions in any research design are\nsimply untestable. And so the burden of proof is on researchers to convince the reader,\nmore instruments. Speci\ufb01cally, they use speci\ufb01cations with 30 dum- mies (quarter of birth \u00d7 year) and 150 dummies (quarter of birth \u00d7 state) as instruments. The idea is that the quarter of birth effect may\nBut at what cost? Many of these instruments are only now weakly\ncorrelated with schooling\u2014in some locations, they have almost no cor-\nrelation, and for some cohorts as well. We got a \ufb02avor of that, in fact,\nin Table 54, where the later cohorts show less variation in schooling\nby quarter of birth than the earlier cohorts. What is the effect, then, of\nreducing the variance in the estimator by loading up the \ufb01rst stage with\nBound et al. [1995] is a classic work in what is sometimes called\nthe \u201cweak instrument\u201d literature. It\u2019s in this paper that we learn some\nsome very basic problems created by weak instruments, such as the\nform of 2SLS bias in \ufb01nite samples. Since Bound et al. [1995] focused\non the compulsory schooling application that Angrist and Krueger\n[1991] had done, I will stick with that example throughout. Let\u2019s con-\nsider their model with a single endogenous regressor and a simple\nconstant treatment effect. The causal model of interest here is as\nwhere y is some outcome and s is some endogenous regressor, such\nas schooling. Our instrument is Z and the \ufb01rst-stage equation is:\ns = Z(cid:12)\u03c0 + \u03b7\nLet\u2019s start off by assuming that \u03b5 and \u03b7 are correlated. Then estimating the \ufb01rst equation by OLS would lead to biased results, wherein the OLS\nbias is:\nE[(cid:12)\u03b2OLS \u2212 \u03b2] = C(\u03b5, s) V(s)\nWe will rename this ratio as\nbias of 2SLS centers on the previously de\ufb01ned OLS bias as the weak-\n. Bound et al. [1995] show that the\nness of the instrument grows. Following Angrist and Pischke [2009],\nTable 55. Effect of completed schooling on men\u2019s log weekly wages.\nYears of schooling\nFirst stage F Excluded instruments Quarter of birth Quarter of birth \u00d7 year of birth Number of excluded instruments\nNote: Standard errors in parentheses. First stage is quarter of birth dummies.\nI\u2019ll express that bias as a function of the \ufb01rst-stage F statistic:\nwhere F is the population analogy of the F-statistic for the joint signi\ufb01- cance of the instruments in the \ufb01rst-stage regression. If the \ufb01rst stage is weak, and F \u2192 0, then the bias of 2SLS approaches . But if the\n\ufb01rst stage is very strong, F \u2192 \u221e, then the 2SLS bias goes to 0.\nReturning to our rhetorical question from earlier, what was the cost of adding instruments without predictive power? Adding more weak instruments causes the \ufb01rst-stage F statistic to approach zero and increase the bias of 2SLS.\nBound et al. [1995] studied this empirically, replicating Angrist and Krueger [1991], and using simulations. Table 55 shows what happens once they start adding in controls. Notice that as they do, the F statis- tic on the excludability of the instruments falls from 13.5 to 4.7 to 1.6. So by the F statistic, they are already running into a weak instrument once they include the 30 quarter of birth \u00d7 year dummies, and I think that\u2019s because as we saw, the relationship between quarter of birth and schooling got smaller for the later cohorts.\nNext, they added in the weak instruments\u2014all 180 of them\u2014which is shown in Table 56. And here we see that the problem persists. The\nTable 56. Effect of completed schooling on men\u2019s log weekly wages controlling for\nYears of schooling\nFirst stage F Excluded instruments Quarter of birth Quarter of birth \u00d7 year of birth Quarter of birth \u00d7 state of birth Number of excluded instruments\ninstruments are weak, and therefore the bias of the 2SLS coe\ufb03cient is close to that of the OLS bias.\nBut the really damning part of the Bound et al. [1995] was their\nsimulation.", "start_char_idx": 10681, "end_char_idx": 14520, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "05866629-f5a8-4208-b3bf-4c8fd629a0a0": {"__data__": {"id_": "05866629-f5a8-4208-b3bf-4c8fd629a0a0", "embedding": null, "metadata": {"page number": "366 - 376", "chapter": "Instrumental Variables"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b1db15f2-4c85-4d62-bcb0-8f7f43a76d83", "node_type": "4", "metadata": {"page number": "366 - 376", "chapter": "Instrumental Variables"}, "hash": "9e1f6224307f70abfd0142adac26799ab4dc145eab927bfb6e95ae23d47ff177", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d7347f9-3a89-4981-a560-599d6e99aa72", "node_type": "1", "metadata": {"page number": "366 - 376", "chapter": "Instrumental Variables"}, "hash": "4c80f9eee76a30289b3614f2434e5d9c5df939f3f2008a4176443be5085eadc6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d1a42ced-5e84-455c-a33f-94da9da184e1", "node_type": "1", "metadata": {}, "hash": "b2bd06b7294066309b5e1970e8ea52d0dd5e51507db057e8354c85e88891577b", "class_name": "RelatedNodeInfo"}}, "text": "So by the F statistic, they are already running into a weak instrument once they include the 30 quarter of birth \u00d7 year dummies, and I think that\u2019s because as we saw, the relationship between quarter of birth and schooling got smaller for the later cohorts.\nNext, they added in the weak instruments\u2014all 180 of them\u2014which is shown in Table 56. And here we see that the problem persists. The\nTable 56. Effect of completed schooling on men\u2019s log weekly wages controlling for\nYears of schooling\nFirst stage F Excluded instruments Quarter of birth Quarter of birth \u00d7 year of birth Quarter of birth \u00d7 state of birth Number of excluded instruments\ninstruments are weak, and therefore the bias of the 2SLS coe\ufb03cient is close to that of the OLS bias.\nBut the really damning part of the Bound et al. [1995] was their\nsimulation. The authors write:\nTo illustrate that second-stage results do not give us any indication of the existence of quantitatively important \ufb01nite-sample biases, we reestimated Table 1, columns (4) and (6) and Table 2, columns (2) and (4), using randomly generated information in place of the actual quarter of birth, following a suggestion by Alan Krueger. The means of the estimated standard errors reporting in the last row are quite close to the actual standard deviations of the 500 esti- mates for each model. . . . It is striking that the second-stage results reported in Table 3 look quite reasonable even with no information about educational attainment in the simulated instruments. They give no indication that the instruments were randomly generated. . . On the other hand, the F statistics on the excluded instruments in the \ufb01rst-stage regressions are always near their expected value of essentially 1 and do give a clear indication that the estimates of the second-stage coe\ufb03cients suffer from \ufb01nite-sample biases. (Bound et al., 448)\nSo, what can you do if you have weak instruments? Unfortunately, not a lot. You can use a just-identi\ufb01ed model with your strongest IV.\nSecond, you can use a limited-information maximum likelihood estima- tor (LIML). This is approximately median unbiased for over identi\ufb01ed constant effects models. It provides the same asymptotic distribu- tion as 2SLS under homogeneous treatment effects but provides a \ufb01nite-sample bias reduction.\nlet\u2019s be real for a second. If you have a weak instrument problem, then you only get so far by using LIML or estimating a just- identi\ufb01ed model. The real solution for a weak instrument problem is to get better instruments. Under homogeneous treatment effects, you\u2019re always identifying the same effect, so there\u2019s no worry about a complier only parameter. So you should just continue searching for stronger instruments that simultaneously satisfy the exclusion restriction.11\nIn conclusion, I think we\u2019ve learned a lot about instrumental vari- ables and why they are so powerful. The estimators based on this design are capable of identifying causal effects when your data suffer from selection on unobservables. Since selection on unobservables is believed to be very common, this is a very useful methodology for addressing that. But, that said, we also have learned some of the design\u2019s weaknesses, and hence why some people eschew it. Let\u2019s now move to heterogeneous treatment effects so that we can better understand some limitations a bit better.", "start_char_idx": 13702, "end_char_idx": 17048, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d1a42ced-5e84-455c-a33f-94da9da184e1": {"__data__": {"id_": "d1a42ced-5e84-455c-a33f-94da9da184e1", "embedding": null, "metadata": {"page number": "376 - 383", "chapter": "Instrumental Variables"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9717ba0e-2ae8-49f8-84fa-cf5372da3f73", "node_type": "4", "metadata": {"page number": "376 - 383", "chapter": "Instrumental Variables"}, "hash": "46f31d7544aca7ed7ff04c44af7a6216b8a499cf532f70ff965194f1b6222577", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "05866629-f5a8-4208-b3bf-4c8fd629a0a0", "node_type": "1", "metadata": {"page number": "366 - 376", "chapter": "Instrumental Variables"}, "hash": "03cd76f518ab9a1a721fbdcfa0f952c463d7962e107e31af50c3a1a70d03471d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ce1c60e1-c7d7-4f0b-8a5d-8362c1f04442", "node_type": "1", "metadata": {}, "hash": "24cafb0e51816a118c27e1fde0a77032eb52877212f33c4c58556a7627d20399", "class_name": "RelatedNodeInfo"}}, "text": "Heterogeneous Treatment Effects:\nNow we turn to a scenario where we relax the assumption that treatment effects are the same for every unit. This is where the poten- tial outcomes notation comes in handy. Instead, we will allow for each unit to have a unique response to the treatment, or\nNote that the treatment effect parameter now differs by individual i. We call this heterogeneous treatment effects.\n11 Good luck with that. Seriously, good luck because\u2014and I\u2019m going out on a limb\nhere\u2014if you had a better instrument, you\u2019d be using it!\nSecond, you can use a limited-information maximum likelihood estima- tor (LIML). This is approximately median unbiased for over identi\ufb01ed constant effects models. It provides the same asymptotic distribu- tion as 2SLS under homogeneous treatment effects but provides a \ufb01nite-sample bias reduction.\nlet\u2019s be real for a second. If you have a weak instrument problem, then you only get so far by using LIML or estimating a just- identi\ufb01ed model. The real solution for a weak instrument problem is to get better instruments. Under homogeneous treatment effects, you\u2019re always identifying the same effect, so there\u2019s no worry about a complier only parameter. So you should just continue searching for stronger instruments that simultaneously satisfy the exclusion restriction.11\nIn conclusion, I think we\u2019ve learned a lot about instrumental vari- ables and why they are so powerful. The estimators based on this design are capable of identifying causal effects when your data suffer from selection on unobservables. Since selection on unobservables is believed to be very common, this is a very useful methodology for addressing that. But, that said, we also have learned some of the design\u2019s weaknesses, and hence why some people eschew it. Let\u2019s now move to heterogeneous treatment effects so that we can better understand some limitations a bit better.\nNow we turn to a scenario where we relax the assumption that treatment effects are the same for every unit. This is where the poten- tial outcomes notation comes in handy. Instead, we will allow for each unit to have a unique response to the treatment, or\nNote that the treatment effect parameter now differs by individual i. We call this heterogeneous treatment effects.\n11 Good luck with that. Seriously, good luck because\u2014and I\u2019m going out on a limb\nhere\u2014if you had a better instrument, you\u2019d be using it!\nThe main questions we have now are: (1) what is IV estimating when we have heterogeneous treatment effects, and (2) under what assumptions will IV identify a causal effect with heterogeneous treat- ment effects? The reason this matters is that once we introduce het- erogeneous treatment effects, we introduce a distinction between the internal validity of a study and its external validity. Internal validity means our strategy identi\ufb01ed a causal effect for the population we studied. But external validity means the study\u2019s \ufb01nding applied to differ- ent populations (not in the study). The deal is that under homogeneous treatment effects, there is no tension between external and internal validity because everyone has the same treatment effect. But under heterogeneous treatment effects, there is huge tension; the tension is so great, in fact, that it may even undermine the meaningfulness of the relevance of the estimated causal effect despite an otherwise valid IV design!12\nHeterogeneous treatment effects are built on top of the potential outcomes notation, with a few modi\ufb01cations. Since now we have two arguments\u2014D and Z\u2014we have to modify the notation slightly. We say that Y is a function of D and Z as Yi(Di = 0, Zi = 1), which is represented as Yi(0, 1).\nPotential outcomes as we have been using the term refers to the Y variable, but now we have a new potential variable\u2014potential treat- ment status (as opposed to observed treatment status). Here are the characteristics:\n= i\u2019s treatment status when Zi = 1 = i\u2019s treatment status when Zi = 0\nAnd observed treatment status is based on a treatment status switch- ing equations:\n\u2212 D0 i = \u03c00 + \u03c01Zi + \u03c6i\n12 My hunch is economists\u2019 priors assume heterogeneous treatment effects are\nthe rule and constant treatment effects are the exception, but that many others have the\nopposite priors. It\u2019s actually not obvious to me there is a reason to have any particular\npriors, but economists\u2019 training tends to start with heterogeneity.\nwhere \u03c00i = E[D0 \u2212 D0 i i the IV on Di, and E[\u03c01i] = the average causal effect of Zi on Di.\n)", "start_char_idx": 0, "end_char_idx": 4486, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ce1c60e1-c7d7-4f0b-8a5d-8362c1f04442": {"__data__": {"id_": "ce1c60e1-c7d7-4f0b-8a5d-8362c1f04442", "embedding": null, "metadata": {"page number": "376 - 383", "chapter": "Instrumental Variables"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9717ba0e-2ae8-49f8-84fa-cf5372da3f73", "node_type": "4", "metadata": {"page number": "376 - 383", "chapter": "Instrumental Variables"}, "hash": "46f31d7544aca7ed7ff04c44af7a6216b8a499cf532f70ff965194f1b6222577", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d1a42ced-5e84-455c-a33f-94da9da184e1", "node_type": "1", "metadata": {"page number": "376 - 383", "chapter": "Instrumental Variables"}, "hash": "9e3d7e87c517fec2d59879826848836feaf481a071b9fa8f88cf9c8611aa9776", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d30e5654-56e4-4943-8f38-5f0177c293ea", "node_type": "1", "metadata": {}, "hash": "4cf4195d04ff53bcd9a61ce362eb3b2e4175f31bc88bed006b8eca87697c25ba", "class_name": "RelatedNodeInfo"}}, "text": "Potential outcomes as we have been using the term refers to the Y variable, but now we have a new potential variable\u2014potential treat- ment status (as opposed to observed treatment status). Here are the characteristics:\n= i\u2019s treatment status when Zi = 1 = i\u2019s treatment status when Zi = 0\nAnd observed treatment status is based on a treatment status switch- ing equations:\n\u2212 D0 i = \u03c00 + \u03c01Zi + \u03c6i\n12 My hunch is economists\u2019 priors assume heterogeneous treatment effects are\nthe rule and constant treatment effects are the exception, but that many others have the\nopposite priors. It\u2019s actually not obvious to me there is a reason to have any particular\npriors, but economists\u2019 training tends to start with heterogeneity.\nwhere \u03c00i = E[D0 \u2212 D0 i i the IV on Di, and E[\u03c01i] = the average causal effect of Zi on Di.\n) is the heterogeneous causal effect of\nThere are considerably more assumptions necessary for iden- ti\ufb01cation once we introduce heterogeneous treatment effects\u2014 speci\ufb01cally \ufb01ve assumptions. We now review each of them. And to be concrete, I use repeatedly as an example the effect of military service on earnings using a draft lottery as the instrumental variable [Angrist, 1990]. In that paper, Angrist estimated the returns to military service using as an instrument the person\u2019s draft lottery number. The draft lot- tery number was generated by a random number generator and if a person\u2019s number was in a particular range, they were drafted, otherwise they weren\u2019t.\nFirst, as before, there is a stable unit treatment value assumption (SUTVA) that states that the potential outcomes for each person i are unrelated to the treatment status of other individuals. The assumption states that if Zi = Z(cid:12) i and Di = D(cid:12) i, then Yi(D, Z) = Yi(D(cid:12), Z(cid:12)). A violation of SUTVA would be if the status of a person at risk of being drafted was affected by the draft status of others at risk of being drafted. Such spillovers violate SUTVA.13 Not knowing a lot about how that works, I can\u2019t say whether Angrist\u2019s draft study would\u2019ve violated SUTVA. But it seems like he\u2019s safe to me.\ni, then Di(Z) = Di(Z(cid:12)). And if Zi = Z(cid:12)\nSecond, there is the independence assumption. The independence assumption is also sometimes called the \u201cas good as random assign- ment\u201d assumption. It states that the IV is independent of the potential outcomes and potential treatment assignments. Notationally, it is\nThe independence assumption is su\ufb03cient for a causal interpretation of the reduced form:\n13 Probably no other identifying assumption is given shorter shrift than SUTVA.\nRarely is it mentioned in applied studies, let alone taken seriously.\nAnd many people may actually prefer to work just with the instrument and its reduced form because they \ufb01nd independence satisfying and acceptable. The problem, though, is technically the instrument is not the program you\u2019re interested in studying. And there may be many mechanisms leading from the instrument to the outcome that you need to think about (as we will see below). Ultimately, independence is noth- ing more and nothing less than assuming that the instrument itself is random.\nIndependence means that the \ufb01rst stage measures the causal\nAn example of this is if Vietnam conscription for military service was based on randomly generated draft lottery numbers. The assign- ment of draft lottery number was independent of potential earnings or potential military service because it was \u201cas good as random.\u201d\nThird, there is the exclusion restriction. The exclusion restriction states that any effect of Z on Y must be via the effect ofZ on D. In other words, Yi(Di, Zi) is a function of Di only. Or formally:\nAgain, our Vietnam example. In the Vietnam draft lottery, an individ- ual\u2019s earnings potential as a veteran or a non-veteran are assumed to be the same regardless of draft eligibility status. The exclusion restric- tion would be violated if low lottery numbers affected schooling by people avoiding the draft. If this was the case, then the lottery num- ber would be correlated with earnings for at least two cases. One, through the instrument\u2019s effect on military service. And two, through the instrument\u2019s effect on schooling. The implication of the exclusion restriction is that a random lottery number (independence) does not therefore imply that the exclusion restriction is satis\ufb01ed. These are different assumptions.\nFourth is the \ufb01rst stage.", "start_char_idx": 3672, "end_char_idx": 8107, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d30e5654-56e4-4943-8f38-5f0177c293ea": {"__data__": {"id_": "d30e5654-56e4-4943-8f38-5f0177c293ea", "embedding": null, "metadata": {"page number": "376 - 383", "chapter": "Instrumental Variables"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9717ba0e-2ae8-49f8-84fa-cf5372da3f73", "node_type": "4", "metadata": {"page number": "376 - 383", "chapter": "Instrumental Variables"}, "hash": "46f31d7544aca7ed7ff04c44af7a6216b8a499cf532f70ff965194f1b6222577", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ce1c60e1-c7d7-4f0b-8a5d-8362c1f04442", "node_type": "1", "metadata": {"page number": "376 - 383", "chapter": "Instrumental Variables"}, "hash": "083dd03f0fa64c96bb1c87c3e067f7c10917ed8357b6af194ca8d86c7de05eb7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3ec763d7-3cbd-45b9-838d-b4a83c516072", "node_type": "1", "metadata": {}, "hash": "ea44f945b6a4c43b61813a96cd7ffbee7c997bffc0ebd07463f2212a8f7144c1", "class_name": "RelatedNodeInfo"}}, "text": "Or formally:\nAgain, our Vietnam example. In the Vietnam draft lottery, an individ- ual\u2019s earnings potential as a veteran or a non-veteran are assumed to be the same regardless of draft eligibility status. The exclusion restric- tion would be violated if low lottery numbers affected schooling by people avoiding the draft. If this was the case, then the lottery num- ber would be correlated with earnings for at least two cases. One, through the instrument\u2019s effect on military service. And two, through the instrument\u2019s effect on schooling. The implication of the exclusion restriction is that a random lottery number (independence) does not therefore imply that the exclusion restriction is satis\ufb01ed. These are different assumptions.\nFourth is the \ufb01rst stage. IV designs require that Z be correlated\nZ has to have some statistically signi\ufb01cant effect on the average prob- ability of treatment. An example would be having a low lottery number. Does it increase the average probability of military service? If so, then it satis\ufb01es the \ufb01rst stage requirement. Note, unlike independence and exclusion, the \ufb01rst stage is testable as it is based solely on D and Z, both of which you have data on.\nAnd \ufb01nally, there is the monotonicity assumption. This is only strange at \ufb01rst glance but is actually quite intuitive. Monotonicity requires that the instrumental variable (weakly) operate in the same direction on all individual units. In other words, while the instrument may have no effect on some people, all those who are affected are affected in the same direction (i.e., positively or negatively, but not both). We write it out like this:\nEither \u03c01i \u2265 0 for all i or \u03c01i \u2264 0 for all i = 1, . . . , N\nWhat this means, using our military draft example, is that draft eligibil- ity may have no effect on the probability of military service for some people, like patriots, people who love and want to serve their country in the military, but when it does have an effect, it shifts them all into ser- vice, or out of service, but not both. The reason we have to make this assumption is that without monotonicity, IV estimators are not guaran- teed to estimate a weighted average of the underlying causal effects of the affected group.\nIf all \ufb01ve assumptions are satis\ufb01ed, then we have a valid IV strat- egy. But that being said, while valid, it is not doing what it was doing when we had homogeneous treatment effects. What, then, is the IV strategy estimating under heterogeneous treatment effects? Answer: the local average treatment effect (LATE) of D on Y:\nThe LATE parameter is the average causal effect of D on Y for those whose treatment status was changed by the instrument, Z. We know\nthat because notice the difference in the last line: D1 i . So, for those i people for whom that is equal to 1, we calculate the difference in poten- tial outcomes. Which means we are only averaging over treatment effects for whom D1 i . Hence why the parameter we are estimating i is \u201clocal.\u201d\nHow do we interpret Angrist\u2019s estimated causal effect in his Viet- nam draft project? Well, IV estimates the average effect of military service on earnings for the subpopulations who enrolled in military service because of the draft. These are speci\ufb01cally only those peo- ple, though, who would not have served otherwise. It doesn\u2019t identify the causal effect on patriots who always serve, for instance, because = 1 for patri- D1 i ots because they\u2019re patriots! It also won\u2019t tell us the effect of military service on those who were exempted from military service for medical reasons because for these people D1 i The LATE framework has even more jargon, so let\u2019s review it now. The LATE framework partitions the population of units with an instrument into potentially four mutually exclusive groups. Those groups are:\n= 0 for patriots. They always serve! D1 i\n1. Compliers: This is the subpopulation whose treatment status is = 1 affected by the instrument in the correct direction. That is, D1 i and D0 i\n2. De\ufb01ers: This is the subpopulation whose treatment status is = 0 affected by the instrument in the wrong direction. That is, D1 i and D0 i\n3. Never takers: This is the subpopulation of units that never take = the treatment regardless of the value of the instrument.", "start_char_idx": 7346, "end_char_idx": 11602, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3ec763d7-3cbd-45b9-838d-b4a83c516072": {"__data__": {"id_": "3ec763d7-3cbd-45b9-838d-b4a83c516072", "embedding": null, "metadata": {"page number": "376 - 383", "chapter": "Instrumental Variables"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9717ba0e-2ae8-49f8-84fa-cf5372da3f73", "node_type": "4", "metadata": {"page number": "376 - 383", "chapter": "Instrumental Variables"}, "hash": "46f31d7544aca7ed7ff04c44af7a6216b8a499cf532f70ff965194f1b6222577", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d30e5654-56e4-4943-8f38-5f0177c293ea", "node_type": "1", "metadata": {"page number": "376 - 383", "chapter": "Instrumental Variables"}, "hash": "d2bfbe1178bb9e2a987dfbb7ef09b51be08995a73dea6bb21cffd0e7c5e1afaa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea9555e6-6521-4fe4-86a9-3368541bd73f", "node_type": "1", "metadata": {}, "hash": "ed06c2d932f2571e41fc7cfb69c6682fcf5d3031072a4876b8afb5376f683c5a", "class_name": "RelatedNodeInfo"}}, "text": "It also won\u2019t tell us the effect of military service on those who were exempted from military service for medical reasons because for these people D1 i The LATE framework has even more jargon, so let\u2019s review it now. The LATE framework partitions the population of units with an instrument into potentially four mutually exclusive groups. Those groups are:\n= 0 for patriots. They always serve! D1 i\n1. Compliers: This is the subpopulation whose treatment status is = 1 affected by the instrument in the correct direction. That is, D1 i and D0 i\n2. De\ufb01ers: This is the subpopulation whose treatment status is = 0 affected by the instrument in the wrong direction. That is, D1 i and D0 i\n3. Never takers: This is the subpopulation of units that never take = the treatment regardless of the value of the instrument. So, D1 i D0 i\n= 0. They simply never take the treatment.16\n14 We have reviewed the properties of IV with heterogeneous treatment effects\nusing a very simple dummy endogeneous variable, dummy IV, and no additional controls\nexample. The intuition of LATE generalizes to most cases where we have continuous\n15 These are funny people. If they\u2019re drafted, they dodge the draft. But if they\u2019re not\ndrafted, then they voluntarily enroll. In this context, de\ufb01ance seems kind of irrational, but\nthat\u2019s not always the case.\n16 These are draft dodgers. For instance, maybe it\u2019s someone whose doctor gave\nhim a bone-spur diagnosis so he could avoid service.\n4. Always takers: This is the subpopulation of units that always take the treatment regardless of the value of the instrument. So, = 1. They simply always take the instrument.17 D1 i\nAs outlined above, with all \ufb01ve assumptions satis\ufb01ed, IV estimates the average treatment effect for compliers, which is the parameter we\u2019ve called the local average treatment effect. It\u2019s local in the sense that it is average treatment effect to the compliers only. Contrast this with the traditional IV pedagogy with homogeneous treatment effects. In that situation, compliers have the same treatment effects as non- compliers, so the distinction is irrelevant. Without further assumptions, LATE is not informative about effects on never-takers or always-takers because the instrument does not affect their treatment status.\nDoes this matter? Yes, absolutely. It matters because in most applications, we would be mostly interested in estimating the aver- age treatment effect on the whole population, but that\u2019s not usually possible with IV.18\nNow that we have reviewed the basic idea and mechanics of instru- mental variables, including some of the more important tests asso- ciated with it, let\u2019s get our hands dirty with some data. We\u2019ll work with a couple of data sets now to help you better understand how to implement 2SLS in real data.", "start_char_idx": 10790, "end_char_idx": 13575, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ea9555e6-6521-4fe4-86a9-3368541bd73f": {"__data__": {"id_": "ea9555e6-6521-4fe4-86a9-3368541bd73f", "embedding": null, "metadata": {"page number": "383 - 391", "chapter": "Instrumental Variables"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "853cf565-79e6-4840-ba1e-7ef621273528", "node_type": "4", "metadata": {"page number": "383 - 391", "chapter": "Instrumental Variables"}, "hash": "a5fcdbe08ef108a3399ca7750260bb6bdc9a9988fb543cf06fc803f1a1eb1ffc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3ec763d7-3cbd-45b9-838d-b4a83c516072", "node_type": "1", "metadata": {"page number": "376 - 383", "chapter": "Instrumental Variables"}, "hash": "6497dc7ebac1e188313eb13dc273d89f5a587d2193237ad7483ee79e92f3c7ae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "33ce5c06-0491-4537-8529-a4a72d8bcc3c", "node_type": "1", "metadata": {}, "hash": "c237c1dd6d9ab3e81b403df1a16a3f7cd7adf8ffb4879018fc3595c64bf13fb1", "class_name": "RelatedNodeInfo"}}, "text": "Applications:\nCollege in the county. We will once again look at the returns to school- ing since it is such a historically popular topic for causal questions in labor. In this application, we will simply estimate a 2SLS model, calcu- late the \ufb01rst-stage F statistic, and compare the 2SLS results with the OLS results. I will be keeping it simple, because my goal is just to help the reader become familiarized with the procedure.\nThe data comes from the NLS Young Men Cohort of the National Longitudinal Survey. This data began in 1966 with 5,525 men aged\n17 These are our patriots.\nwas worked out in Angrist et al. [1996].\n4. Always takers: This is the subpopulation of units that always take the treatment regardless of the value of the instrument. So, = 1. They simply always take the instrument.17 D1 i\nAs outlined above, with all \ufb01ve assumptions satis\ufb01ed, IV estimates the average treatment effect for compliers, which is the parameter we\u2019ve called the local average treatment effect. It\u2019s local in the sense that it is average treatment effect to the compliers only. Contrast this with the traditional IV pedagogy with homogeneous treatment effects. In that situation, compliers have the same treatment effects as non- compliers, so the distinction is irrelevant. Without further assumptions, LATE is not informative about effects on never-takers or always-takers because the instrument does not affect their treatment status.\nDoes this matter? Yes, absolutely. It matters because in most applications, we would be mostly interested in estimating the aver- age treatment effect on the whole population, but that\u2019s not usually possible with IV.18\nNow that we have reviewed the basic idea and mechanics of instru- mental variables, including some of the more important tests asso- ciated with it, let\u2019s get our hands dirty with some data. We\u2019ll work with a couple of data sets now to help you better understand how to implement 2SLS in real data.\nCollege in the county. We will once again look at the returns to school- ing since it is such a historically popular topic for causal questions in labor. In this application, we will simply estimate a 2SLS model, calcu- late the \ufb01rst-stage F statistic, and compare the 2SLS results with the OLS results. I will be keeping it simple, because my goal is just to help the reader become familiarized with the procedure.\nThe data comes from the NLS Young Men Cohort of the National Longitudinal Survey. This data began in 1966 with 5,525 men aged\n17 These are our patriots.\nwas worked out in Angrist et al. [1996].\n14\u201324 and continued to follow up with them through 1981. These data come from 1966, the baseline survey, and there are a number of questions related to local labor-markets. One of them is whether the respondent lives in the same county as a 4-year (and a 2-year) college. Card [1995] is interested in estimating the following regression\nwhere Y is log earnings, S is years of schooling, X is a matrix of exoge- nous covariates, and \u03b5 is an error term that contains, among other things, unobserved ability. Under the assumption that \u03b5 contains ability, and ability is correlated with schooling, then C(S, \u03b5) (cid:9)= 0 and therefore schooling is biased. Card [1995] proposes therefore an instrumental variables strategy whereby he will instrument for schooling with the college-in-the-county dummy variable.\nIt is worth asking ourselves why the presence of a 4-year col- lege in one\u2019s county would increase schooling. The main reason I can think of is that the presence of the 4-year college increases the likelihood of going to college by lowering the costs, since the stu- dent can live at home. This therefore means that we are selecting on a group of compliers whose behavior is affected by the variable. Some kids, in other words, will always go to college regardless of whether a college is in their county, and some will never go despite the presence of the nearby college. But there may exist a group of compliers who go to college only because their county has a col- lege, and if I\u2019m right that this is primarily picking up people going because they can attend while living at home, then it\u2019s necessarily peo- ple at some margin who attend only because college became slightly cheaper. This is, in other words, a group of people who are liquid- ity constrained.", "start_char_idx": 0, "end_char_idx": 4331, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "33ce5c06-0491-4537-8529-a4a72d8bcc3c": {"__data__": {"id_": "33ce5c06-0491-4537-8529-a4a72d8bcc3c", "embedding": null, "metadata": {"page number": "383 - 391", "chapter": "Instrumental Variables"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "853cf565-79e6-4840-ba1e-7ef621273528", "node_type": "4", "metadata": {"page number": "383 - 391", "chapter": "Instrumental Variables"}, "hash": "a5fcdbe08ef108a3399ca7750260bb6bdc9a9988fb543cf06fc803f1a1eb1ffc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea9555e6-6521-4fe4-86a9-3368541bd73f", "node_type": "1", "metadata": {"page number": "383 - 391", "chapter": "Instrumental Variables"}, "hash": "60a964e1e692c9ccc045de808d737e496f5f62979bcdeac857355565d210e034", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b3e6d18d-d36c-4eee-ba5b-12cf20992371", "node_type": "1", "metadata": {}, "hash": "9968620d209d9ee29f38246727cb5f7fd2c0241c1bd496105583217b020cba9f", "class_name": "RelatedNodeInfo"}}, "text": "The main reason I can think of is that the presence of the 4-year college increases the likelihood of going to college by lowering the costs, since the stu- dent can live at home. This therefore means that we are selecting on a group of compliers whose behavior is affected by the variable. Some kids, in other words, will always go to college regardless of whether a college is in their county, and some will never go despite the presence of the nearby college. But there may exist a group of compliers who go to college only because their county has a col- lege, and if I\u2019m right that this is primarily picking up people going because they can attend while living at home, then it\u2019s necessarily peo- ple at some margin who attend only because college became slightly cheaper. This is, in other words, a group of people who are liquid- ity constrained. And if we believe the returns to schooling for this group are different from those of the always-takers, then our esti- mates may not represent the ATE. Rather, they would represent the LATE. But in this case, that might actually be an interesting parameter since it gets at the issue of lowering costs of attendance for poorer families.\nHere we will do some simple analysis based on Card [1995].\n1 use https://github.com/scunning1975/mixtape/raw/master/card.dta, clear 2 reg lwage educ exper black south married smsa 3 ivregress 2sls lwage (educ=nearc4) exper black south married smsa, first 4 reg educ nearc4 exper black south married smsa 5 test nearc4\n1 library(AER) 2 library(haven) 3 library(tidyverse) 4 5 read_data <- function(df) 6 { 7 8 9 10 11 } 12 13 card <- read_data(\"card.dta\") 14 15 #Define variable 16 #(Y1 = Dependent Variable, Y2 = endogenous variable, X1 = exogenous variable,\ndf, sep = \"\")\nNote: Standard errors in parentheses. *p < 0.10. **p < 0.05. ***p < 0.01.\nOur results from this analysis have been arranged into Table 57. First, we report our OLS results. For every one year additional of school- ing, respondents\u2019 earnings increase by approximately 7.1%. Next we estimated 2SLS using the ivregress 2sls command in Stata. Here we \ufb01nd a much larger return to schooling than we had found using OLS\u2014 around 75% larger in fact. But let\u2019s look at the \ufb01rst stage. We \ufb01nd that the college in the county is associated with 0.327 more years of schooling. This is highly signi\ufb01cant (p < 0.001). The F statistic exceeds 15, suggesting we don\u2019t have a weak instrument problem. The return to schooling associated with this 2SLS estimate is 0.124\u2014that is, for every additional year of schooling, earnings increase by 12.4%. Other covariates are listed if you\u2019re interested in studying them as well.\nWhy would the return to schooling be so much larger for the com- pliers than for the general population? After all, we showed earlier that if this was simply ability bias, then we\u2019d expect the 2SLS coe\ufb03cient to be smaller than the OLS coe\ufb03cient, because ability bias implies that the coe\ufb03cient on schooling is too large. Yet we\u2019re \ufb01nding the oppo- site. So a couple of things it could be. First, it could be that schooling has measurement error. Measurement error would bias the coe\ufb03cient toward zero, and 2SLS would recover its true value. But I \ufb01nd this expla- nation to be unlikely, because I don\u2019t foresee people really not knowing with accuracy how many years of schooling they currently have. Which leads us to the other explanation, and that is that compliers have larger returns to schooling. But why would this be the case? Assuming that the exclusion restriction holds, then why would compliers, returns be so much larger? We\u2019ve already established that these people are likely being shifted into more schooling because they live with their par- ents, which suggests that the college is lowering the marginal cost of going to college. All we are left saying is that for some reason, the higher marginal cost of attending college is causing these people to underinvest in schooling; that in fact their returns are much higher.\nFulton Fish Markets.", "start_char_idx": 3478, "end_char_idx": 7501, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b3e6d18d-d36c-4eee-ba5b-12cf20992371": {"__data__": {"id_": "b3e6d18d-d36c-4eee-ba5b-12cf20992371", "embedding": null, "metadata": {"page number": "383 - 391", "chapter": "Instrumental Variables"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "853cf565-79e6-4840-ba1e-7ef621273528", "node_type": "4", "metadata": {"page number": "383 - 391", "chapter": "Instrumental Variables"}, "hash": "a5fcdbe08ef108a3399ca7750260bb6bdc9a9988fb543cf06fc803f1a1eb1ffc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "33ce5c06-0491-4537-8529-a4a72d8bcc3c", "node_type": "1", "metadata": {"page number": "383 - 391", "chapter": "Instrumental Variables"}, "hash": "c212a9f69ebadb1eec4a1babfd64d892b6d1793789a49b0189be402ace4a7309", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "94a1193d-71ce-4c7a-a209-090d19a983e2", "node_type": "1", "metadata": {}, "hash": "0af5b4521303ec9fe16b37276ea6c6190bb4bbe4c5a1830d92924ffbe455d1b8", "class_name": "RelatedNodeInfo"}}, "text": "Measurement error would bias the coe\ufb03cient toward zero, and 2SLS would recover its true value. But I \ufb01nd this expla- nation to be unlikely, because I don\u2019t foresee people really not knowing with accuracy how many years of schooling they currently have. Which leads us to the other explanation, and that is that compliers have larger returns to schooling. But why would this be the case? Assuming that the exclusion restriction holds, then why would compliers, returns be so much larger? We\u2019ve already established that these people are likely being shifted into more schooling because they live with their par- ents, which suggests that the college is lowering the marginal cost of going to college. All we are left saying is that for some reason, the higher marginal cost of attending college is causing these people to underinvest in schooling; that in fact their returns are much higher.\nFulton Fish Markets. The second exercise that we\u2019ll be doing is based on Graddy [2006]. My understanding is that Graddy collected these data herself by recording prices of \ufb01sh at the actual Fulton Fish Mar- ket. I\u2019m not sure if that is true, but I like to believe it\u2019s true. Anyhow, the Fulton Fish Market operated in New York on Fulton Street for 150 years. In November 2005, it moved from Lower Manhattan to a large facility building for the market in the South Bronx. At the time when Graddy (2006) was published, the market was called the New Fulton Fish Market. It\u2019s one of the world\u2019s largest \ufb01sh markets, second only to the Tsukiji in Tokyo.\nFish are heterogeneous, highly differentiated products. There are anywhere between one hundred and three hundred varieties of \ufb01sh sold at the market. There are over \ufb01fteen varieties of shrimp alone. Within each variety, there\u2019s small \ufb01sh, large \ufb01sh, medium \ufb01sh, \ufb01sh just caught, \ufb01sh that have been around a while. There\u2019s so much hetero- geneity that customers often want to examine \ufb01sh personally. You get the picture. This \ufb01sh market functions just like a two-sided platform\nmatching buyers to sellers, which is made more e\ufb03cient by the thick- ness the market produces. It\u2019s not surprising, therefore, that Graddy found the market such an interesting thing to study.\nLet\u2019s move to the data. I want us to estimate the price elasticity of demand for \ufb01sh, which makes this problem much like the problem that Philip Wright faced in that price and quantity are determined simulta- neously. The elasticity of demand is a sequence of quantity and price pairs, but with only one pair observed at a given point in time. In that sense, the demand curve is itself a sequence of potential outcomes (quantity) associated with different potential treatments (price). This means the demand curve is itself a real object, but mostly unobserved. Therefore, to trace out the elasticity, we need an instrument that is cor- related with supply only. Graddy proposes a few of them, all of which have to do with the weather at sea in the days before the \ufb01sh arrived to market.\nThe \ufb01rst instrument is the average maximum wave height in the\nprevious two days. The model we are interested in estimating is:\nwhere Q is log quantity of whiting sold in pounds, P is log average daily price per pound, X are day of the week dummies and a time trend, and \u03b5 is the structural error term. Table 58 presents the results from estimat- ing this equation with OLS (\ufb01rst column) and 2SLS (second column). The OLS estimate of the elasticity of demand is \u22120.549. It could\u2019ve been anything given price is determined by how many sellers and how many buyers there are at the market on any given day. But when we use the average wave height as the instrument for price, we get a \u22120.96 price elasticity of demand. A 10% increase in the price causes quan- tity to decrease by 9.6%. The instrument is strong (F > 22). For every one-unit increase in the wave-height, price rose 10%.\nI suppose the question we have to ask ourselves, though, is what exactly is this instrument doing to supply. What are higher waves doing exactly? They are making it more di\ufb03cult to \ufb01sh, but are they also changing the composition of the \ufb01sh caught?", "start_char_idx": 6591, "end_char_idx": 10715, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "94a1193d-71ce-4c7a-a209-090d19a983e2": {"__data__": {"id_": "94a1193d-71ce-4c7a-a209-090d19a983e2", "embedding": null, "metadata": {"page number": "383 - 391", "chapter": "Instrumental Variables"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "853cf565-79e6-4840-ba1e-7ef621273528", "node_type": "4", "metadata": {"page number": "383 - 391", "chapter": "Instrumental Variables"}, "hash": "a5fcdbe08ef108a3399ca7750260bb6bdc9a9988fb543cf06fc803f1a1eb1ffc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b3e6d18d-d36c-4eee-ba5b-12cf20992371", "node_type": "1", "metadata": {"page number": "383 - 391", "chapter": "Instrumental Variables"}, "hash": "73635f80a09c67f76ec0ed9de8f9b0b7b68c4898264079e24b4a483ac83a5d7a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5f78f4f5-8740-4a68-8f15-33f4ce983b74", "node_type": "1", "metadata": {}, "hash": "29034469a221ab59333a7ac1d628a85bf6a2639b029fb1911ed3744d2b3eed05", "class_name": "RelatedNodeInfo"}}, "text": "The OLS estimate of the elasticity of demand is \u22120.549. It could\u2019ve been anything given price is determined by how many sellers and how many buyers there are at the market on any given day. But when we use the average wave height as the instrument for price, we get a \u22120.96 price elasticity of demand. A 10% increase in the price causes quan- tity to decrease by 9.6%. The instrument is strong (F > 22). For every one-unit increase in the wave-height, price rose 10%.\nI suppose the question we have to ask ourselves, though, is what exactly is this instrument doing to supply. What are higher waves doing exactly? They are making it more di\ufb03cult to \ufb01sh, but are they also changing the composition of the \ufb01sh caught? If so, then it would seem that the exclusion restriction is violated because that would mean the wave height is directly causing \ufb01sh composition to change, which will directly determine quantities bought and sold.\nAverage wave height Robust standard error F statistic for IV in \ufb01rst stage N Mean dependent variable SD dependent variable\nNote: Standard errors in parentheses. *p < 0.10. **p < 0.05. ***p < 0.01.\nNow let\u2019s look at a different instrument: wind speed (Table 59). Speci\ufb01cally, it\u2019s the three-day lagged maximum wind speed. We present these results in Table 58. Here we see something we did not see before, which is that this is a weak instrument. The F statistic is less than 10 (approximately 6.5). And correspondingly, the estimated elasticity is twice as large as what we found with wave height. Thus, we know from our earlier discussion of weak instruments that this estimate is likely severely biased, and therefore less reliable than the previous one\u2014even though the previous one itself (1) may not convinc- ingly satisfy the exclusion restriction and (2) is at best a LATE relevant to compliers only. But as we\u2019ve said, if we think that the compliers\u2019\nNote: Standard errors in parentheses. *p < 0.10, **p < 0.05, ***p < 0.01\ncausal effects are similar to that of the broader population, then the LATE may itself be informative and useful.", "start_char_idx": 10000, "end_char_idx": 12073, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5f78f4f5-8740-4a68-8f15-33f4ce983b74": {"__data__": {"id_": "5f78f4f5-8740-4a68-8f15-33f4ce983b74", "embedding": null, "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5b71990f-327a-41b7-82e3-7cd6ba4744ae", "node_type": "4", "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "hash": "468c66e601983f967c99b39a1333e32fd92b79c7a42b247cfc9afc6446eadace", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "94a1193d-71ce-4c7a-a209-090d19a983e2", "node_type": "1", "metadata": {"page number": "383 - 391", "chapter": "Instrumental Variables"}, "hash": "c3d33c18b756ffafbeaf12d1df5c7f8e3fa38881a331100d0eb6ebaf4664f0ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9cca0889-6813-42c0-85a0-9bf29be18983", "node_type": "1", "metadata": {}, "hash": "8f62bcd819b2b9241acfa758dcbcf7db5faccd1275071bfee4d79de821ec609f", "class_name": "RelatedNodeInfo"}}, "text": "Popular IV Designs:\nInstrumental variables is a strategy to adopt when you have a good instrument, and so in that sense, it is a very general design that can be used in just about any context. But over the years, certain types of IV strategies have been used so many times that they constitute their own designs. And from repetition and re\ufb02ection, we have a better understanding of how these speci\ufb01c IV designs do and do not work.\nNote: Standard errors in parentheses. *p < 0.10, **p < 0.05, ***p < 0.01\ncausal effects are similar to that of the broader population, then the LATE may itself be informative and useful.\nInstrumental variables is a strategy to adopt when you have a good instrument, and so in that sense, it is a very general design that can be used in just about any context. But over the years, certain types of IV strategies have been used so many times that they constitute their own designs. And from repetition and re\ufb02ection, we have a better understanding of how these speci\ufb01c IV designs do and do not work.\nLet\u2019s discuss three such popular designs: the lottery design, the judge \ufb01xed effects design, and Bartik instruments.\nLotteries. Previously, we reviewed the use of IV in identifying causal effects when some regressor is endogenous in observational data. But one particular kind of IV application is randomized trials. In many randomized trials, participation is voluntary among those randomly chosen to be in the treatment group. On the other hand, persons in the control group usually don\u2019t have access to the treatment. Only those who are particularly likely to bene\ufb01t from treatment therefore will prob- ably take up treatment, which almost always leads to positive selection bias. If you compare means between treated and untreated individuals using OLS, you will obtain biased treatment effects even for the ran- domized trial because of noncompliance. A solution to the problems of least squares in this application is to instrument for Medicaid with whether you were offered treatment and estimate the LATE. Thus even when treatment itself is randomly assigned, it is common for people to use a randomized lottery as an instrument for participation. For a mod- ern example of this, see Baicker et al. [2013], who used the randomized lottery to be enrolled in Oregon\u2019s Medicaid program as an instrument for being on Medicaid. Let\u2019s discuss the Oregon Medicaid studies now, as they are excellent illustrations of the lottery IV design.\nWhat are the effects of expanding access to public health insur- ance for low-income adults? Are they positive or negative? Are they large or small? Surprisingly, we have not historically had reliable esti- mates for these very basic questions because we lacked the kind of experiment needed to make claims one way or another. The limited existing evidence was suggestive, with a lot of uncertainty. Observa- tional studies are confounded by selection into health insurance, and the quasi-experimental evidence tended to only focus on the elderly and small children. There has been only one randomized experiment in a developed country, and it was the RAND health insurance experiment in the 1970s. This was an important, expensive, ambitious experiment, but it only randomized cost-sharing\u2014not coverage itself.\nBut in the 2000s, Oregon chose to expand its Medicaid program for poor adults by making it more generous. Adults aged 19\u201364 with income less than 100% of the federal poverty line were eligible so long\nas they weren\u2019t eligible for other similar programs. They also had to be uninsured for fewer than six months and be a legal US resident. The program was called the Oregon Health Plan Standard and it provided comprehensive coverage (but no dental or vision) and minimum cost- sharing. It was similar to other states in payments and management, and the program was closed to new enrollment in 2004.\nThe expansion is popularly known as the Oregon Medicaid Exper- iment because the state used a lottery to enroll volunteers. For \ufb01ve weeks, people were allowed to sign up for Medicaid. The state used heavy advertising to make the program salient. There were low barri- ers to signing up and no eligibility requirements for prescreening. The state in March to October 2008 randomly drew 30,000 people out of a list of 85,000. Those selected were given a chance to apply. If they did apply, then their entire household was enrolled, so long as they returned the application within 45 days. Out of this original 30,000, only 10,000 people were enrolled.\nA team of economists became involved with this project early on, out of which several in\ufb02uential papers were written.", "start_char_idx": 0, "end_char_idx": 4646, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9cca0889-6813-42c0-85a0-9bf29be18983": {"__data__": {"id_": "9cca0889-6813-42c0-85a0-9bf29be18983", "embedding": null, "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5b71990f-327a-41b7-82e3-7cd6ba4744ae", "node_type": "4", "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "hash": "468c66e601983f967c99b39a1333e32fd92b79c7a42b247cfc9afc6446eadace", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5f78f4f5-8740-4a68-8f15-33f4ce983b74", "node_type": "1", "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "hash": "e83f02724d9ef37eb7023b9a0162b2a17812567e5a702f1965d3c13d6d07ef77", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ccb1bfc1-8657-457d-b5f2-d1cfb7848dfa", "node_type": "1", "metadata": {}, "hash": "235093b53eac56fd23d59350777ccbf60baece4c1dc828b789b41d71b09dac84", "class_name": "RelatedNodeInfo"}}, "text": "The expansion is popularly known as the Oregon Medicaid Exper- iment because the state used a lottery to enroll volunteers. For \ufb01ve weeks, people were allowed to sign up for Medicaid. The state used heavy advertising to make the program salient. There were low barri- ers to signing up and no eligibility requirements for prescreening. The state in March to October 2008 randomly drew 30,000 people out of a list of 85,000. Those selected were given a chance to apply. If they did apply, then their entire household was enrolled, so long as they returned the application within 45 days. Out of this original 30,000, only 10,000 people were enrolled.\nA team of economists became involved with this project early on, out of which several in\ufb02uential papers were written. I\u2019ll now discuss some of the main results from Finkelstein et al. [2012] and Baicker et al. [2013]. The authors of these studies sought to study a broad range of outcomes that might be plausibly affected by health insurance\u2014 from \ufb01nancial outcomes, to health-care utilization, to health outcomes. The data needed for these outcomes were meticulously collected from third parties. For instance, the pre-randomization demographic infor- mation was available from the lottery sign-up. The state administra- tive records on Medicaid enrollment were also collected and became the primary measure of a \ufb01rst stage (i.e., insurance coverage). And outcomes were collected from administrative sources (e.g., hospi- tal discharge, mortality, credit), mail surveys, and in-person survey and measurement (e.g., blood samples, body mass index, detailed questionnaires).\nThe empirical framework in these studies is a straightforward IV design. Sometimes they estimated the reduced form, and sometimes they estimated the full 2SLS model. The two-stages were:\nwhere the \ufb01rst equation is the \ufb01rst stage (insurance regressed onto the lottery outcome plus a bunch of covariates), and the second stage regresses individual-level outcomes onto predicted insurance (plus all those controls). We already know that so long as the \ufb01rst stage is strong, then the F statistic will be large, and the \ufb01nite sample bias lessens.\nThe effects of winning the lottery had large effects on enrollment. We can see the results of the \ufb01rst stage in Table 60. They used different samples, but the effect sizes were similar. Winning the lottery raised the probability of being enrolled on Medicaid by 26% and raised the number of months of being on Medicaid from 3.3 to 4 months.\nAcross the two papers, the authors looked at the effect of Med- icaid\u2019s health insurance coverage on a variety of outcomes includ- ing \ufb01nancial health, mortality, and health-care utilization, but I will review only a few here. In Table 61, the authors present two regres- sion models: column 2 is the intent to treat estimates, which is the reduced form model, and column 3 is the local average treatment effect estimate, which is our full instrumental variables speci\ufb01cation. Interestingly, Medicaid increased the number of hospital admissions but had no effect on emergency room visits. The effect on emergency rooms, in fact, is not signi\ufb01cant, but the effect on non-emergency-room admissions is positive and signi\ufb01cant. This is interesting because it appears that Medicaid is increasing hospital admission without putting additional strain on emergency rooms, which already have scarce resources.\nHave a usual place of care Have a personal doctor Got all needed healthcare Got all needed prescriptions Satis\ufb01ed with quality of care\nWhat other kinds of health-care utilization are we observing in Medicaid enrollees? Let\u2019s look at Table 62, which has \ufb01ve health-care utilization outcomes. Again, I will focus on column 3, which is the LATE estimates. Medicaid enrollees were 34% more likely to have a usual place of care, 28% to have a personal doctor, 24% to complete their health-care needs, 20% more likely to get all needed prescriptions and 14% increased satisfaction with the quality of their care.\nBut Medicaid is not merely a way to increase access to health care; it also functions effectively as health care insurance in the event of catastrophic health events. And one of the most widely circulated results of the experiment was the \ufb01nding that Medicaid had on \ufb01nan- cial outcomes. In Table 63 we see that one of the main effects was reduction in personal debt (by $390) and reducing debt going to debt collection.", "start_char_idx": 3879, "end_char_idx": 8314, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ccb1bfc1-8657-457d-b5f2-d1cfb7848dfa": {"__data__": {"id_": "ccb1bfc1-8657-457d-b5f2-d1cfb7848dfa", "embedding": null, "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5b71990f-327a-41b7-82e3-7cd6ba4744ae", "node_type": "4", "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "hash": "468c66e601983f967c99b39a1333e32fd92b79c7a42b247cfc9afc6446eadace", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9cca0889-6813-42c0-85a0-9bf29be18983", "node_type": "1", "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "hash": "b818470536e0e913483004076f441a3eb5bdac56be9d82f1ba0050aa7ec522a8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "42b578e5-3b92-462e-b820-59d7d5036cef", "node_type": "1", "metadata": {}, "hash": "1bf357c681bd430738c23a2df57a30ac3545768956bf16bbbe6853b6a305afe3", "class_name": "RelatedNodeInfo"}}, "text": "Let\u2019s look at Table 62, which has \ufb01ve health-care utilization outcomes. Again, I will focus on column 3, which is the LATE estimates. Medicaid enrollees were 34% more likely to have a usual place of care, 28% to have a personal doctor, 24% to complete their health-care needs, 20% more likely to get all needed prescriptions and 14% increased satisfaction with the quality of their care.\nBut Medicaid is not merely a way to increase access to health care; it also functions effectively as health care insurance in the event of catastrophic health events. And one of the most widely circulated results of the experiment was the \ufb01nding that Medicaid had on \ufb01nan- cial outcomes. In Table 63 we see that one of the main effects was reduction in personal debt (by $390) and reducing debt going to debt collection. The authors also found reductions in out-of-pocket medical expenses, and medical expenses, borrowing money or skipping bills for medical expenses, and whether they refused medical treatment due to medical debt.\nHad a bankruptcy Had a collection Had a medical collection Had non-medical collection Money owed medical collection\nHealth good, very good, or excellent Health stable or improving Depression screen NEGATIVE CDC healthy days (physical) CDC healthy days (mental)\nBut the effect on health outcomes was a little unclear from this study. The authors \ufb01nd self-reported health outcomes to be improv- ing, as well as a reduction in depression. They also \ufb01nd more healthy physical and mental health days. But the effects are overall small. Furthermore, they ultimately do not \ufb01nd that Medicaid had any effect on mortality\u2014a result we will return to again in the difference-in- differences chapter.\nIn conclusion, we see a powerful use of IV in the assignment of lot- teries to recipients. The lotteries function as instruments for treatment assignment, which can then be used to estimate some local aver- age treatment effect. This is incredibly useful in experimental designs if only because humans often refuse to comply with their treatment assignment or even participate in the experiment altogether!\nJudge \ufb01xed effects. A second IV design that has become extremely popular in recent years is the \u201cjudge \ufb01xed effects\u201d design. You may also hear it called the \u201cleniency design,\u201d but because the applications so often involve judges, it seems the former name has stuck. A search\nFigure 51. Randomized judge assignment. Although justice is supposedly blind, judges\nare complex bundles of characteristics that affect their judgments. Artwork by Seth Hahne \u00a92020.\nThe concept of the judge \ufb01xed effects design is that there exists a narrow pipeline through which all individuals must pass, numerous randomly assigned decision-makers blocking the individuals\u2019 passage who assign a treatment to the individuals and discretion among the decision-makers. When all three are there, you probably have the mak- ings of a judge \ufb01xed effects design. The reason the method is called the judge \ufb01xed effects design is because it has traditionally exploited a feature in American jurisprudence where jurisdictions will randomly assign judges to defendants. In Harris County, Texas, for instance, they used to use a bingo machine to assign defendants to one of dozens of courts [Mueller-Smith, 2015].\nThe \ufb01rst paper to recognize that there were systematic differences in judge sentencing behavior was an article by Gaudet et al. [1933]. The authors were interested in better understanding what, other than guilt, determined the sentencing outcomes of defendants. They decided to focus on the judge in part because judges were being randomly\nFigure 52. Variation in judge sentencing outcomes. This \ufb01gure originally appeared in Frederick J. Gaudet, George S. Harris, Charles W. St. John, Individual Differ- ences in the Sentencing Tendencies of Judges, 23, J. Crim. L. & Criminol- ogy, 811 (1933). Reprinted by special permission of Northwestern University Pritzker School of Law, Journal of Criminal Law and Criminology.\nrotated to defendants. And since they were being \u201cby chance\u201d rotated to defendants, in a large sample, the characteristics of the defendants should\u2019ve remained approximately the same across all judges. Any dif- ferences in sentencing outcomes, therefore, wouldn\u2019t be because of the underlying charge or even the defendant\u2019s guilt, but rather, would be connected to the judge. See Figure 51 for a beautiful drawing of this identi\ufb01cation strategy based on over 7,000 hand collected cases showing systematic differences in judge sentencing behavior.", "start_char_idx": 7506, "end_char_idx": 12066, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "42b578e5-3b92-462e-b820-59d7d5036cef": {"__data__": {"id_": "42b578e5-3b92-462e-b820-59d7d5036cef", "embedding": null, "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5b71990f-327a-41b7-82e3-7cd6ba4744ae", "node_type": "4", "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "hash": "468c66e601983f967c99b39a1333e32fd92b79c7a42b247cfc9afc6446eadace", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ccb1bfc1-8657-457d-b5f2-d1cfb7848dfa", "node_type": "1", "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "hash": "bcca0b7ecbe130772deb1e0175386ed75337f5c56855fb982240dfb2fef33874", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f7247636-53fa-40f2-8ac3-f69dd40f826c", "node_type": "1", "metadata": {}, "hash": "a9bf7a566dcab71872ba4ee772b2577e454b6e4ff310aea6eb4e9b627ad4a9c6", "class_name": "RelatedNodeInfo"}}, "text": "John, Individual Differ- ences in the Sentencing Tendencies of Judges, 23, J. Crim. L. & Criminol- ogy, 811 (1933). Reprinted by special permission of Northwestern University Pritzker School of Law, Journal of Criminal Law and Criminology.\nrotated to defendants. And since they were being \u201cby chance\u201d rotated to defendants, in a large sample, the characteristics of the defendants should\u2019ve remained approximately the same across all judges. Any dif- ferences in sentencing outcomes, therefore, wouldn\u2019t be because of the underlying charge or even the defendant\u2019s guilt, but rather, would be connected to the judge. See Figure 51 for a beautiful drawing of this identi\ufb01cation strategy based on over 7,000 hand collected cases showing systematic differences in judge sentencing behavior.\nFigure 52 is a pretty typical graphic for any paper on judge \ufb01xed effects showing the variation in the judge\u2019s propensities; it\u2019s just kind of interesting that it appears from the very beginning back in 1933. And as you can see in Figure 52, there is weirdly enough a lot of vari- ation in the sentencing propensities across the six judges. Judge 2 imposed imprisonment in only 33.6% of his cases, whereas Judge 4 imposed imprisonment in a whopping 57.7%. And since they are all seeing on average the same defendants, it can\u2019t be that Judge 4 is simply seeing worse cases. Rather, there appears to be something sys- tematic, like a tendency for certain judges to always judge defendants more harshly. But why are they like this? Gaudet et al. [1933] offer the following conjecture:\nPerhaps the most interesting thing to be noticed in these graphs is the fact that the sentencing tendency of the judge seems to be\nfairly well determined before he sits on the bench. In other words what determines whether a judge will be severe or lenient is to be found in the environment to which the judge has been subjected previous to his becoming an administrator of sentences.\nBut the main takeaway is that the authors were the \ufb01rst to dis- cover that the leniency or severity of the judge, and not merely the defendant\u2019s own guilt, plays a signi\ufb01cant role apparently in the \ufb01nal determination of a case against the defendant. The authors write:\nThe authors wish to point out that these results tend to show that some of our previous studies in the \ufb01elds of criminology and penology are based upon very unreliable evidence if our results are typical of sentencing tendencies. In other words, what type of sentence received by a prisoner may be either an indication of the seriousness of his crime or of the severity of the judge. [815]\nThe next mention of the explicit judge \ufb01xed effects design is in the Imbens and Angrist [1994] article decomposing IV into the LATE parameter using potential outcomes notation. At the conclusion of their article, they provide three examples of IV designs that may or may not \ufb01t the \ufb01ve identifying assumptions of IV that I discussed earlier. They write:\nExample 2 (Administrative Screening): Suppose applicants for a social program are screened by two o\ufb03cials. The two o\ufb03cials are likely to have different admission rates, even if the stated admis- sion criteria are identical. Since the identity of the o\ufb03cial is proba- bly immaterial to the response, it seems plausible that Condition 1 [independence] is satis\ufb01ed. The instrument is binary so Condition 3 is trivially satis\ufb01ed. However, Condition 2 [monotonicity] requires that if o\ufb03cial A accepts applicants with probability P(0), and o\ufb03cial B accepts people with probability (P1) >P(0), o\ufb03cial B must accept any applicant who would have been accepted by o\ufb03cial A. This is unlikely to hold if admission is based on a number of criteria. There- fore, in this example we cannot use Theorem 1 to identify a local\naverage treatment effect nonparametrically despite the presence of an instrument satisfying Condition 1 [independence]. [472]\nWhile the \ufb01rst time we see the method used for any type of empirical identi\ufb01cation is Waldfogel [1995], the \ufb01rst explicit IV strat- egy is a paper ten years later by Kling [2006], who used randomized judge assignment with judge propensities to instrument for incarcera- tion length.", "start_char_idx": 11280, "end_char_idx": 15463, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f7247636-53fa-40f2-8ac3-f69dd40f826c": {"__data__": {"id_": "f7247636-53fa-40f2-8ac3-f69dd40f826c", "embedding": null, "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5b71990f-327a-41b7-82e3-7cd6ba4744ae", "node_type": "4", "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "hash": "468c66e601983f967c99b39a1333e32fd92b79c7a42b247cfc9afc6446eadace", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "42b578e5-3b92-462e-b820-59d7d5036cef", "node_type": "1", "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "hash": "8be8ba6d1a062bbbf8d7466fb6c082eed52f2586ee8a6a0aef4c295c81d6a8c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d4224a67-d264-48d0-b4d0-c1c499a35b1e", "node_type": "1", "metadata": {}, "hash": "d6beece6ba9cbcd0c63eb19d4eca68b9368ae3267ec1ba4426e211356f570858", "class_name": "RelatedNodeInfo"}}, "text": "There- fore, in this example we cannot use Theorem 1 to identify a local\naverage treatment effect nonparametrically despite the presence of an instrument satisfying Condition 1 [independence]. [472]\nWhile the \ufb01rst time we see the method used for any type of empirical identi\ufb01cation is Waldfogel [1995], the \ufb01rst explicit IV strat- egy is a paper ten years later by Kling [2006], who used randomized judge assignment with judge propensities to instrument for incarcera- tion length. He then linked defendants to employment and earnings records, which he then used to estimate the causal effect of incar- ceration on labor-market outcomes. He ultimately \ufb01nds no adverse effects on labor-market consequences from longer sentences in the two states he considers.\nBut this question was revisited by Mueller-Smith [2015], who used Harris County, Texas, for his location. Harris County has dozens of courts and defendants are randomly assigned to one of them. Mueller- Smith linked defendant outcomes to a variety of labor-market and crim- inal outcomes, and came to the opposite conclusion as Kling [2006]. Mueller-Smith [2015] \ufb01nds that incarceration generates net increases in the frequency and severity of recidivism, worsens labor-market out- comes, and increases defendant\u2019s dependence on public assistance. Judicial severity causing adverse consequences on defendants is practically a hallmark of the judge \ufb01xed effects literature. Just to name a few such examples, there is the \ufb01nding that less allowance of Chapter 13 bankruptcy worsens future \ufb01nancial events [Dobbie et al., 2017], racial bias among bail judges [Arnold et al., 2018], pretrial deten- tion having higher rates of guilty pleas, conviction, recidivism, and worsened labor-market outcomes [Dobbie et al., 2018; Leslie and Pope, 2018; Stevenson, 2018], juvenile incarceration worsening high school outcomes and adult recidivism rates [Aizer and Doyle, 2015], fos- ter care raising juvenile delinquency, teen pregnancy, and worsening future employment [Doyle, 2007], foster care increasing adult crime [Doyle, 2008], and countless others. But there are a few exceptions. For instance, Norris et al. [2020] \ufb01nd bene\ufb01cial effects to children when the marginal siblings and parents are incarcerated.\nThe three main identifying assumptions that should be on the researcher\u2019s mind when attempting to implement a judge \ufb01xed effects design are the independence assumption, exclusion restriction, and\nthe monotonicity assumption. Let\u2019s discuss them each at a time because, in some scenarios, one of these may be more credible than the other.\nThe independence assumption seems to be satis\ufb01ed in many cases because the administrators in question are literally being ran- domly assigned to individual cases. As such, our instrument\u2014which is sometimes modeled as the average propensity of the judge exclud- ing the case in question or simply as a series of judge \ufb01xed effects (which, as I\u2019ll mention in a moment, turns out to be equivalent)\u2014easily passes the independence test. But it\u2019s possible that strategic behavior on the part of the defendant in response to the strictness of the judge they were assigned can undermine the otherwise random assignment. Consider something that Gaudet et al. [1933] observed in their orig- inal study regarding the dynamics of the courtroom when randomly assigned a severe judge:\nThe individual tendencies in the sentencing tendencies of judges are evidently recognized by many who are accustomed to observe this sentencing. The authors have been told by several lawyers that some recidivists know the sentencing tendencies of judges so well that the accused will frequently attempt to choose which judge is to sentence them, and further, some lawyers say that they are frequently able to do this. It is said to be done in this way. If the prisoner sees that he is going to be sentenced by Judge X, whom he believes to be severe in his sentencing tendency, he will change his plea from \u201cGuilty\u201d to \u201cNon Vult\u201d or from \u201cNon Vult\u201d to \u201cNot Guilty,\u201d etc. His hope is that in this way the sentencing will be postponed and hence he will probably be sentenced by another judge. [812]\nThere are several approaches one can take to assessing inde- pendence. First, checking for balance on pre-treatment covariates is an absolute must. Insofar as this is a randomized experiment, then all observable and unobservable characteristics will be distributed equally across the judges. While we cannot check for balance on unob- servables, we can check for balance on observables.", "start_char_idx": 14982, "end_char_idx": 19533, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d4224a67-d264-48d0-b4d0-c1c499a35b1e": {"__data__": {"id_": "d4224a67-d264-48d0-b4d0-c1c499a35b1e", "embedding": null, "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5b71990f-327a-41b7-82e3-7cd6ba4744ae", "node_type": "4", "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "hash": "468c66e601983f967c99b39a1333e32fd92b79c7a42b247cfc9afc6446eadace", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f7247636-53fa-40f2-8ac3-f69dd40f826c", "node_type": "1", "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "hash": "b155ffcfb8ada80047187e1bdf222efcd832218c5860e2554c8be532c18a4757", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d12f1e3-a926-479b-92c1-e32df90f37e8", "node_type": "1", "metadata": {}, "hash": "d4174a653c950fe5e65f4b8b475b83af2ee07d62d1ec72cd5265118fb10b54ae", "class_name": "RelatedNodeInfo"}}, "text": "It is said to be done in this way. If the prisoner sees that he is going to be sentenced by Judge X, whom he believes to be severe in his sentencing tendency, he will change his plea from \u201cGuilty\u201d to \u201cNon Vult\u201d or from \u201cNon Vult\u201d to \u201cNot Guilty,\u201d etc. His hope is that in this way the sentencing will be postponed and hence he will probably be sentenced by another judge. [812]\nThere are several approaches one can take to assessing inde- pendence. First, checking for balance on pre-treatment covariates is an absolute must. Insofar as this is a randomized experiment, then all observable and unobservable characteristics will be distributed equally across the judges. While we cannot check for balance on unob- servables, we can check for balance on observables. Most papers of which I am aware check for covariate balance, usually before doing any actual analysis.\nInsofar as you suspect endogenous sorting, you might simply use the original assignment, not the \ufb01nal assignment, for identi\ufb01cation. This is because in most cases, we will know the initial judge assign- ment was random. But this approach may not be feasible in many settings if initial judge or court assignment is not available. Never- theless, endogenous sorting in response to the severity of the judge could undermine the design by introducing a separate mechanism by which the instrument impacts the \ufb01nal decision (via sorting into the lenient judge\u2019s courtroom if possible), and the researcher should attempt to ascertain through conversations with administrators the degree to which this practically occurs in the data.\nThe violation of exclusion is more often the worry, though, and really should be evaluated case by case. For instance, in Dobbie et al. [2018], the authors are focused on pretrial detention. But pretrial deten- tion is determined by bail set by judges who do not themselves have any subsequent interaction with the next level\u2019s randomized judge, and de\ufb01nitely don\u2019t have any interaction with the defendant upon the judi- cial ruling and punishment rendered. So in this case, it does seem like Dobbie et al. [2018] might have a more credible argument that exclusion holds.\nBut consider a situation where a defendant is randomly assigned a severe judge. In expectation, if the case goes to trial, the defendant faces a higher expected penalty even given a \ufb01xed probability of con- viction across any judge for no other reason than that the stricter judge will likely choose a harsher penalty and thus drive up the expected penalty. Facing this higher expected penalty, the defense attorney and defendant might decide to accept a lesser plea in response to the judge\u2019s anticipated severity, which would violate exclusion since exclusion requires the instrument effect the outcome only through the judge\u2019s decision (sentence).\nBut even if exclusion can be defended, in many situations mono- tonicity becomes the more di\ufb03cult case to make for this design. It was explicitly monotonicity that made Imbens and Angrist [1994] skepti- cal that judge \ufb01xed effects could be used to identify the local average treatment effect. This is because the instrument is required to weakly operate the same across all defendants. Either a judge is strict or she isn\u2019t, but she can\u2019t be both in different circumstances. Yet humans\nare complex bundles of thoughts and experiences, and those biases may operate in non-transitive ways. For instance, a judge may be lenient, except when the defendant is black or if the offense is a drug charge, in which case they switch and become strict. Mueller-Smith [2015] attempted to overcome potential violations of exclusion and monotonicity through a parametric strategy of simultaneously instru- menting for all observed sentencing dimensions and thus allowing the instruments\u2019 effect on sentencing outcomes to be heterogeneous in defendant traits and crime characteristics.\nFormal solutions to querying the plausibility of these assumptions have appeared in recent years, though. Frandsen et al. [2019] propose a test for exclusion and monotonicity based on relaxing the monotonic- ity assumption. This test requires that the average treatment effect among individuals who violate monotonicity be identical to the aver- age treatment effect among some subset of individuals who satisfy it. Their test simultaneously tests for exclusion and monotonicity, so one cannot be sure which violation is driving the test\u2019s result unless theoretically one rules out one of the two using a priori information. Their proposed test is based on two observations: that the average outcomes, conditional on judge assignment, should \ufb01t a continuous function of judge propensities, and secondly, the slope of that con- tinuous function should be bounded in magnitude by the width of the outcome variable\u2019s support.", "start_char_idx": 18769, "end_char_idx": 23580, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4d12f1e3-a926-479b-92c1-e32df90f37e8": {"__data__": {"id_": "4d12f1e3-a926-479b-92c1-e32df90f37e8", "embedding": null, "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5b71990f-327a-41b7-82e3-7cd6ba4744ae", "node_type": "4", "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "hash": "468c66e601983f967c99b39a1333e32fd92b79c7a42b247cfc9afc6446eadace", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d4224a67-d264-48d0-b4d0-c1c499a35b1e", "node_type": "1", "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "hash": "bf3028c83a0b5cb66e807f26c48a53daecfb3c3536ffad856548c0a2e5c8ba0e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6db17b8-3a22-4781-9dab-cb85c8aaafc0", "node_type": "1", "metadata": {}, "hash": "a7fb967737820f84d3eb386586852e05a313c06cc6189090b57ed2621d4590fe", "class_name": "RelatedNodeInfo"}}, "text": "Formal solutions to querying the plausibility of these assumptions have appeared in recent years, though. Frandsen et al. [2019] propose a test for exclusion and monotonicity based on relaxing the monotonic- ity assumption. This test requires that the average treatment effect among individuals who violate monotonicity be identical to the aver- age treatment effect among some subset of individuals who satisfy it. Their test simultaneously tests for exclusion and monotonicity, so one cannot be sure which violation is driving the test\u2019s result unless theoretically one rules out one of the two using a priori information. Their proposed test is based on two observations: that the average outcomes, conditional on judge assignment, should \ufb01t a continuous function of judge propensities, and secondly, the slope of that con- tinuous function should be bounded in magnitude by the width of the outcome variable\u2019s support. The test itself is relatively straightforward and simply requires examining whether observed outcomes averaged by judges are consistent with such a function. We can see a picture of what it looks like to pass this test in the top panel of Figure 53 versus the bottom panel, which fails the test.\nWhile the authors have made available code and documentation that can be used to implement this test,19 it is not currently available in R and therefore will not be reviewed here.\nIn this section, I\u2019d like to accomplish two things. First, I\u2019d like to review an interesting new paper by Megan Stevenson that examined how cash bail affected case outcomes [Stevenson, 2018]. As this is an important policy question, I felt it would be good to review this\n19 See Emily Leslie\u2019s website https://sites.google.com/view/emilycleslie/home/\nresearch to download the Stata ado package.\nFigure 53. Average outcomes as a function of judge propensities. Frandsen, B. R., Lef-\ngren, L. J., and Leslie, E. C. (2019). Judging judge \ufb01xed effects. Working\nReprinted with permission from authors.\nexcellent study. But the second purpose of this section is to replicate her main results so that the reader can see exactly how to implement\nAs with most judge \ufb01xed effects papers, Stevenson is working with administrative data for a large city. Large cities are probably the best context due to the large samples which can help ameliorate the \ufb01nite sample bias of IV. Fortunately, these data are often publicly available and need only be scraped from court records that are in many loca- tions posted online. Stevenson [2018] focuses on Philadelphia, where the natural experiment is the random assignment of bail judges (\u201cmag- istrates\u201d) who unsurprisingly differ widely in their propensity to set bail at affordable levels. In other words, bail judges differ systematically in the price they set for bail, and given a downward-sloping demand curve, more severe judges setting expensive bails will see more defen- dants unable to pay their bail. As a result, they are forced to remain in detention prior to the trial.\nUsing a variety of IV estimators, Stevenson [2018] \ufb01nds that an increase in randomized pretrial detention leads to a 13% increase in the likelihood of receiving a conviction. She argues that this is caused by an increase in guilty pleas among defendants who otherwise would have been acquitted or had their charges dropped\u2014a particu- larly problematic mechanism, if true. Pretrial detention also led to a 42% increase in the length of the incarceration sentence and a 41% increase in the amount of non-bail fees owed. This provides support for idea that cash bail contributes to a cycle of poverty in which defen- dants unable to pay their court fees end up trapped in the penal system through higher rates of guilt, higher court fees, and likely higher rates of reoffending [Dobbie et al., 2018].\nOne might think that the judge \ufb01xed effects design is a \u201cjust identi- \ufb01ed\u201d model. Can\u2019t we just use as our instrument the average strictness for each judge (excluding the defendant\u2019s own case)? Then we have just one instrument for our one endogenous variable, and 2SLS seems like a likely candidate, right? After all, that one instrument would be unique to each individual because each individual would have a unique judge and a unique average strictness if average strictness was calcu- lated as the mean of all judge sentencing excluding the individual under consideration.\nThe problem is that this is still just a high-dimension instrument. The correct speci\ufb01cation is to use the actual judge \ufb01xed effects, and depending on your application you may have anywhere from eight (as\nin Stevenson\u2019s case) to hundreds of judges.", "start_char_idx": 22658, "end_char_idx": 27294, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6db17b8-3a22-4781-9dab-cb85c8aaafc0": {"__data__": {"id_": "e6db17b8-3a22-4781-9dab-cb85c8aaafc0", "embedding": null, "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5b71990f-327a-41b7-82e3-7cd6ba4744ae", "node_type": "4", "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "hash": "468c66e601983f967c99b39a1333e32fd92b79c7a42b247cfc9afc6446eadace", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d12f1e3-a926-479b-92c1-e32df90f37e8", "node_type": "1", "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "hash": "491daa3cebe21e27acaf978ad3bfd9747247e72657b0308aa1d83ac340e212d2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b6ff1f30-2536-4baf-bb4d-98ae3353bbf5", "node_type": "1", "metadata": {}, "hash": "141eb8511e965cb926a502c70d99c8680ade0852986d428397bdeb3c55b4b610", "class_name": "RelatedNodeInfo"}}, "text": "One might think that the judge \ufb01xed effects design is a \u201cjust identi- \ufb01ed\u201d model. Can\u2019t we just use as our instrument the average strictness for each judge (excluding the defendant\u2019s own case)? Then we have just one instrument for our one endogenous variable, and 2SLS seems like a likely candidate, right? After all, that one instrument would be unique to each individual because each individual would have a unique judge and a unique average strictness if average strictness was calcu- lated as the mean of all judge sentencing excluding the individual under consideration.\nThe problem is that this is still just a high-dimension instrument. The correct speci\ufb01cation is to use the actual judge \ufb01xed effects, and depending on your application you may have anywhere from eight (as\nin Stevenson\u2019s case) to hundreds of judges. Insofar as some of these are weak, which they probably will be, you run into a typical kind of overidenti\ufb01cation problem where in \ufb01nite samples you begin moving the point estimates back to centering on the OLS bias as I discussed earlier. This issue is still being resolved by econometricians and is likely to be an active area of research going forward. Some solutions may be to use high-dimension reduction techniques such as LASSO [Gilchrist and Sands, 2016], instrument selection [Donald and Newey, 2001], or perhaps combining individual judges with similar strictness into only one instrument.\nStevenson\u2019s data contains 331,971 observations and eight ran- domly assigned bail judges. Like many papers in the judge \ufb01xed effects literature, she uses the jackknife instrumental variables esti- mator (JIVE) [Angrist et al., 1999]. While 2SLS is the most commonly used IV estimator in applied microeconomics applications, it suffers from \ufb01nite sample problems when there are weak instruments and the use of many instruments as we showed with the discussion of Bound et al. [1995]. Angrist et al. [1999] proposed an estimator that attempts to eliminate the \ufb01nite-sample bias of 2SLS called JIVE.20 These aren\u2019t perfect, as their distributions are larger than that of the 2SLS estimator, but they may have an advantage when there are several instruments and some of which are weak (as is likely to occur with judge \ufb01xed effects).\nJIVE is popularly known as a \u201cleave one out\u201d estimator. Angrist et al. [1999] suggest using all observations in this estimator except for the i unit. This is the nice feature for judge \ufb01xed effects because ideally the instrument is the mean strictness of the judge in all other cases, excluding the particular defendant\u2019s case. So JIVE is nice both for its handling of the \ufb01nite sample bias, and for its construction of the theoretical instrument more generally.\nGiven the econometrics of judge \ufb01xed effects with its many instru- ments is potentially the frontier of econometrics, my goal here will be somewhat backwards looking. We will simply run through some simple exercises using JIVE so that you can see how historically researchers are estimating their models.", "start_char_idx": 26470, "end_char_idx": 29491, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b6ff1f30-2536-4baf-bb4d-98ae3353bbf5": {"__data__": {"id_": "b6ff1f30-2536-4baf-bb4d-98ae3353bbf5", "embedding": null, "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5b71990f-327a-41b7-82e3-7cd6ba4744ae", "node_type": "4", "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "hash": "468c66e601983f967c99b39a1333e32fd92b79c7a42b247cfc9afc6446eadace", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6db17b8-3a22-4781-9dab-cb85c8aaafc0", "node_type": "1", "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "hash": "3565ce854c56552064faeacb439c5792ad2b9812851dbc971d7d4e4c8e8deb43", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e26e7897-8f47-44ba-ab0d-26b7b956d7bc", "node_type": "1", "metadata": {}, "hash": "34d286f09a715bd2585d90948b5e967a07e050c53e1950a07d66c8fdd2706155", "class_name": "RelatedNodeInfo"}}, "text": "JIVE is popularly known as a \u201cleave one out\u201d estimator. Angrist et al. [1999] suggest using all observations in this estimator except for the i unit. This is the nice feature for judge \ufb01xed effects because ideally the instrument is the mean strictness of the judge in all other cases, excluding the particular defendant\u2019s case. So JIVE is nice both for its handling of the \ufb01nite sample bias, and for its construction of the theoretical instrument more generally.\nGiven the econometrics of judge \ufb01xed effects with its many instru- ments is potentially the frontier of econometrics, my goal here will be somewhat backwards looking. We will simply run through some simple exercises using JIVE so that you can see how historically researchers are estimating their models.\n4 global demo black age male white 5 global off 6 global prior priorCases priorWI5 prior_felChar prior_guilt onePrior threePriors 7 global control2 8 9 10 * Naive OLS 11 * minimum controls 12 reg guilt jail3 $control2, robust 13 * maximum controls 14 reg guilt jail3 possess robbery DUI1st drugSell aggAss $demo $prior $off\nday day2 day3 bailDate t1 t2 t3 t4 t5 t6\n15 16 17 ** Instrumental variables estimation 18 * 2sls main results 19 * minimum controls 20 ivregress 2sls guilt (jail3= $judge_pre) $control2, robust 21 * maximum controls 22 ivregress 2sls guilt (jail3= $judge_pre) possess robbery DUI1st drugSell aggAss\n1 library(tidyverse) 2 library(haven) 3 library(estimatr) 4 library(lfe) 5 library(SteinIV) 6 7 read_data <- function(df) 8 { 9 10 11 12 13 } 14 15 judge <- read_data(\"judge_fe.dta\") 16 17 #grouped variable names from the data set 18 judge_pre <- judge %>% 19 20 21 22 23 24 demo <- judge %>% 25 26 27 28 29 off <- judge %>% 30 31 32 33 34 prior <- judge %>% 35 36 37 38 39\nselect(starts_with(\"judge_\")) %>% colnames() %>% subset(., . != \"judge_pre_8\") %>% # remove one for colinearity paste(., collapse = \" + \")\ndf, sep = \"\")\n(continued)\nR (continued)\n40 control2 <- judge %>% 41 mutate(bailDate = as.numeric(bailDate)) %>% 42 43 44 45 46 47 #formulas used in the OLS 48 min_formula <- as.formula(paste(\"guilt ~ jail3 + \", control2)) 49 max_formula <- as.formula(paste(\"guilt ~ jail3 + possess + robbery + DUI1st +\ndrugSell + aggAss\",\n50 51 52 #max variables and min variables 53 min_ols <- lm_robust(min_formula, data = judge) 54 max_ols <- lm_robust(max_formula, data = judge) 55 56 #--- Instrumental Variables Estimations 57 #-- 2sls main results 58 #- Min and Max Control formulas 59 min_formula <- as.formula(paste(\"guilt ~ \", control2, \" | 0 | (jail3 ~ 0 +\", judge_pre,\ndemo, prior, off, control2, sep = \" + \"))\n60 max_formula <- as.formula(paste(\"guilt ~\", demo, \"+ possess +\", prior, \"+ robbery\noff, \"+ DUI1st +\", control2, \"+ drugSell + aggAss | 0 | (jail3 ~ 0 (cid:12)\u2192 62 #2sls for min and max 63 min_iv <- felm(min_formula, data = judge) 64 summary(min_iv) 65 max_iv <- felm(max_formula, data = judge) 66 summary(max_iv) 67 68 69 70 #-- JIVE main results 71 #- minimum controls 72 y <- judge %>% 73 pull(guilt) 74\n(continued)\nR (continued)\n75 X_min <- judge %>% 76 mutate(bailDate = as.numeric(bailDate)) %>% 77 78 model.matrix(data = .,~.) 79 80 Z_min <- judge %>% 81 mutate(bailDate = as.numeric(bailDate)) %>% 82 83 84 model.matrix(data = .,~.)", "start_char_idx": 28724, "end_char_idx": 31975, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e26e7897-8f47-44ba-ab0d-26b7b956d7bc": {"__data__": {"id_": "e26e7897-8f47-44ba-ab0d-26b7b956d7bc", "embedding": null, "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5b71990f-327a-41b7-82e3-7cd6ba4744ae", "node_type": "4", "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "hash": "468c66e601983f967c99b39a1333e32fd92b79c7a42b247cfc9afc6446eadace", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b6ff1f30-2536-4baf-bb4d-98ae3353bbf5", "node_type": "1", "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "hash": "4b80bdea9a1177d6f0640e9a4e9b43f6334d87b803ad6b7557ffb52c3f3c9754", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0229e2ab-70b0-440c-9a4a-9059405b2e2f", "node_type": "1", "metadata": {}, "hash": "6433dd9bc2cfc284114ef5a034e2f609f98a202df179b8c65e55d5dde913aff5", "class_name": "RelatedNodeInfo"}}, "text": "79 80 Z_min <- judge %>% 81 mutate(bailDate = as.numeric(bailDate)) %>% 82 83 84 model.matrix(data = .,~.) 85 86 jive.est(y = y, X = X_min, Z = Z_min) 87 88 #- maximum controls 89 X_max <- judge %>% 90 mutate(bailDate = as.numeric(bailDate)) %>% 91 select(jail3, white, age, male, black, 92 possess, robbery, prior_guilt, 93 prior_guilt, onePrior, priorWI5, prior_felChar, priorCases, 94 DUI1st, drugSell, aggAss, fel, mis, sum, 95 threePriors, 96 F1, F2, F3, 97 M, M1, M2, M3, 98 day, day2, bailDate, 99 t1, t2, t3, t4, t5) %>% 100 model.matrix(data = .,~.) 101 102 Z_max <- judge %>% 103 mutate(bailDate = as.numeric(bailDate)) %>% 104 105 106 possess, robbery, prior_guilt, 107 prior_guilt, onePrior, priorWI5, prior_felChar, priorCases, 108 DUI1st, drugSell, aggAss, fel, mis, sum, 109 threePriors, 110 F1, F2, F3, 111 M, M1, M2, M3, 112 day, day2, bailDate, 113 t1, t2, t3, t4, t5) %>% 114 model.matrix(data = .,~.) 115 116 jive.est(y = y, X = X_max, Z = Z_max)\nNote: First model includes controls for time; second model controls for characteris- tics of the defendant. Outcome is guilty plea. Heteroskedastic robust standard errors in parentheses. * p<0.10, ** p<0.05, *** p<0.01\nThese results are pretty interesting. Notice that if we just were to examine this using OLS, you\u2019d conclude there was actually no connec- tion between pre-trial detention and a guilty plea. It was either zero using only time controls, or it raised the probability 3% with our fuller set of controls (mainly demographic controls, prior offenses, and the characteristics of the offense itself). But, when we use IV with the binary judge \ufb01xed effects as instruments, the effects change a lot. We end up with estimates ranging from 15 to 21%, and of these probably we should be more focused on JIVE because of its advantages, as pre- viously mentioned. You can examine the strength of the instruments yourself by regressing detention onto the binary instruments to see just how strong the instruments are, but they are very strong. All but two are statistically signi\ufb01cant at the 1% level. Of the other two, one has a p-value of 0.076 and the other is weak (p < 0.25).\nThe judge \ufb01xed effects design is a very popular form of instru- mental variables. It is used whenever there exists a wheel of randomly assigned decision makers assigning a treatment of some kind to other people. Important questions and answers in the area of criminal jus- tice have been examined using this design. When linked with exter- nal administrative data sources, researchers have been able to more carefully evaluate the causal effect of criminal justice interventions on long-term outcomes. But the procedure has uniquely sensitive iden- tifying assumptions related to independence, exclusion, and mono- tonicity that must be carefully contemplated before going forward with\nthe design. Nevertheless, when those assumptions can be credibly defended, it is a powerful estimator of local average treatment effects.\nBartik instruments. Bartik instruments, also known as shift-share instruments, were named after Timothy Bartik, who used them in a careful study of regional labor-markets [Bartik, 1991]. Both Bartik\u2019s book and the instrument received wider attention the following year with Blanchard and Katz [1992]. It has been particularly in\ufb02uential in the areas of migration and trade, as well as labor, public, and several other \ufb01elds. A simple search for the phrase \u201cBartik instrument\u201d on Google Scholar reveals almost \ufb01ve hundred cites at the time of this writing.\nBut just as Stigler\u2019s law of eponymy promises [Stigler, 1980], Bar- tik instruments do not originate with Bartik [1991].", "start_char_idx": 31869, "end_char_idx": 35520, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0229e2ab-70b0-440c-9a4a-9059405b2e2f": {"__data__": {"id_": "0229e2ab-70b0-440c-9a4a-9059405b2e2f", "embedding": null, "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5b71990f-327a-41b7-82e3-7cd6ba4744ae", "node_type": "4", "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "hash": "468c66e601983f967c99b39a1333e32fd92b79c7a42b247cfc9afc6446eadace", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e26e7897-8f47-44ba-ab0d-26b7b956d7bc", "node_type": "1", "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "hash": "b356b602f8409566a765e954b4063709b3800d29d0e1a6521c962b273d19d089", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "442df979-8215-4159-8bb8-207789a55d65", "node_type": "1", "metadata": {}, "hash": "c1c35c91ffef5dd85104f2c18e8a2e24905ccb0cf4ad30ad0dc76354f1efd061", "class_name": "RelatedNodeInfo"}}, "text": "Nevertheless, when those assumptions can be credibly defended, it is a powerful estimator of local average treatment effects.\nBartik instruments. Bartik instruments, also known as shift-share instruments, were named after Timothy Bartik, who used them in a careful study of regional labor-markets [Bartik, 1991]. Both Bartik\u2019s book and the instrument received wider attention the following year with Blanchard and Katz [1992]. It has been particularly in\ufb02uential in the areas of migration and trade, as well as labor, public, and several other \ufb01elds. A simple search for the phrase \u201cBartik instrument\u201d on Google Scholar reveals almost \ufb01ve hundred cites at the time of this writing.\nBut just as Stigler\u2019s law of eponymy promises [Stigler, 1980], Bar- tik instruments do not originate with Bartik [1991]. Goldsmith-Pinkham et al. [2020] notes that traces of it can be found as early as Perloff [1957] who showed that industry shares could be used to predict income levels. Freeman [1980] also used the change in industry com- position as an instrument for labor demand. But due to Bartik\u2019s careful empirical analysis using the instrument combined with his detailed exposition of the logic of how the national growth shares created vari- ation in labor-market demand in Appendix 4 of his book, the design has been named after him.\nOLS estimates of the effect of employment growth rates on labor-market outcomes are likely hopelessly biased since labor-market outcomes are simultaneously determined by labor supply and labor demand. Bartik therefore suggested using IV to resolve the issue and in Appendix 4 describes the ideal instrument.\nObvious candidates for instruments are variables shifting MSA labor demand. In this book, only one type of demand shifter is used to form instrumental variables: the share effect from a shift-share analysis of each metropolitan area and year-to-year employment change. A shift-share analysis decomposes MSA growth into three components: a national growth component, which calculates what growth would have occurred if all industries in the MSA had grown at the all-industry national average; a share component, which cal- culates what extra growth would have occurred if each industry in the MSA had grown at that industry\u2019s national average; and a shift\ncomponent, which calculates the extra growth that occurs because industries grow at different rates locally than they do nationally. [Bartik, 1991, 202])\nSummarizing all of this, the idea behind a Bartik instrument is to measure the change in a region\u2019s labor demand due to changes in the national demand for different industries\u2019 products.21 To make this con- crete, let\u2019s assume that we are interested in estimating the following wage equation:\nwhere Yl,t is log wages in location l (e.g., Detroit) in time period t (e.g., 2000) among native workers, Il,t are immigration \ufb02ows in region l at time period t and Xl,t are controls that include region and time \ufb01xed effects, among other things. The parameter \u03b4 as elsewhere is some average treatment effect of the immigration \ufb02ows\u2019 effect on native wages. The problem is that it is almost certainly the case that immi- gration \ufb02ows are highly correlated with the disturbance term such as the time-varying characteristics of location l (e.g., changing amenities) [Sharpe, 2019].\nThe Bartik instrument is created by interacting initial \u201cshares\u201d of geographic regions, prior to the contemporaneous immigration \ufb02ow, with national growth rates. The deviations of a region\u2019s growth from the US national average are explained by deviations in the growth pre- diction variable from the US national average. And deviations of the growth prediction variables from the US national average are due to the shares because the national growth effect for any particular time period is the same for all regions. We can de\ufb01ne the Bartik instrument as follows:\n21 Goldsmith-Pinkham et al. [2020] note that many instruments have Bartik fea-\ntures. They describe an instrument as \u201cBartik-like\u201d if it uses the inner product structure\nof the endogenous variable to construct an instrument.\nwhere zl,k,t0 are the \u201cinitial\u201d t0 share of immigrants from source country k (e.g., Mexico) in location l (e.g., Detroit) and mk,t is the change in immi- gration from country k (e.g., Mexico) into the US as a whole. The \ufb01rst term is the share variable and the second term is the shift variable.", "start_char_idx": 34718, "end_char_idx": 39118, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "442df979-8215-4159-8bb8-207789a55d65": {"__data__": {"id_": "442df979-8215-4159-8bb8-207789a55d65", "embedding": null, "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5b71990f-327a-41b7-82e3-7cd6ba4744ae", "node_type": "4", "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "hash": "468c66e601983f967c99b39a1333e32fd92b79c7a42b247cfc9afc6446eadace", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0229e2ab-70b0-440c-9a4a-9059405b2e2f", "node_type": "1", "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "hash": "8a8dc1a029e8fe9acd80f1f2ce52d613cb8ce22325314c1613ceab883689ac2c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e7c52954-8f2f-4afc-9c28-46c5771ce138", "node_type": "1", "metadata": {}, "hash": "0cd5159a49b3c3518e365f9e5b86d81b825fd120f77d40074ce6af1904f3724c", "class_name": "RelatedNodeInfo"}}, "text": "And deviations of the growth prediction variables from the US national average are due to the shares because the national growth effect for any particular time period is the same for all regions. We can de\ufb01ne the Bartik instrument as follows:\n21 Goldsmith-Pinkham et al. [2020] note that many instruments have Bartik fea-\ntures. They describe an instrument as \u201cBartik-like\u201d if it uses the inner product structure\nof the endogenous variable to construct an instrument.\nwhere zl,k,t0 are the \u201cinitial\u201d t0 share of immigrants from source country k (e.g., Mexico) in location l (e.g., Detroit) and mk,t is the change in immi- gration from country k (e.g., Mexico) into the US as a whole. The \ufb01rst term is the share variable and the second term is the shift variable. The predicted \ufb02ow of immigrants, B, into destination l (e.g., Detroit) is then just a weighted average of the national in\ufb02ow rates from each country in which weights depend on the initial distribution of immigrants.\nOnce we have constructed our instrument, we have a two-stage least squares estimator that \ufb01rst regresses the endogenous Il,t onto the controls and our Bartik instrument. Using the \ufb01tted values from that regression, we then regress Yl,t onto(cid:12)Il,t to recover the impact of immigration \ufb02ows onto log wages.\nShifts vs Shares. I\u2019d like to now turn to the identifying assumptions that are unique to this design. There are two perspectives as to what is needed to leverage a Bartik design to identify a causal effect and they separately address the roles of the exogeneity of the shares ver- sus the shifts. Which perspective you take will depend on the ex ante plausibility of certain assumptions. They will also depend on different tools.\nGoldsmith-Pinkham et al. [2020] explain the shares perspective. They show that while the shifts affect the strength of the \ufb01rst stage, it is actually the initial shares that provide the exogenous variation. They write that \u201cthe Bartik instrument is \u2018equivalent\u2019 to using local indus- try shares as instruments, and so the exogeneity condition should be interpreted in terms of the shares.\u201d Insofar as a researcher\u2019s applica- tion is exploiting differential exogenous exposure to common shocks, industry speci\ufb01c shocks, or a two-industry scenario, then it is likely that the source of exogeneity comes from the initial shares and not the shifts. This is a type of strict exogeneity assumption where the initial shares are exogenous conditional on observables, such as loca- tion \ufb01xed effects. What this means in practice is that the burden is on the researcher to argue why they believe the initial shares are indeed exogenous.\nBut while exogenous shares are su\ufb03cient, it turns out they are not necessary for identi\ufb01cation of causal effects. Temporal shocks may provide exogeous sources of variation. Borusyak et al. [2019] explain\nthe shifts perspective. They show that exogenous independent shocks to many industries allow a Bartik design to identify causal effects regardless of whether the shares are exogenous so long as the shocks are uncorrelated with the bias of the shares. Otherwise, it may be the shock itself that is creating exogenous variation, in which case the focus on excludability moves away from the initial shares and more towards the national shocks themselves [Borusyak et al., 2019]. The authors write:\nUltimately, the plausibility of our exogenous shocks framework, as with the alternative framework of Goldsmith-Pinkham et al. [2020] based on exogenous shares, depends on the shift-share IV appli- cation. We encourage practitioners to use shift-share instruments based on an a priori argument supporting the plausibility of either one of these approaches; various diagnostics and tests of the framework that is most suitable for the setting may then be applied. While [Borusyak et al. [2019]] develops such procedures for the \u201cshocks\u201d view, Goldsmith-Pinkham et al. [2020] provide different tools for the \u201cshares\u201d view. [29]\nInsofar as we think about the initial shares as the instruments, and not the shocks, then we are in a world in which those initial shares are measuring differential exogenous exposures to some common shock. As the shares are equilibrium values, based on past labor supply and demand, it may be tough to justify why we should consider them exoge- nous to the structural unobserved determinants of some future labor- market outcome.", "start_char_idx": 38356, "end_char_idx": 42757, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e7c52954-8f2f-4afc-9c28-46c5771ce138": {"__data__": {"id_": "e7c52954-8f2f-4afc-9c28-46c5771ce138", "embedding": null, "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5b71990f-327a-41b7-82e3-7cd6ba4744ae", "node_type": "4", "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "hash": "468c66e601983f967c99b39a1333e32fd92b79c7a42b247cfc9afc6446eadace", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "442df979-8215-4159-8bb8-207789a55d65", "node_type": "1", "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "hash": "c882f05511a3dc504f89e21bdb9b354aa1372fd95556a039bed5e50a9d3788d9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cc3fc2ae-4423-42c3-85b1-ee9b1dde6059", "node_type": "1", "metadata": {}, "hash": "dce0daf48c28012c7e90ea3b8210d1e08a9297a71090486392c276aac7b3e304", "class_name": "RelatedNodeInfo"}}, "text": "We encourage practitioners to use shift-share instruments based on an a priori argument supporting the plausibility of either one of these approaches; various diagnostics and tests of the framework that is most suitable for the setting may then be applied. While [Borusyak et al. [2019]] develops such procedures for the \u201cshocks\u201d view, Goldsmith-Pinkham et al. [2020] provide different tools for the \u201cshares\u201d view. [29]\nInsofar as we think about the initial shares as the instruments, and not the shocks, then we are in a world in which those initial shares are measuring differential exogenous exposures to some common shock. As the shares are equilibrium values, based on past labor supply and demand, it may be tough to justify why we should consider them exoge- nous to the structural unobserved determinants of some future labor- market outcome. But it turns out that that is not the critical piece. A valid Bartik design can be valid even if the shares are correlated indi- rectly with the levels of the outcomes; they just can\u2019t be correlated with the differential changes associated with the national shock itself, which is a subtle but distinct point.\nOne challenge with Bartik instruments is the sheer number of shifting values. For instance, there are almost four hundred differ- ent industries in the United States. Multiplied over many time periods and the exclusion restriction becomes a bit challenging to defend. Goldsmith-Pinkham et al. [2020] provide several suggestions for eval- uating the central identifying assumption in this design. For instance, if\nthere is a pre-period, then ironically this design begins to resemble the difference-in-differences design that we will discuss in a subsequent chapter. In that case, we might test for placebos, pre-trends, and so forth.\nAnother possibility is based on the observation that the Bartik instrument is simply a speci\ufb01c combination of many instruments. In that sense, it bears some resemblance to the judge \ufb01xed effects design from earlier in which the judge\u2019s propensity was itself a speci\ufb01c com- bination of many binary \ufb01xed effects. With many instruments, other options become available. If the researcher is willing to assume a null of constant treatment effects, then overidenti\ufb01cation tests are an option. But overidenti\ufb01cation tests can fail if there is treatment hetero- geneity as opposed to exclusion not holding. Similar to Borusyak et al. [2019], insofar as one is willing to assume cross-sectional heterogene- ity in which treatment effects are constant within a location only, then Goldsmith-Pinkham et al. [2020] provides some diagnostic aids to help evaluate the plausibility of the design itself.\nA second result in Goldsmith-Pinkham et al. [2020] is a decompo- sition of the Bartik estimator into a weighted combination of estimates where each share is an instrument. These weights, called Rotem- berg weights, sum to one, and the authors note that higher valued weights indicate that those instruments are responsible for more of the identifying variation in the design itself. These weights provide insight into which of the shares get more weight in the overall esti- mate, which helps clarify which industry shares should be scrutinized. If regions with high weights pass some basic speci\ufb01cation tests, then con\ufb01dence in the overall identi\ufb01cation strategy is more defensible.\ninstrumental variables are a powerful design for identifying causal effects when your data suffer from selection on unobservables. But even with that in mind, it has many limitations that have in the contemporary period caused many applied researchers to eschew it. First, it only identi\ufb01es the LATE under heterogeneous treat- ment effects, and that may or may not be a policy relevant variable.\nthere is a pre-period, then ironically this design begins to resemble the difference-in-differences design that we will discuss in a subsequent chapter. In that case, we might test for placebos, pre-trends, and so forth.\nAnother possibility is based on the observation that the Bartik instrument is simply a speci\ufb01c combination of many instruments. In that sense, it bears some resemblance to the judge \ufb01xed effects design from earlier in which the judge\u2019s propensity was itself a speci\ufb01c com- bination of many binary \ufb01xed effects. With many instruments, other options become available. If the researcher is willing to assume a null of constant treatment effects, then overidenti\ufb01cation tests are an option. But overidenti\ufb01cation tests can fail if there is treatment hetero- geneity as opposed to exclusion not holding.", "start_char_idx": 41907, "end_char_idx": 46490, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc3fc2ae-4423-42c3-85b1-ee9b1dde6059": {"__data__": {"id_": "cc3fc2ae-4423-42c3-85b1-ee9b1dde6059", "embedding": null, "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5b71990f-327a-41b7-82e3-7cd6ba4744ae", "node_type": "4", "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "hash": "468c66e601983f967c99b39a1333e32fd92b79c7a42b247cfc9afc6446eadace", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e7c52954-8f2f-4afc-9c28-46c5771ce138", "node_type": "1", "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "hash": "f30145855bd0906074798fd29c9d79d7b75342f9bbfee23f50cb5c8a3701fe6e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d0b870c-d28a-4b61-b176-1a93dbab2915", "node_type": "1", "metadata": {}, "hash": "b35efe940f2c3439ded3d6ad80d52e83fbff78616772a45ec2ebeabfb9d2ca61", "class_name": "RelatedNodeInfo"}}, "text": "there is a pre-period, then ironically this design begins to resemble the difference-in-differences design that we will discuss in a subsequent chapter. In that case, we might test for placebos, pre-trends, and so forth.\nAnother possibility is based on the observation that the Bartik instrument is simply a speci\ufb01c combination of many instruments. In that sense, it bears some resemblance to the judge \ufb01xed effects design from earlier in which the judge\u2019s propensity was itself a speci\ufb01c com- bination of many binary \ufb01xed effects. With many instruments, other options become available. If the researcher is willing to assume a null of constant treatment effects, then overidenti\ufb01cation tests are an option. But overidenti\ufb01cation tests can fail if there is treatment hetero- geneity as opposed to exclusion not holding. Similar to Borusyak et al. [2019], insofar as one is willing to assume cross-sectional heterogene- ity in which treatment effects are constant within a location only, then Goldsmith-Pinkham et al. [2020] provides some diagnostic aids to help evaluate the plausibility of the design itself.\nA second result in Goldsmith-Pinkham et al. [2020] is a decompo- sition of the Bartik estimator into a weighted combination of estimates where each share is an instrument. These weights, called Rotem- berg weights, sum to one, and the authors note that higher valued weights indicate that those instruments are responsible for more of the identifying variation in the design itself. These weights provide insight into which of the shares get more weight in the overall esti- mate, which helps clarify which industry shares should be scrutinized. If regions with high weights pass some basic speci\ufb01cation tests, then con\ufb01dence in the overall identi\ufb01cation strategy is more defensible.\ninstrumental variables are a powerful design for identifying causal effects when your data suffer from selection on unobservables. But even with that in mind, it has many limitations that have in the contemporary period caused many applied researchers to eschew it. First, it only identi\ufb01es the LATE under heterogeneous treat- ment effects, and that may or may not be a policy relevant variable.\nIts value ultimately depends on how closely the compliers\u2019 average treatment effect resembles that of the other subpopulations. Second, unlike RDD, which has only one main identifying assumption (the conti- nuity assumption), IV has up to \ufb01ve assumptions! Thus, you can imme- diately see why people \ufb01nd IV estimation less credible\u2014not because it fails to identify a causal effect, but rather because it\u2019s harder and harder to imagine a pure instrument that satis\ufb01es all \ufb01ve conditions.\nBut all this is to say, IV is an important strategy and sometimes the opportunity to use it will come along, and you should be prepared for when that happens by understanding it and how to implement it in practice. And where can the best instruments be found? Angrist and Krueger [2001] note that the best instruments come from in-depth knowledge of the institutional details of some program or intervention. The things you spend your life studying will in time reveal good instru- ments. Rarely will you \ufb01nd them from simply downloading a new data set, though. Intimate familiarity is how you \ufb01nd instrumental variables, and there is, alas, no shortcut to achieving that.", "start_char_idx": 45671, "end_char_idx": 49019, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8d0b870c-d28a-4b61-b176-1a93dbab2915": {"__data__": {"id_": "8d0b870c-d28a-4b61-b176-1a93dbab2915", "embedding": null, "metadata": {"page number": "420 - 420", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f7ccddad-5194-4d88-a0e7-f754ebc4b955", "node_type": "4", "metadata": {"page number": "420 - 420", "chapter": "Difference in Differences"}, "hash": "bef66b41f42c42fc0f052b3b923168bcf3481da319c7234a01ba1ff15e877580", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cc3fc2ae-4423-42c3-85b1-ee9b1dde6059", "node_type": "1", "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}, "hash": "ff649d2efd6b1761fb56718a91a40da851dda7390227229b5851b9d6f0a88b78", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7eb35797-2fba-4d08-a761-1bdd22824810", "node_type": "1", "metadata": {}, "hash": "75131a4872fb2e55773b6ba7a83535e317f89b184a503e7874b612e413b8973f", "class_name": "RelatedNodeInfo"}}, "text": "Panel Data:\nThat\u2019s just the way it is Things will never be the same That\u2019s just the way it is Some things will never change.\nOne of the most important tools in the causal inference toolkit is the panel data estimator. The estimators are designed explicitly for longitudinal data\u2014the repeated observing of a unit over time. Under certain situations, repeatedly observing the same unit over time can overcome a particular kind of omitted variable bias, though not all kinds. While it is possible that observing the same unit over time will not resolve the bias, there are still many applications where it can, and that\u2019s why this method is so important. We review \ufb01rst the DAG describ- ing just such a situation, followed by discussion of a paper, and then present a data set exercise in R and Stata.1", "start_char_idx": 0, "end_char_idx": 799, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7eb35797-2fba-4d08-a761-1bdd22824810": {"__data__": {"id_": "7eb35797-2fba-4d08-a761-1bdd22824810", "embedding": null, "metadata": {"page number": "420 - 422", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8d094328-4f0c-4ba0-b1c2-de6a937b4266", "node_type": "4", "metadata": {"page number": "420 - 422", "chapter": "Difference in Differences"}, "hash": "ada464772a50a27aeaa61a2dd19fd9d3b44e72666fd00808f923fe85adc12eb2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8d0b870c-d28a-4b61-b176-1a93dbab2915", "node_type": "1", "metadata": {"page number": "420 - 420", "chapter": "Difference in Differences"}, "hash": "30e34cdc7c0b53e40ec09a7d220d1b80d03271f16df0464b70d6e3c61815a634", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0059cad4-fbb1-4299-ba9f-5bd0780861ea", "node_type": "1", "metadata": {}, "hash": "8aef7a45ac3378184209e70e11edee7e069131ac79506bd729fd70f8d7c242e7", "class_name": "RelatedNodeInfo"}}, "text": "DAG Example:\nBefore I dig into the technical assumptions and estimation methodology for panel data techniques, I want to review a simple DAG illustrating those assumptions. This DAG comes from Imai and Kim [2017]. Let\u2019s say that we have data on a column of outcomes Yi, which appear in three time periods. In other words, Yi1, Yi2, and Yi3 where i indexes a particular unit and t = 1, 2, 3 index the time period where\n1 There\u2019s a second reason to learn this estimator. Some of these estimators, such\nas linear models with time and unit \ufb01xed effects, are the modal estimators used in\neach i unit is observed. Likewise, we have a matrix of covariates, Di, which also vary over time\u2014Di1, Di2, and Di3. And \ufb01nally, there exists a single unit-speci\ufb01c unobserved variable, ui, which varies across units, but which does not vary over time for that unit. Hence the reason that there is no t = 1, 2, 3 subscript for our ui variable. Key to this variable is (a) it is unobserved in the data set, (b) it is unit-speci\ufb01c, and (c) it does not change over time for a given unit i. Finally there exists some unit- speci\ufb01c time-invariant variable, Xi. Notice that it doesn\u2019t change over time, just ui, but unlike ui it is observed.\nAs this is the busiest DAG we\u2019ve seen so far, it merits some dis- let us note that Di1 causes both Yi1 as well as the cussion. First, next period\u2019s treatment value, Di2. Second, note that an unobserved confounder, ui, determines all Y and D variables. Consequently, D is endogenous since ui is unobserved and absorbed into the structural error term of the regression model. Thirdly, there is no time-varying unobserved confounder correlated with Dit\u2014the only confounder is ui, which we call the unobserved heterogeneity. Fourth, past outcomes do not directly affect current outcomes (i.e., no direct edge between the Yit variables). Fifth, past outcomes do not directly affect current treatments (i.e., no direct edge from Yi,t\u22121 to Dit). And \ufb01nally, past treat- ments, Di,t\u22121 do not directly affect current outcomes, Yit (i.e., no direct edge from Di,t\u22121 and Yit). It is under these assumptions that we can use a particular panel method called \ufb01xed effects to isolate the causal effect of D on Y.2\n2 The \ufb01xed-effects estimator, when it includes year \ufb01xed effects,\nis popularly\nknown as the \u201ctwoway \ufb01xed effects\u201d estimator.\nWhat might an example of this be? Let\u2019s return to our story about the returns to education. Let\u2019s say that we are interested in the effect of schooling on earnings, and schooling is partly determined by unchang- ing genetic factors which themselves determine unobserved ability, like intelligence, contentiousness, and motivation [Conley and Fletcher, 2017]. If we observe the same people\u2019s time-varying earnings and schoolings over time, then if the situation described by the above DAG describes both the directed edges and the missing edges, then we can use panel \ufb01xed effects models to identify the causal effect of schooling on earnings.", "start_char_idx": 0, "end_char_idx": 2982, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0059cad4-fbb1-4299-ba9f-5bd0780861ea": {"__data__": {"id_": "0059cad4-fbb1-4299-ba9f-5bd0780861ea", "embedding": null, "metadata": {"page number": "422 - 431", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "38d19395-2e08-4e6f-9cdc-39326b0ef108", "node_type": "4", "metadata": {"page number": "422 - 431", "chapter": "Difference in Differences"}, "hash": "42a89c20a9eb42c03b762e9a46e2b915a28c582534ac2ad2fb4ebdb40458b210", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7eb35797-2fba-4d08-a761-1bdd22824810", "node_type": "1", "metadata": {"page number": "420 - 422", "chapter": "Difference in Differences"}, "hash": "d788994a70fc75f90bf760b344932bf4410863143165ecabce761b18fb56047c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bfa0d8c7-7f38-4908-800c-7eee9d2a0d10", "node_type": "1", "metadata": {}, "hash": "7e163488b868edb6e698692b68c651ae1166cfc35878cfc8172121edd57b4667", "class_name": "RelatedNodeInfo"}}, "text": "Estimation:\nWhen we use the term \u201cpanel data,\u201d what do we mean? We mean a data set where we observe the same units (e.g., individuals, \ufb01rms, countries, schools) over more than one time period. Often our outcome variable depends on several factors, some of which are observed and some of which are unobserved in our data, and insofar as the unob- served variables are correlated with the treatment variable, then the treatment variable is endogenous and correlations are not estimates of a causal effect. This chapter focuses on the conditions under which a correlation between D and Y re\ufb02ects a causal effect even with unob- served variables that are correlated with the treatment variable. Specif- ically, if these omitted variables are constant over time, then even if they are heterogeneous across units, we can use panel data estima- tors to consistently estimate the effect of our treatment variable on outcomes.\nThere are several different kinds of estimators for panel data, but we will in this chapter only cover two: pooled ordinary least squares (POLS) and \ufb01xed effects (FE).3\n3 A common third type of panel estimator is the random effects estimator, but\nin my experience, I have used it less often than \ufb01xed effects, so I decided to omit it.\nAgain, this is not because it is unimportant. I just have chosen to do fewer things in more\ndetail based on whether I think they qualify as the most common methods used in the present period by applied empiricists. See Wooldridge [2010] for a more comprehensive\ntreatment, though, of all panel methods including random effects.\nWhat might an example of this be? Let\u2019s return to our story about the returns to education. Let\u2019s say that we are interested in the effect of schooling on earnings, and schooling is partly determined by unchang- ing genetic factors which themselves determine unobserved ability, like intelligence, contentiousness, and motivation [Conley and Fletcher, 2017]. If we observe the same people\u2019s time-varying earnings and schoolings over time, then if the situation described by the above DAG describes both the directed edges and the missing edges, then we can use panel \ufb01xed effects models to identify the causal effect of schooling on earnings.\nWhen we use the term \u201cpanel data,\u201d what do we mean? We mean a data set where we observe the same units (e.g., individuals, \ufb01rms, countries, schools) over more than one time period. Often our outcome variable depends on several factors, some of which are observed and some of which are unobserved in our data, and insofar as the unob- served variables are correlated with the treatment variable, then the treatment variable is endogenous and correlations are not estimates of a causal effect. This chapter focuses on the conditions under which a correlation between D and Y re\ufb02ects a causal effect even with unob- served variables that are correlated with the treatment variable. Specif- ically, if these omitted variables are constant over time, then even if they are heterogeneous across units, we can use panel data estima- tors to consistently estimate the effect of our treatment variable on outcomes.\nThere are several different kinds of estimators for panel data, but we will in this chapter only cover two: pooled ordinary least squares (POLS) and \ufb01xed effects (FE).3\n3 A common third type of panel estimator is the random effects estimator, but\nin my experience, I have used it less often than \ufb01xed effects, so I decided to omit it.\nAgain, this is not because it is unimportant. I just have chosen to do fewer things in more\ndetail based on whether I think they qualify as the most common methods used in the present period by applied empiricists. See Wooldridge [2010] for a more comprehensive\ntreatment, though, of all panel methods including random effects.\nFirst we need to set up our notation. With some exceptions, panel methods are usually based on the traditional notation and not the potential outcomes notation.\nLet Y and D \u2261 (D1, D2, . . . , Dk) be observable random variables and u be an unobservable random variable. We are interested in the partial effects of variable Dj in the population regression function:\nWe observe a sample of i = 1, 2, . . . , N cross-sectional units for t = 1, 2, . . . , T time periods (a balanced panel). For each unit i, we denote the observable variables for all time periods as {(Yit, Dit) : t = 1, 2, . . .", "start_char_idx": 0, "end_char_idx": 4384, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bfa0d8c7-7f38-4908-800c-7eee9d2a0d10": {"__data__": {"id_": "bfa0d8c7-7f38-4908-800c-7eee9d2a0d10", "embedding": null, "metadata": {"page number": "422 - 431", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "38d19395-2e08-4e6f-9cdc-39326b0ef108", "node_type": "4", "metadata": {"page number": "422 - 431", "chapter": "Difference in Differences"}, "hash": "42a89c20a9eb42c03b762e9a46e2b915a28c582534ac2ad2fb4ebdb40458b210", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0059cad4-fbb1-4299-ba9f-5bd0780861ea", "node_type": "1", "metadata": {"page number": "422 - 431", "chapter": "Difference in Differences"}, "hash": "89501ae7dc2854d10824173afbc30c91cca7a64a93a1c7c542ddf27fd0fe1944", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "81377956-b9a0-49c5-9d4f-224e4ffdebc1", "node_type": "1", "metadata": {}, "hash": "d315b6d4d397cf0d90709eb64735b8f63a70360fd416fee2c9d1dcbfe582f3d5", "class_name": "RelatedNodeInfo"}}, "text": "See Wooldridge [2010] for a more comprehensive\ntreatment, though, of all panel methods including random effects.\nFirst we need to set up our notation. With some exceptions, panel methods are usually based on the traditional notation and not the potential outcomes notation.\nLet Y and D \u2261 (D1, D2, . . . , Dk) be observable random variables and u be an unobservable random variable. We are interested in the partial effects of variable Dj in the population regression function:\nWe observe a sample of i = 1, 2, . . . , N cross-sectional units for t = 1, 2, . . . , T time periods (a balanced panel). For each unit i, we denote the observable variables for all time periods as {(Yit, Dit) : t = 1, 2, . . . , T}.4 Let Dit \u2261 (Dit1, Dit2, . . . , Ditk) be a 1 \u00d7 K vector. We typically assume that the actual cross-sectional units (e.g., individuals in a panel) are identical and independent draws from the population in which case {Yi, Di, ui}N \u223c i.i.d., or cross-sectional independence. We describe the i=1 main observables, then, as Yi \u2261 (Yi1, Yi2, . . . , YiT)(cid:12) and Di \u2261 (Di1, Di2, . . . , DiT). It\u2019s helpful now to illustrate the actual stacking of individual units across their time periods. A single unit i will have multiple time periods t\nAnd the entire panel itself with all units included will look like this:\n4 For simplicity, I\u2019m ignoring the time-invariant observations, Xi from our DAG for\nreasons that will hopefully soon be made clear.\nFor a randomly drawn cross-sectional unit i, the model is given by\nAs always, we use our schooling-earnings example for motivation. Let Yit be log earnings for a person i in year t. Let Dit be schooling for person i in year t. Let \u03b4 be the returns to schooling. Let ui be the sum of all time-invariant person-speci\ufb01c characteristics, such as unobserved ability. This is, as I said earlier, often called the unobserved heterogene- ity. And let \u03b5it be the time-varying unobserved factors that determine a person\u2019s wage in a given period. This is often called the idiosyncratic error. We want to know what happens when we regress Yit on Dit.\nPooled OLS. The \ufb01rst estimator we will discuss is the pooled ordinary least squares, or POLS estimator. When we ignore the panel structure and regress Yit on Dit we get\nwith composite error \u03b7it \u2261 ci + \u03b5it. The main assumption necessary to obtain consistent estimates for \u03b4 is:\nWhile our DAG did not include \u03b5it, this would be equivalent to assuming that the unobserved heterogeneity, ci, was uncorrelated with Dit for all time periods.\nBut this is not an appropriate assumption in our case because our DAG explicitly links the unobserved heterogeneity to both the outcome and the treatment in each period. Or using our schooling-earnings example, schooling is likely based on unobserved background factors, ui, and therefore without controlling for it, we have omitted variable bias and (cid:12)\u03b4 is biased. No correlation between Dit and \u03b7it necessarily means no correlation between the unobserved ui and Dit for all t and that is just probably not a credible assumption. An additional problem is that \u03b7it is serially correlated for unit i since ui is present in each t period. And thus heteroskedastic robust standard errors are also likely too small.\nFixed effects (within Estimator). Let\u2019s rewrite our unobserved effects model so that this is still \ufb01rmly in our minds:\nIf we have data on multiple time periods, we can think of ui as \ufb01xed effects to be estimated. OLS estimation with \ufb01xed effects yields\n= arg min b,m1,...,mN\nwhich amounts to including N individual dummies in regression of Yit on Dit.\nThe \ufb01rst-order conditions (FOC) for this minimization problem are:\nTherefore, for i = 1, . . .", "start_char_idx": 3680, "end_char_idx": 7374, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "81377956-b9a0-49c5-9d4f-224e4ffdebc1": {"__data__": {"id_": "81377956-b9a0-49c5-9d4f-224e4ffdebc1", "embedding": null, "metadata": {"page number": "422 - 431", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "38d19395-2e08-4e6f-9cdc-39326b0ef108", "node_type": "4", "metadata": {"page number": "422 - 431", "chapter": "Difference in Differences"}, "hash": "42a89c20a9eb42c03b762e9a46e2b915a28c582534ac2ad2fb4ebdb40458b210", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bfa0d8c7-7f38-4908-800c-7eee9d2a0d10", "node_type": "1", "metadata": {"page number": "422 - 431", "chapter": "Difference in Differences"}, "hash": "93e2aa279b1b3f0636218261280d7bf84f9bf5da7dd84661813b0d8630c1c454", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ace06f35-cb61-4b0d-8c48-df4e94eb0243", "node_type": "1", "metadata": {}, "hash": "0031dd35906661c19e212fcd094e10356b357222f5683f29b80328e8593e88ba", "class_name": "RelatedNodeInfo"}}, "text": "No correlation between Dit and \u03b7it necessarily means no correlation between the unobserved ui and Dit for all t and that is just probably not a credible assumption. An additional problem is that \u03b7it is serially correlated for unit i since ui is present in each t period. And thus heteroskedastic robust standard errors are also likely too small.\nFixed effects (within Estimator). Let\u2019s rewrite our unobserved effects model so that this is still \ufb01rmly in our minds:\nIf we have data on multiple time periods, we can think of ui as \ufb01xed effects to be estimated. OLS estimation with \ufb01xed effects yields\n= arg min b,m1,...,mN\nwhich amounts to including N individual dummies in regression of Yit on Dit.\nThe \ufb01rst-order conditions (FOC) for this minimization problem are:\nTherefore, for i = 1, . . . , N,\nPlug this result into the \ufb01rst FOC to obtain: (cid:8) N(cid:6)\nIn case it isn\u2019t clear, though, running a regression with the time- demeaned variables \u00a8Yit \u2261 Yit \u2212 Yi and \u00a8Dit \u2261 Dit \u2212 D is numerically equiv- alent to a regression of Yit on Dit and unit-speci\ufb01c dummy variables. Hence the reason this is sometimes called the \u201cwithin\u201d estimator, and sometimes called the \u201c\ufb01xed effects\u201d estimator. And when year \ufb01xed effects are included, the \u201ctwoway \ufb01xed effects\u201d estimator. They are the same thing.5\nEven better, the regression with the time-demeaned variables is consistent for \u03b4 even when C[Dit, ui] (cid:9)= 0 because time-demeaning eliminates the unobserved effects. Let\u2019s see this now:\nWhere\u2019d the unobserved heterogeneity go?! It was deleted when we time-demeaned the data. And as we said, including individual \ufb01xed effects does this time demeaning automatically so that you don\u2019t have to go to the actual trouble of doing it yourself manually.6\nSo how do we precisely do this form of estimation? There are three\nways to implement the \ufb01xed effects (within) estimator. They are:\n1. Demean and regress \u00a8Yit on \u00a8Dit (need to correct degrees of free-\n2. Regress Yit on Dit and unit dummies (dummy variable regression). 3. Regress Yit on Dit with canned \ufb01xed effects routine in Stata or R.\nLater in this chapter, I will review an example from my research. We will estimate a model involving sex workers using pooled OLS, a FE, and a demeaned OLS model. I\u2019m hoping this exercise will help you see the similarities and differences across all three approaches.\n5 One of the things you\u2019ll \ufb01nd over time is that things have different names, depend-\ning on the author and tradition, and those names are often completely uninformative.\n6 Though feel free to do it if you want to convince yourself that they are numerically\nequivalent, probably just starting with a bivariate regression for simplicity.\nIdentifying Assumptions. We kind of reviewed the assumptions nec- essary to identify \u03b4 with our \ufb01xed effects (within) estimator when we walked through that original DAG, but let\u2019s supplement that DAG intuition with some formality. The main identi\ufb01cation assumptions are:\n(cid:129) This means that the regressors are strictly exogenous condi- tional on the unobserved effect. This allows Dit to be arbitrarily related to ui, though. It only concerns the relationship between Dit and \u03b5it, notD it\u2019s relationship to ui. (cid:9)\n(cid:129) It shouldn\u2019t be a surprise to you by this point that we have a rank condition, because even when we were working with the simpler linear models, the estimated coe\ufb03cient was always a scaled covariance, where the scaling was by a variance term. Thus regressors must vary over time for at least some i and not be collinear in order that (cid:12)\u03b4 \u2248 \u03b4.\nThe properties of the estimator under assumptions 1 and 2 are that (cid:12)\u03b4FE is consistent (p lim N\u2192\u221e\n(cid:12)\u03b4FE,N = \u03b4) and (cid:12)\u03b4FE is unbiased conditional on D.\nI only brie\ufb02y mention inference. But the standard errors in this framework must be \u201cclustered\u201d by panel unit (e.g., individual) to allow for correlation in the \u03b5it for the same person i over time.", "start_char_idx": 6582, "end_char_idx": 10523, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ace06f35-cb61-4b0d-8c48-df4e94eb0243": {"__data__": {"id_": "ace06f35-cb61-4b0d-8c48-df4e94eb0243", "embedding": null, "metadata": {"page number": "422 - 431", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "38d19395-2e08-4e6f-9cdc-39326b0ef108", "node_type": "4", "metadata": {"page number": "422 - 431", "chapter": "Difference in Differences"}, "hash": "42a89c20a9eb42c03b762e9a46e2b915a28c582534ac2ad2fb4ebdb40458b210", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "81377956-b9a0-49c5-9d4f-224e4ffdebc1", "node_type": "1", "metadata": {"page number": "422 - 431", "chapter": "Difference in Differences"}, "hash": "ff293e71c577f3deae1c08c5eafa4d43132deec39e2b88c378e3d2944bfa3763", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5700686b-089a-4f58-9661-dd1cb0c976ed", "node_type": "1", "metadata": {}, "hash": "daf10bec5350b030830f584d1f8a5e8b153c78aff61dc6b94eee3a3baccca20e", "class_name": "RelatedNodeInfo"}}, "text": "(cid:9)\n(cid:129) It shouldn\u2019t be a surprise to you by this point that we have a rank condition, because even when we were working with the simpler linear models, the estimated coe\ufb03cient was always a scaled covariance, where the scaling was by a variance term. Thus regressors must vary over time for at least some i and not be collinear in order that (cid:12)\u03b4 \u2248 \u03b4.\nThe properties of the estimator under assumptions 1 and 2 are that (cid:12)\u03b4FE is consistent (p lim N\u2192\u221e\n(cid:12)\u03b4FE,N = \u03b4) and (cid:12)\u03b4FE is unbiased conditional on D.\nI only brie\ufb02y mention inference. But the standard errors in this framework must be \u201cclustered\u201d by panel unit (e.g., individual) to allow for correlation in the \u03b5it for the same person i over time. This yields valid inference so long as the number of clusters is \u201clarge.\u201d7\nCaveat #1: Fixed effects cannot solve reverse causality. But, there are still things that \ufb01xed effects (within) estimators cannot solve. For instance, let\u2019s say we regressed crime rates onto police spending per capita. Becker [1968] argues that increases in the probability of arrest, usually proxied by police per capita or police spending per capita, will reduce crime. But at the same time, police spending per capita is itself\n7 In my experience, when an econometrician is asked how large is large, they say\n\u201cthe size of your data.\u201d But that said, there is a small clusters literature and usually it\u2019s\nthought that fewer than thirty clusters is too small (as a rule of thumb). So it may be that having around thirty to forty clusters is su\ufb03cient for the approaching of in\ufb01nity. This will\nusually hold in most panel applications such as US states or individuals in the NSLY, etc.\nNote: North Carolina county-level data. Standard errors in parentheses.\na function of crime rates. This kind of reverse causality problem shows up in most panel models when regressing crime rates onto police. For instance, see Cornwell and Trumbull [1994]. I\u2019ve reproduced a portion of this in Table 66. The dependent variable is crime rates by county in North Carolina for a panel, and they \ufb01nd a positive correlation between police and crime rates, which is the opposite of what Becker predicts. Does this mean having more police in an area causes higher crime rates? Or does it likely re\ufb02ect the reverse causality problem?\nSo, one situation in which you wouldn\u2019t want to use panel \ufb01xed effects is if you have reverse causality or simultaneity bias. And specif- ically when that reverse causality is very strong in observational data. This would technically violate the DAG, though, that we presented at the start of the chapter. Notice that if we had reverse causality, then Y \u2192 D, which is explicitly ruled out by this theoretical model contained in the DAG. But obviously, in the police\u2014crime example, that DAG would be inappropriate, and any amount of re\ufb02ection on the problem should tell you that that DAG is inappropriate. Thus it requires, as I\u2019ve said repeatedly, some careful re\ufb02ection, and writing out exactly what the relationship is between the treatment variables and the outcome vari- ables in a DAG can help you develop a credible identi\ufb01cation strategy.\nCaveat #2: Fixed effects cannot address time-variant unobserved het- erogeneity. The second situation in which panel \ufb01xed effects don\u2019t buy you anything is if the unobserved heterogeneity is time-varying. In this situation, the demeaning has simply demeaned an unobserved time- variant variable, which is then moved into the composite error term, and which since time-demeaned \u00a8uit correlated with \u00a8Dit, \u00a8Dit remains endogenous. Again, look carefully at the DAG\u2014panel \ufb01xed effects are only appropriate if ui is unchanging. Otherwise it\u2019s just another form of\nomitted variable bias. So, that said, don\u2019t just blindly use \ufb01xed effects and think that it solves your omitted variable bias problem\u2014in the same way that you shouldn\u2019t use matching just because it\u2019s convenient to do. You need a DAG, based on an actual economic model, which will allow you to build the appropriate research design. Nothing substitutes for careful reasoning and economic theory, as they are the necessary conditions for good research design.\nReturns to marriage and unobserved heterogeneity. When might this be true?", "start_char_idx": 9791, "end_char_idx": 14045, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5700686b-089a-4f58-9661-dd1cb0c976ed": {"__data__": {"id_": "5700686b-089a-4f58-9661-dd1cb0c976ed", "embedding": null, "metadata": {"page number": "422 - 431", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "38d19395-2e08-4e6f-9cdc-39326b0ef108", "node_type": "4", "metadata": {"page number": "422 - 431", "chapter": "Difference in Differences"}, "hash": "42a89c20a9eb42c03b762e9a46e2b915a28c582534ac2ad2fb4ebdb40458b210", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ace06f35-cb61-4b0d-8c48-df4e94eb0243", "node_type": "1", "metadata": {"page number": "422 - 431", "chapter": "Difference in Differences"}, "hash": "121b4ae6c7a837b7413af2108a8412188f0e6936a70f2cda4e7f7ee52df9fc5f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f64d852-8973-480b-b100-06a32c77ff15", "node_type": "1", "metadata": {}, "hash": "af64765db202ddee9f16e397c6beebac48dd0913c4518355329e2c96ef4bb356", "class_name": "RelatedNodeInfo"}}, "text": "In this situation, the demeaning has simply demeaned an unobserved time- variant variable, which is then moved into the composite error term, and which since time-demeaned \u00a8uit correlated with \u00a8Dit, \u00a8Dit remains endogenous. Again, look carefully at the DAG\u2014panel \ufb01xed effects are only appropriate if ui is unchanging. Otherwise it\u2019s just another form of\nomitted variable bias. So, that said, don\u2019t just blindly use \ufb01xed effects and think that it solves your omitted variable bias problem\u2014in the same way that you shouldn\u2019t use matching just because it\u2019s convenient to do. You need a DAG, based on an actual economic model, which will allow you to build the appropriate research design. Nothing substitutes for careful reasoning and economic theory, as they are the necessary conditions for good research design.\nReturns to marriage and unobserved heterogeneity. When might this be true? Let\u2019s use an example from Cornwell and Rupert [1997] in which the authors attempt to estimate the causal effect of marriage on earn- ings. It\u2019s a well-known stylized fact that married men earn more than unmarried men, even controlling for observables. But the question is whether that correlation is causal, or whether it re\ufb02ects unobserved heterogeneity (i.e., selection bias).\nSo let\u2019s say that we had panel data on individuals. These individu- als i are observed for four periods t. We are interested in the following equation:8\nLet the outcome be their wage Yit observed in each period, and which changes each period. Let wages be a function of marriage. Since peo- ple\u2019s marital status changes over time, the marriage variable is allowed to change value over time. But race and gender, in most scenarios, do not ordinarily change over time; these are variables which are ordinarily unchanging, or what you may sometimes hear called \u201ctime invariant.\u201d Finally, the variables Ai and \u03b3i are variables which are unobserved, vary cross-sectionally across the sample, but do not vary over time. I will call these measures of unobserved ability, which may refer to any \ufb01xed endowment in the person, like \ufb01xed cognitive ability or noncognitive ability such as \u201cgrit.\u201d The key here is that it is unit-speci\ufb01c, unob- served, and time-invariant. The \u03b5it is the unobserved determinants of wages which are assumed to be uncorrelated with marriage and other covariates.\nCornwell and Rupert [1997] estimate both a feasible generalized least squares model and three \ufb01xed effects models (each of which\n8 We use the same notation as used in their paper, as opposed to the \u00a8Y notation\npresented earlier.\nTable 67. Estimated wage regressions.\nEducation controls Tenure Quadratics in years married\nincludes different time-varying controls). The authors call the \ufb01xed effects regression a \u201cwithin\u201d estimator, because it uses the within unit variation for eliminating the confounding. Their estimates are presented in Table 67.\nNotice that the FGLS (column 1) \ufb01nds a strong marriage premium of around 8.3%. But, once we begin estimating \ufb01xed effects models, the effect gets smaller and less precise. The inclusion of marriage characteristics, such as years married and job tenure, causes the coef- \ufb01cient on marriage to fall by around 60% from the FGLS estimate, and is no longer statistically signi\ufb01cant at the 5% level.", "start_char_idx": 13159, "end_char_idx": 16448, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9f64d852-8973-480b-b100-06a32c77ff15": {"__data__": {"id_": "9f64d852-8973-480b-b100-06a32c77ff15", "embedding": null, "metadata": {"page number": "431 - 432", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2f0786ff-e065-4248-a6ab-472c11b4f948", "node_type": "4", "metadata": {"page number": "431 - 432", "chapter": "Difference in Differences"}, "hash": "c2626596b758622fe5d07579b50b7b6e3e0ba423257afd6b2a01d7317514f335", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5700686b-089a-4f58-9661-dd1cb0c976ed", "node_type": "1", "metadata": {"page number": "422 - 431", "chapter": "Difference in Differences"}, "hash": "59ba7a96a06ee0acffca3f423ddfab0ee3b6bf918ea18b1da021b2b17016daf8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7b4e8419-e87c-48cb-baa8-d33e17d47c5b", "node_type": "1", "metadata": {}, "hash": "466ba2efed4b0a444c0cba8a73ac0c95d8ff1440380c0ddf57f8dc3c5ffa138d", "class_name": "RelatedNodeInfo"}}, "text": "Data Exercise: Survey of Adult Service Providers:\nNext I\u2019d like to introduce a Stata exercise based on data collection for my own research: a survey of sex workers. You may or may not know this, but the Internet has had a profound effect on sex markets. It has moved sex work indoors while simultaneously breaking the tradi- tional link between sex workers and pimps. It has increased safety and anonymity, too, which has had the effect of causing new entrants. The marginal sex worker has more education and better outside options than traditional US sex workers [Cunningham and Kendall, 2011, 2014, 2016]. The Internet, in sum, caused the marginal sex worker to shift towards women more sensitive to detection, harm, and arrest.\nIn 2008 and 2009, I surveyed (with Todd Kendall) approximately 700 US Internet-mediated sex workers. The survey was a basic labor- market survey; I asked them about their illicit and legal labor-market experiences, and about demographics. The survey had two parts:\nTable 67. Estimated wage regressions.\nEducation controls Tenure Quadratics in years married\nincludes different time-varying controls). The authors call the \ufb01xed effects regression a \u201cwithin\u201d estimator, because it uses the within unit variation for eliminating the confounding. Their estimates are presented in Table 67.\nNotice that the FGLS (column 1) \ufb01nds a strong marriage premium of around 8.3%. But, once we begin estimating \ufb01xed effects models, the effect gets smaller and less precise. The inclusion of marriage characteristics, such as years married and job tenure, causes the coef- \ufb01cient on marriage to fall by around 60% from the FGLS estimate, and is no longer statistically signi\ufb01cant at the 5% level.", "start_char_idx": 0, "end_char_idx": 1709, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7b4e8419-e87c-48cb-baa8-d33e17d47c5b": {"__data__": {"id_": "7b4e8419-e87c-48cb-baa8-d33e17d47c5b", "embedding": null, "metadata": {"page number": "432 - 442", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eaf8775c-846e-4264-b2bc-09d41fc9e62a", "node_type": "4", "metadata": {"page number": "432 - 442", "chapter": "Difference in Differences"}, "hash": "48fac9bea6827f812c937426a94a7ae6f2510286bdb1e2bc770fcc8d5a35a20f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9f64d852-8973-480b-b100-06a32c77ff15", "node_type": "1", "metadata": {"page number": "431 - 432", "chapter": "Difference in Differences"}, "hash": "6a572950d593beaba9be9383dca2a3188a89659a7fbaee8869b3529f9fc57c75", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "731c500d-b0d3-412c-9f42-8f1f60295925", "node_type": "1", "metadata": {}, "hash": "01235920b792f987a965602836977fe5722cbb54c33dabe27708e79432193f36", "class_name": "RelatedNodeInfo"}}, "text": "Data Exercise: Survey of Adult Service Providers:\nNext I\u2019d like to introduce a Stata exercise based on data collection for my own research: a survey of sex workers. You may or may not know this, but the Internet has had a profound effect on sex markets. It has moved sex work indoors while simultaneously breaking the tradi- tional link between sex workers and pimps. It has increased safety and anonymity, too, which has had the effect of causing new entrants. The marginal sex worker has more education and better outside options than traditional US sex workers [Cunningham and Kendall, 2011, 2014, 2016]. The Internet, in sum, caused the marginal sex worker to shift towards women more sensitive to detection, harm, and arrest.\nIn 2008 and 2009, I surveyed (with Todd Kendall) approximately 700 US Internet-mediated sex workers. The survey was a basic labor- market survey; I asked them about their illicit and legal labor-market experiences, and about demographics. The survey had two parts:\na \u201cstatic\u201d provider-speci\ufb01c section and a \u201cpanel\u201d section. The panel section asked respondents to share information about each of the previous four sessions with clients.9\nI have created a shortened version of the data set and uploaded it to Github. It includes a few time-invariant provider characteristics, such as race, age, marital status, years of schooling, and body mass index, as well as several time-variant session-speci\ufb01c characteristics includ- ing the log of the hourly price, the log of the session length (in hours), characteristics of the client himself, whether a condom was used in any capacity during the session, whether the client was a \u201cregular,\u201d etc. In this exercise, you will estimate three types of models: a pooled OLS model, a \ufb01xed effects (FE), and a demeaned OLS model. The model will be of the following form:\nwhere ui is both unobserved and correlated with Zis.\nThe \ufb01rst regression model will be estimated with pooled OLS and the second model will be estimated using both \ufb01xed effects and OLS. In other words, I\u2019m going to have you estimate the model using canned routines in Stata and R with individual \ufb01xed effects, as well as demean the data manually and estimate the demeaned regression using OLS.\nNotice that the second regression has a different notation on the dependent and independent variable; it represents the fact that the variables are columns of demeaned variables. Thus \u00a8Yis = Yis \u2212 Yi. Sec- ondly, notice that the time-invariant Xi variables are missing from the second equation. Do you understand why that is the case? These vari- ables have also been demeaned, but since the demeaning is across time, and since these time-invariant variables do not change over time, the demeaning deletes them from the expression. Notice, also, that the unobserved individual speci\ufb01c heterogeneity, ui, has disappeared.\n9 Technically, I asked them to share about the last \ufb01ve sessions, but for this\nexercise, I have dropped the \ufb01fth due to low response rates on the \ufb01fth session.\nIt has disappeared for the same reason that the Xi terms are gone\u2014 because the mean of ui over time is itself, and thus the demeaning deletes it.\nLet\u2019s examine these models using the following R and Stata pro-\nclear 2 tsset id session 3 foreach x of varlist lnw age asq bmi hispanic black other asian schooling cohab married divorced separated age_cl unsafe llength reg asq_cl appearance_cl provider_second asian_cl black_cl hispanic_cl othrace_cl hot massage_cl\n4 drop if `x'==.", "start_char_idx": 0, "end_char_idx": 3490, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "731c500d-b0d3-412c-9f42-8f1f60295925": {"__data__": {"id_": "731c500d-b0d3-412c-9f42-8f1f60295925", "embedding": null, "metadata": {"page number": "432 - 442", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eaf8775c-846e-4264-b2bc-09d41fc9e62a", "node_type": "4", "metadata": {"page number": "432 - 442", "chapter": "Difference in Differences"}, "hash": "48fac9bea6827f812c937426a94a7ae6f2510286bdb1e2bc770fcc8d5a35a20f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7b4e8419-e87c-48cb-baa8-d33e17d47c5b", "node_type": "1", "metadata": {"page number": "432 - 442", "chapter": "Difference in Differences"}, "hash": "6d9ef5ab90afdc9b6bbffebe02755c2a54d08a23a83942f8cb4285078529baf0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "94272ef9-035b-4b90-ac77-eab8478e093f", "node_type": "1", "metadata": {}, "hash": "7d315573823a4e9e4987baeecb4af53b91b8313cee54b14a8d142ffe6569a1a4", "class_name": "RelatedNodeInfo"}}, "text": "Notice, also, that the unobserved individual speci\ufb01c heterogeneity, ui, has disappeared.\n9 Technically, I asked them to share about the last \ufb01ve sessions, but for this\nexercise, I have dropped the \ufb01fth due to low response rates on the \ufb01fth session.\nIt has disappeared for the same reason that the Xi terms are gone\u2014 because the mean of ui over time is itself, and thus the demeaning deletes it.\nLet\u2019s examine these models using the following R and Stata pro-\nclear 2 tsset id session 3 foreach x of varlist lnw age asq bmi hispanic black other asian schooling cohab married divorced separated age_cl unsafe llength reg asq_cl appearance_cl provider_second asian_cl black_cl hispanic_cl othrace_cl hot massage_cl\n4 drop if `x'==. 5 bysort id: gen s=_N 6 keep if s==4 7 foreach x of varlist lnw age asq bmi hispanic black other asian schooling cohab married divorced separated age_cl unsafe llength reg asq_cl appearance_cl provider_second asian_cl black_cl hispanic_cl othrace_cl hot massage_cl\n8 9 egen mean_`x'=mean(`x'), by(id) 10 gen demean_`x'=`x' - mean_`x' 11 drop mean* 12 13 xi: reg lnw age asq bmi hispanic black other asian schooling cohab married\ndivorced separated age_cl unsafe llength reg asq_cl appearance_cl provider_second asian_cl black_cl hispanic_cl othrace_cl hot massage_cl, robust\n14 xi: xtreg lnw age asq bmi hispanic black other asian schooling cohab married\ndivorced separated age_cl unsafe llength reg asq_cl appearance_cl provider_second asian_cl black_cl hispanic_cl othrace_cl hot massage_cl, fe i(id) robust\ndemean_black demean_other demean_asian demean_schooling demean_cohab demean_married demean_divorced demean_separated demean_age_cl demean_unsafe demean_llength demean_reg demean_asq_cl demean_appearance_cl demean_provider_second demean_asian_cl demean_black_cl demean_hispanic_cl demean_othrace_cl demean_hot demean_massage_cl, robust cluster(id)\n1 library(tidyverse) 2 library(haven) 3 library(estimatr) 4 library(plm) 5 6 read_data <- function(df) 7 { 8 9 10 11 12 } 13 14 sasp <- read_data(\"sasp_panel.dta\") 15 16 #-- Delete all NA 17 sasp <- na.omit(sasp) 18 19 #-- order by id and session 20 sasp <- sasp %>% 21 22 23 #Balance Data 24 balanced_sasp <- make.pbalanced(sasp, 25 26 27 #Demean Data 28 balanced_sasp <- balanced_sasp %>% mutate( 29 30 31 32 33 34 35 36\ndemean_lnw = lnw - ave(lnw, id), demean_age = age - ave(age, id), demean_asq = asq - ave(asq, id), demean_bmi = bmi - ave(bmi, id), demean_hispanic = hispanic - ave(hispanic, id), demean_black = black - ave(black, id), demean_other = other - ave(other, id), demean_asian = asian - ave(asian, id),\ndf, sep = \"\")\n(continued)\nR (continued)\ndemean_schooling = schooling - ave(schooling, id), demean_cohab = cohab - ave(cohab, id), demean_married = married - ave(married, id), demean_divorced = divorced - ave(divorced, id), demean_separated = separated - ave(separated, id), demean_age_cl = age_cl - ave(age_cl, id), demean_unsafe = unsafe - ave(unsafe, id), demean_llength = llength - ave(llength, id), demean_reg = reg - ave(reg, id), demean_asq_cl = asq_cl - ave(asq_cl, id), demean_appearance_cl = appearance_cl - ave(appearance_cl, id), demean_provider_second = provider_second - ave(provider_second, id), demean_asian_cl = asian_cl - ave(asian_cl, id), demean_black_cl = black_cl - ave(black_cl, id), demean_hispanic_cl = hispanic_cl - ave(hispanic_cl, id), demean_othrace_cl = othrace_cl - ave(lnw, id), demean_hot = hot - ave(hot, id), demean_massage_cl = massage_cl - ave(massage_cl, id) )\nschooling + cohab + married + divorced + separated +\nschooling +\ncohab + married + divorced + separated + age_cl + unsafe + llength + reg + asq_cl + appearance_cl + provider_second + asian_cl + black_cl + hispanic_cl + othrace_cl + hot + massage_cl\")\n(continued)\nR (continued)\n71 model_fe <- lm_robust(formula = formula, 72 73 74 75 76 summary(model_fe) 77 78 #-- Demean OLS 79 dm_formula <- as.formula(\"demean_lnw ~ demean_age + demean_asq +\ndata = balanced_sasp, clusters = id, se_type = \"stata\")\nA few comments about this analysis.", "start_char_idx": 2762, "end_char_idx": 6800, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "94272ef9-035b-4b90-ac77-eab8478e093f": {"__data__": {"id_": "94272ef9-035b-4b90-ac77-eab8478e093f", "embedding": null, "metadata": {"page number": "432 - 442", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eaf8775c-846e-4264-b2bc-09d41fc9e62a", "node_type": "4", "metadata": {"page number": "432 - 442", "chapter": "Difference in Differences"}, "hash": "48fac9bea6827f812c937426a94a7ae6f2510286bdb1e2bc770fcc8d5a35a20f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "731c500d-b0d3-412c-9f42-8f1f60295925", "node_type": "1", "metadata": {"page number": "432 - 442", "chapter": "Difference in Differences"}, "hash": "42124e494b7b5bd6067f9f49d0d9801b12e406ae552e2ae5ae518ae8aa10665e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f5b8de4c-93f5-41cf-bb3e-c5ebf22c02e6", "node_type": "1", "metadata": {}, "hash": "2fbefbdd0d25903c819d63c31908ef77904be2788c264a5e318e1d4c66cd8e98", "class_name": "RelatedNodeInfo"}}, "text": "Some of the respondents left certain questions blank, most likely due to concerns about anonymity and privacy. So I dropped anyone who had missing values for the sake of this exercise. This leaves us with a balanced panel. I have organized the output into Table 68. There\u2019s a lot of interesting information in these three columns, some of which may surprise you if only for the novelty of the regressions. So let\u2019s talk about the statistically signi\ufb01cant ones. The pooled OLS regressions, recall, do not control for unobserved het- erogeneity, because by de\ufb01nition those are unobservable. So these are potentially biased by the unobserved heterogeneity, which is a kind of selection bias, but we will discuss them anyhow.\nFirst, a simple scan of the second and third column will show that the \ufb01xed effects regression which included (not shown) dummies for the individual herself is equivalent to a regression on the demeaned\nUnprotected sex with client of any kind\nClient was a Regular\nNote: Heteroskedastic robust standard errors in parentheses clustered at the provider level. *p < 0.10, **p < 0.05, ***p < 0.01\ndata. This should help persuade you that the \ufb01xed effects and the demeaned (within) estimators are yielding the same coe\ufb03cients.\nBut second, let\u2019s dig into the results. One of the \ufb01rst things we observe is that in the pooled OLS model, there is not a compensating wage differential detectable on having unprotected sex with a client.10 But, notice that in the \ufb01xed effects model, unprotected sex has a pre- mium. This is consistent with Rosen [1986] who posited the existence of risk premia, as well as Gertler et al. [2005] who found risk premia for sex workers using panel data. Gertler et al. [2005], though, \ufb01nd a much larger premia of over 20% for unprotected sex, whereas I am \ufb01nding only a mere 5%. This could be because a large number of the unprotected instances are fellatio, which carries a much lower risk of infection than unprotected receptive intercourse. Nevertheless, it is interesting that unprotected sex, under the assumption of strict exo- geneity, appears to cause wages to rise by approximately 5%, which is statistically signi\ufb01cant at the 10% level. Given an hourly wage of $262,\n10 There were three kinds of sexual encounter\u2014vaginal receptive sex, anal recep-\ntive sex, and fellatio. Unprotected sex is coded as any sex act without a condom.\nthis amounts to a mere 13 additional dollars per hour. The lack of a \ufb01nding in the pooled OLS model seems to suggest that the unobserved heterogeneity was masking the effect.\nNext we look at the session length. Note that I have already adjusted the price the client paid for the length of the session so that the outcome is a log wage, as opposed to a log price. As this is a log-log regression, we can interpret the coe\ufb03cient on log length as an elastic- ity. When we use \ufb01xed effects, the elasticity increases from \u22120.308 to \u22120.435. The signi\ufb01cance of this result, in economic terms, though, is that there appear to be \u201cvolume discounts\u201d in sex work. That is, longer sessions are more expensive, but at a decreasing rate. Another inter- esting result is whether the client was a \u201cregular,\u201d which meant that she had seen him before in another session. In our pooled OLS model, regulars paid 4.7% less, but this shrinks slightly in our \ufb01xed effects model to 3.7% reductions. Economically, this could be lower because new clients pose risks that repeat customers do not pose. Thus, if we expect prices to move closer to marginal cost, the disappearance of some of the risk from the repeated session should lower price, which it appears to do.\nAnother factor related to price is the attractiveness of the client. Interestingly, this does not go in the direction we may have expected. One might expect that the more attractive the client, the less he pays. But in fact it is the opposite. Given other research that \ufb01nds beautiful people earn more money [Hamermesh and Biddle, 1994], it\u2019s possi- ble that sex workers are price-discriminating. That is, when they see a handsome client, they deduce he earns more, and therefore charge him more.", "start_char_idx": 6801, "end_char_idx": 10920, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f5b8de4c-93f5-41cf-bb3e-c5ebf22c02e6": {"__data__": {"id_": "f5b8de4c-93f5-41cf-bb3e-c5ebf22c02e6", "embedding": null, "metadata": {"page number": "432 - 442", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eaf8775c-846e-4264-b2bc-09d41fc9e62a", "node_type": "4", "metadata": {"page number": "432 - 442", "chapter": "Difference in Differences"}, "hash": "48fac9bea6827f812c937426a94a7ae6f2510286bdb1e2bc770fcc8d5a35a20f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "94272ef9-035b-4b90-ac77-eab8478e093f", "node_type": "1", "metadata": {"page number": "432 - 442", "chapter": "Difference in Differences"}, "hash": "1e640b687e90643fa4315a34515fa236e9032be8299d40b45b0c403ba365c8ba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "95dbb194-29c3-4541-b713-eea7f48ebe3e", "node_type": "1", "metadata": {}, "hash": "cdac9f568bf35b5339ba13b441bf3454db0fc32bcdb0e33a4ee717aa4b4f7cbf", "class_name": "RelatedNodeInfo"}}, "text": "Economically, this could be lower because new clients pose risks that repeat customers do not pose. Thus, if we expect prices to move closer to marginal cost, the disappearance of some of the risk from the repeated session should lower price, which it appears to do.\nAnother factor related to price is the attractiveness of the client. Interestingly, this does not go in the direction we may have expected. One might expect that the more attractive the client, the less he pays. But in fact it is the opposite. Given other research that \ufb01nds beautiful people earn more money [Hamermesh and Biddle, 1994], it\u2019s possi- ble that sex workers are price-discriminating. That is, when they see a handsome client, they deduce he earns more, and therefore charge him more. This result does not hold up when including \ufb01xed effects, though, suggesting that it is due to unobserved heterogeneity, at least in part.\nSimilar to unprotected sex, a second provider present has a posi- tive effect on price, which is only detectable in the \ufb01xed effects model. Controlling for unobserved heterogeneity, the presence of a second provider increases prices by 11.3%. We also see that she discriminates against clients of \u201cother\u201d ethnicities, who pay 14.2% more than White clients. There\u2019s a premium associated with meeting in a hotel which is considerably smaller when controlling for provider \ufb01xed effects by almost a third. This positive effect, even in the \ufb01xed effects model, may\nsimply represent the higher costs associated with meeting in a hotel room. The other coe\ufb03cients are not statistically signi\ufb01cant.\nMany of the time-invariant results are also interesting, though. For instance, perhaps not surprisingly, women with higher BMI earn less. Hispanics earn less than White sex workers. And women with more schooling earn more, something which is explored in greater detail in Cunningham and Kendall [2016].\nIn conclusion, we have been exploring the usefulness of panel data for estimating causal effects. We noted that the \ufb01xed effects (within) estimator is a very useful method for addressing a very spe- ci\ufb01c form of endogeneity, with some caveats. First, it will eliminate any and all unobserved and observed time-invariant covariates correlated with the treatment variable. So long as the treatment and the outcome varies over time, and there is strict exogeneity, then the \ufb01xed effects (within) estimator will identify the causal effect of the treatment on some outcome.\nBut this came with certain quali\ufb01cations. For one, the method couldn\u2019t handle time-variant unobserved heterogeneity. It\u2019s thus the burden of the researcher to determine which type of unobserved het- erogeneity problem they face, but if they face the latter, then the panel methods reviewed here are not unbiased and consistent. Sec- ond, when there exists strong reverse causality pathways, then panel methods are biased. Thus, we cannot solve the problem of simultane- ity, such as what Wright faced when estimating the price elasticity of demand, using the \ufb01xed effects (within) estimator. Most likely, we are going to have to move into a different framework when facing that kind of problem.\nStill, many problems in the social sciences may credibly be caused by a time-invariant unobserved heterogeneity problem, in which case the \ufb01xed effects (within) panel estimator is useful and appropriate.\nsimply represent the higher costs associated with meeting in a hotel room. The other coe\ufb03cients are not statistically signi\ufb01cant.\nMany of the time-invariant results are also interesting, though. For instance, perhaps not surprisingly, women with higher BMI earn less. Hispanics earn less than White sex workers. And women with more schooling earn more, something which is explored in greater detail in Cunningham and Kendall [2016].\nIn conclusion, we have been exploring the usefulness of panel data for estimating causal effects. We noted that the \ufb01xed effects (within) estimator is a very useful method for addressing a very spe- ci\ufb01c form of endogeneity, with some caveats. First, it will eliminate any and all unobserved and observed time-invariant covariates correlated with the treatment variable. So long as the treatment and the outcome varies over time, and there is strict exogeneity, then the \ufb01xed effects (within) estimator will identify the causal effect of the treatment on some outcome.\nBut this came with certain quali\ufb01cations. For one, the method couldn\u2019t handle time-variant unobserved heterogeneity.", "start_char_idx": 10157, "end_char_idx": 14639, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "95dbb194-29c3-4541-b713-eea7f48ebe3e": {"__data__": {"id_": "95dbb194-29c3-4541-b713-eea7f48ebe3e", "embedding": null, "metadata": {"page number": "432 - 442", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eaf8775c-846e-4264-b2bc-09d41fc9e62a", "node_type": "4", "metadata": {"page number": "432 - 442", "chapter": "Difference in Differences"}, "hash": "48fac9bea6827f812c937426a94a7ae6f2510286bdb1e2bc770fcc8d5a35a20f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f5b8de4c-93f5-41cf-bb3e-c5ebf22c02e6", "node_type": "1", "metadata": {"page number": "432 - 442", "chapter": "Difference in Differences"}, "hash": "f739701ee184d8d36f8083a4f9645985a0731a73c6e6fa8dee2628eaee7a0fa7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ccddc612-1574-4107-b38e-e8a610165215", "node_type": "1", "metadata": {}, "hash": "7b17133a22415716b108ad0ecf7ca7240bb45ba1745f3b48ec6a1f2ec90ab21a", "class_name": "RelatedNodeInfo"}}, "text": "Hispanics earn less than White sex workers. And women with more schooling earn more, something which is explored in greater detail in Cunningham and Kendall [2016].\nIn conclusion, we have been exploring the usefulness of panel data for estimating causal effects. We noted that the \ufb01xed effects (within) estimator is a very useful method for addressing a very spe- ci\ufb01c form of endogeneity, with some caveats. First, it will eliminate any and all unobserved and observed time-invariant covariates correlated with the treatment variable. So long as the treatment and the outcome varies over time, and there is strict exogeneity, then the \ufb01xed effects (within) estimator will identify the causal effect of the treatment on some outcome.\nBut this came with certain quali\ufb01cations. For one, the method couldn\u2019t handle time-variant unobserved heterogeneity. It\u2019s thus the burden of the researcher to determine which type of unobserved het- erogeneity problem they face, but if they face the latter, then the panel methods reviewed here are not unbiased and consistent. Sec- ond, when there exists strong reverse causality pathways, then panel methods are biased. Thus, we cannot solve the problem of simultane- ity, such as what Wright faced when estimating the price elasticity of demand, using the \ufb01xed effects (within) estimator. Most likely, we are going to have to move into a different framework when facing that kind of problem.\nStill, many problems in the social sciences may credibly be caused by a time-invariant unobserved heterogeneity problem, in which case the \ufb01xed effects (within) panel estimator is useful and appropriate.", "start_char_idx": 11888, "end_char_idx": 13520, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ccddc612-1574-4107-b38e-e8a610165215": {"__data__": {"id_": "ccddc612-1574-4107-b38e-e8a610165215", "embedding": null, "metadata": {"page number": "443 - 461", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "30d6cc4b-0fe1-4dd7-afdd-66923a155732", "node_type": "4", "metadata": {"page number": "443 - 461", "chapter": "Difference in Differences"}, "hash": "23176a9c9027b7e413409ef6e069bd177c677b7fbb8e90bb7dd3671848d45880", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "95dbb194-29c3-4541-b713-eea7f48ebe3e", "node_type": "1", "metadata": {"page number": "432 - 442", "chapter": "Difference in Differences"}, "hash": "51d0b92276e818392c85a2cb9667fae23bdaff427832535bcbae0ec80f694c05", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2480b9dd-1118-4902-b6ac-628c6b96048f", "node_type": "1", "metadata": {}, "hash": "6fd68f4074da7347be57103d5e1129bb842ee66ca1717da2a515086e89f63d8c", "class_name": "RelatedNodeInfo"}}, "text": "Difference- in- Differences:\nWhat\u2019s the difference between me and you? About \ufb01ve bank accounts, three ounces, and two vehicles.\nThe difference-in-differences design is an early quasi-experimental identi\ufb01cation strategy for estimating causal effects that predates the randomized experiment by roughly eighty-\ufb01ve years. It has become the single most popular research design in the quantitative social sci- ences, and as such, it merits careful study by researchers everywhere.1 In this chapter, I will explain this popular and important research design both in its simplest form, where a group of units is treated at the same time, and the more common form, where groups of units are treated at different points in time. My focus will be on the identifying assumptions needed for estimating treatment effects, including sev- eral practical tests and robustness exercises commonly performed, and I will point you to some of the work on difference-in-differences design (DD) being done at the frontier of research. I have included several replication exercises as well.\nWhen thinking about situations in which a difference-in-differences design can be used, one usually tries to \ufb01nd an instance where a\nconsequential treatment was given to some people or units but denied to others \u201chaphazardly.\u201d This is sometimes called a \u201cnatural experi- ment\u201d because it is based on naturally occurring variation in some treatment variable that affects only some units over time. All good difference-in-differences designs are based on some kind of natural experiment. And one of the most interesting natural experiments was also one of the \ufb01rst difference-in-differences designs. This is the story of how John Snow convinced the world that cholera was transmitted by water, not air, using an ingenious natural experiment [Snow, 1855]. Cholera is a vicious disease that attacks victims suddenly, with acute symptoms such as vomiting and diarrhea. In the nineteenth cen- tury, it was usually fatal. There were three main epidemics that hit London, and like a tornado, they cut a path of devastation through the city. Snow, a physician, watched as tens of thousands suffered and died from a mysterious plague. Doctors could not help the vic- tims because they were mistaken about the mechanism that caused cholera to spread between people.\nThe majority medical opinion about cholera transmission at that time was miasma, which said diseases were spread by microscopic poisonous particles that infected people by \ufb02oating through the air. These particles were thought to be inanimate, and because micro- scopes at that time had incredibly poor resolution, it would be years before microorganisms would be seen. Treatments, therefore, tended to be designed to stop poisonous dirt from spreading through the air. But tried and true methods like quarantining the sick were strangely ineffective at slowing down this plague.\nJohn Snow worked in London during these epidemics. Originally, Snow\u2014like everyone\u2014accepted the miasma theory and tried many ingenious approaches based on the theory to block these airborne poisons from reaching other people. He went so far as to cover the sick with burlap bags, for instance, but the disease still spread. People kept getting sick and dying. Faced with the theory\u2019s failure to explain cholera, he did what good scientists do\u2014he changed his mind and began look for a new explanation.\nSnow developed a novel theory about cholera in which the active agent was not an inanimate particle but was rather a living organism. This microorganism entered the body through food and drink, \ufb02owed\nthrough the alimentary canal where it multiplied and generated a poi- son that caused the body to expel water. With each evacuation, the organism passed out of the body and, importantly, \ufb02owed into Eng- land\u2019s water supply. People unknowingly drank contaminated water from the Thames River, which caused them to contract cholera. As they did, they would evacuate with vomit and diarrhea, which would \ufb02ow into the water supply again and again, leading to new infections across the city. This process repeated through a multiplier effect which was why cholera would hit the city in epidemic waves.\nSnow\u2019s years of observing the clinical course of the disease led him to question the usefulness of miasma to explain cholera. While these were what we would call \u201canecdote,\u201d the numerous observa- tions and imperfect studies nonetheless shaped his thinking. Here\u2019s just a few of the observations which puzzled him. He noticed that cholera transmission tended to follow human commerce. A sailor on a ship from a cholera-free country who arrived at a cholera-stricken port would only get sick after landing or taking on supplies; he would not get sick if he remained docked.", "start_char_idx": 0, "end_char_idx": 4767, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2480b9dd-1118-4902-b6ac-628c6b96048f": {"__data__": {"id_": "2480b9dd-1118-4902-b6ac-628c6b96048f", "embedding": null, "metadata": {"page number": "443 - 461", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "30d6cc4b-0fe1-4dd7-afdd-66923a155732", "node_type": "4", "metadata": {"page number": "443 - 461", "chapter": "Difference in Differences"}, "hash": "23176a9c9027b7e413409ef6e069bd177c677b7fbb8e90bb7dd3671848d45880", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ccddc612-1574-4107-b38e-e8a610165215", "node_type": "1", "metadata": {"page number": "443 - 461", "chapter": "Difference in Differences"}, "hash": "76103120c7c3cffeb58254b485e1a0807d1d6749095c4e2cf218da9c9b62f1dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fe104bc7-25d8-4164-9c4b-7d5de1c59a12", "node_type": "1", "metadata": {}, "hash": "41bb83fb49a03d4874cf2dfb9ce7b7dd1988ac7b053afb596d2a2fe7aa3cf620", "class_name": "RelatedNodeInfo"}}, "text": "People unknowingly drank contaminated water from the Thames River, which caused them to contract cholera. As they did, they would evacuate with vomit and diarrhea, which would \ufb02ow into the water supply again and again, leading to new infections across the city. This process repeated through a multiplier effect which was why cholera would hit the city in epidemic waves.\nSnow\u2019s years of observing the clinical course of the disease led him to question the usefulness of miasma to explain cholera. While these were what we would call \u201canecdote,\u201d the numerous observa- tions and imperfect studies nonetheless shaped his thinking. Here\u2019s just a few of the observations which puzzled him. He noticed that cholera transmission tended to follow human commerce. A sailor on a ship from a cholera-free country who arrived at a cholera-stricken port would only get sick after landing or taking on supplies; he would not get sick if he remained docked. Cholera hit the poorest commu- nities worst, and those people were the very same people who lived in the most crowded housing with the worst hygiene. He might \ufb01nd two apartment buildings next to one another, one would be heavily hit with cholera, but strangely the other one wouldn\u2019t. He then noticed that the \ufb01rst building would be contaminated by runoff from privies but the water supply in the second building was cleaner. While these observa- tions weren\u2019t impossible to reconcile with miasma, they were de\ufb01nitely unusual and didn\u2019t seem obviously consistent with miasmis.\nSnow tucked away more and more anecdotal evidence like these. But, while this evidence raised some doubts in his mind, he was not convinced. He needed a smoking gun if he were to eliminate all doubt that cholera was spread by water, not air. But where would he \ufb01nd that evidence? More importantly, what would evidence like that even look like?\nLet\u2019s imagine the following thought experiment. If Snow was a dic- tator with unlimited wealth and power, how could he test his theory that cholera is waterborne? One thing he could do is \ufb02ip a coin over each household member\u2014heads you drink from the contaminated Thames,\ntails you drink from some uncontaminated source. Once the assign- ments had been made, Snow could simply compare cholera mortality between the two groups. If those who drank the clean water were less likely to contract cholera, then this would suggest that cholera was waterborne.\nKnowledge that physical randomization could be used to identify causal effects was still eighty-\ufb01ve years away. But there were other issues besides ignorance that kept Snow from physical randomization. Experiments like the one I just described are also impractical, infeasi- ble, and maybe even unethical\u2014which is why social scientists so often rely on natural experiments that mimic important elements of ran- domized experiments. But what natural experiment was there? Snow needed to \ufb01nd a situation where uncontaminated water had been dis- tributed to a large number of people as if by random chance, and then calculate the difference between those those who did and did not drink contaminated water. Furthermore, the contaminated water would need to be allocated to people in ways that were unrelated to the ordinary determinants of cholera mortality, such as hygiene and poverty, imply- ing a degree of balance on covariates between the groups. And then he remembered\u2014a potential natural experiment in London a year earlier had reallocated clean water to citizens of London. Could this work?\nIn the 1800s, several water companies served different areas of the city. Some neighborhoods were even served by more than one company. They took their water from the Thames, which had been pol- luted by victims\u2019 evacuations via runoff. But in 1849, the Lambeth water company had moved its intake pipes upstream higher up the Thames, above the main sewage discharge point, thus giving its customers uncontaminated water. They did this to obtain cleaner water, but it had the added bene\ufb01t of being too high up the Thames to be infected with cholera from the runoff. Snow seized on this opportunity. He realized that it had given him a natural experiment that would allow him to test his hypothesis that cholera was waterborne by comparing the house- holds. If his theory was right, then the Lambeth houses should have lower cholera death rates than some other set of households whose water was infected with runoff\u2014what we might call today the explicit counterfactual. He found his explicit counterfactual in the Southwark and Vauxhall Waterworks Company.", "start_char_idx": 3824, "end_char_idx": 8393, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fe104bc7-25d8-4164-9c4b-7d5de1c59a12": {"__data__": {"id_": "fe104bc7-25d8-4164-9c4b-7d5de1c59a12", "embedding": null, "metadata": {"page number": "443 - 461", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "30d6cc4b-0fe1-4dd7-afdd-66923a155732", "node_type": "4", "metadata": {"page number": "443 - 461", "chapter": "Difference in Differences"}, "hash": "23176a9c9027b7e413409ef6e069bd177c677b7fbb8e90bb7dd3671848d45880", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2480b9dd-1118-4902-b6ac-628c6b96048f", "node_type": "1", "metadata": {"page number": "443 - 461", "chapter": "Difference in Differences"}, "hash": "fa2d2253aafc47ff2cd23e7c499839013fded147595b3af5ea450041e5b05610", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc223f22-48e6-4f9e-8605-cf8f5d56652b", "node_type": "1", "metadata": {}, "hash": "740cc631e54976905fb35f50a2a14d0c6bc9f19b5f5766b8301081a295e9ffe6", "class_name": "RelatedNodeInfo"}}, "text": "They took their water from the Thames, which had been pol- luted by victims\u2019 evacuations via runoff. But in 1849, the Lambeth water company had moved its intake pipes upstream higher up the Thames, above the main sewage discharge point, thus giving its customers uncontaminated water. They did this to obtain cleaner water, but it had the added bene\ufb01t of being too high up the Thames to be infected with cholera from the runoff. Snow seized on this opportunity. He realized that it had given him a natural experiment that would allow him to test his hypothesis that cholera was waterborne by comparing the house- holds. If his theory was right, then the Lambeth houses should have lower cholera death rates than some other set of households whose water was infected with runoff\u2014what we might call today the explicit counterfactual. He found his explicit counterfactual in the Southwark and Vauxhall Waterworks Company.\nUnlike Lambeth, the Southwark and Vauxhall Waterworks Com- pany had not moved their intake point upstream, and Snow spent an entire book documenting similarities between the two companies\u2019 households. For instance, sometimes their service cut an irregular path through neighborhoods and houses such that the households on either side were very similar; the only difference being they drank different water with different levels of contamination from runoff. Inso- far as the kinds of people that each company serviced were obser- vationally equivalent, then perhaps they were similar on the relevant unobservables as well.\nSnow meticulously collected data on household enrollment in water supply companies, going door to door asking household heads the name of their utility company. Sometimes these individuals didn\u2019t know, though, so he used a saline test to determine the source him- self [Coleman, 2019]. He matched those data with the city\u2019s data on the cholera death rates at the household level. It was in many ways as advanced as any study we might see today for how he carefully collected, prepared, and linked a variety of data sources to show the relationship between water purity and mortality. But he also displayed scienti\ufb01c ingenuity for how he carefully framed the research question and how long he remained skeptical until the research design\u2019s results convinced him otherwise. After combining everthing, he was able to generate extremely persuasive evidence that in\ufb02uenced policymakers in the city.2\nSnow wrote up all of his analysis in a manuscript entitled On the Mode of Communication of Cholera [Snow, 1855]. Snow\u2019s main evi- dence was striking, and I will discuss results based on Table XII and Table IX (not shown) in Table 69. The main difference between my ver- sion and his version of Table XII is that I will use his data to estimate a treatment effect using difference-in-differences.\nTable XII. In 1849, there were 135 cases of cholera per 10,000 house- holds at Southwark and Vauxhall and 85 for Lambeth. But in 1854, there\n2 John Snow is one of my personal heroes. He had a stubborn commitment to the\ntruth and was unpersuaded by low-quality causal evidence. That simultaneous skepti-\ncism and open-mindedness gave him the willingness to question common sense when\ncommon sense failed to provide satisfactory explanations.\nwere 147 per 100,000 in Southwark and Vauxhall, whereas Lambeth\u2019s cholera cases per 10,000 households fell to 19.\nWhile Snow did not explicitly calculate the difference-in-differences, the ability to do so was there [Coleman, 2019]. If we difference Lam- beth\u2019s 1854 value from its 1849 value, followed by the same after and before differencing for Southwark and Vauxhall, we can calculate an estimate of the ATT equaling 78 fewer deaths per 10,000. While Snow would go on to produce evidence showing cholera deaths were con- centrated around a pump on Broad Street contaminated with cholera, he allegedly considered the simple difference-in-differences the more convincing test of his hypothesis.\nThe importance of the work Snow undertook to understand the causes of cholera in London cannot be overstated. It not only lifted our ability to estimate causal effects with observational data, it advanced science and ultimately saved lives. Of Snow\u2019s work on the cause of cholera transmission, Freedman [1991] states:\nThe force of [Snow\u2019s] argument results from the clarity of the prior reasoning, the bringing together of many different lines of evi- dence, and the amount of shoe leather Snow was willing to use to get the data. Snow did some brilliant detective work on nonexperi- mental data.", "start_char_idx": 7475, "end_char_idx": 12043, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc223f22-48e6-4f9e-8605-cf8f5d56652b": {"__data__": {"id_": "fc223f22-48e6-4f9e-8605-cf8f5d56652b", "embedding": null, "metadata": {"page number": "443 - 461", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "30d6cc4b-0fe1-4dd7-afdd-66923a155732", "node_type": "4", "metadata": {"page number": "443 - 461", "chapter": "Difference in Differences"}, "hash": "23176a9c9027b7e413409ef6e069bd177c677b7fbb8e90bb7dd3671848d45880", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fe104bc7-25d8-4164-9c4b-7d5de1c59a12", "node_type": "1", "metadata": {"page number": "443 - 461", "chapter": "Difference in Differences"}, "hash": "48260522ce0139973824904ad85c169fd9b93bdcd7a3b648f1fc4f7f5d41123b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f60c31a5-6d4a-4f95-a61e-cc26358a8662", "node_type": "1", "metadata": {}, "hash": "ce0d274ff8d1c4346756278399a156e6542e6adefbf912d0b555c32693de5057", "class_name": "RelatedNodeInfo"}}, "text": "While Snow would go on to produce evidence showing cholera deaths were con- centrated around a pump on Broad Street contaminated with cholera, he allegedly considered the simple difference-in-differences the more convincing test of his hypothesis.\nThe importance of the work Snow undertook to understand the causes of cholera in London cannot be overstated. It not only lifted our ability to estimate causal effects with observational data, it advanced science and ultimately saved lives. Of Snow\u2019s work on the cause of cholera transmission, Freedman [1991] states:\nThe force of [Snow\u2019s] argument results from the clarity of the prior reasoning, the bringing together of many different lines of evi- dence, and the amount of shoe leather Snow was willing to use to get the data. Snow did some brilliant detective work on nonexperi- mental data. What is impressive is not the statistical technique but the handling of the scienti\ufb01c issues. He made steady progress from shrewd observation through case studies to analyze ecological data. In the end, he found and analyzed a natural experiment. [298]\nA simple table. Let\u2019s look at this example using some tables, which hopefully will help give you an idea of the intuition behind DD, as well as some of its identifying assumptions.3 Assume that the intervention\n3 You\u2019ll sometimes see acronyms for difference-in-differences like DD, DiD, Diff-in-\nwere 147 per 100,000 in Southwark and Vauxhall, whereas Lambeth\u2019s cholera cases per 10,000 households fell to 19.\nWhile Snow did not explicitly calculate the difference-in-differences, the ability to do so was there [Coleman, 2019]. If we difference Lam- beth\u2019s 1854 value from its 1849 value, followed by the same after and before differencing for Southwark and Vauxhall, we can calculate an estimate of the ATT equaling 78 fewer deaths per 10,000. While Snow would go on to produce evidence showing cholera deaths were con- centrated around a pump on Broad Street contaminated with cholera, he allegedly considered the simple difference-in-differences the more convincing test of his hypothesis.\nThe importance of the work Snow undertook to understand the causes of cholera in London cannot be overstated. It not only lifted our ability to estimate causal effects with observational data, it advanced science and ultimately saved lives. Of Snow\u2019s work on the cause of cholera transmission, Freedman [1991] states:\nThe force of [Snow\u2019s] argument results from the clarity of the prior reasoning, the bringing together of many different lines of evi- dence, and the amount of shoe leather Snow was willing to use to get the data. Snow did some brilliant detective work on nonexperi- mental data. What is impressive is not the statistical technique but the handling of the scienti\ufb01c issues. He made steady progress from shrewd observation through case studies to analyze ecological data. In the end, he found and analyzed a natural experiment. [298]\nA simple table. Let\u2019s look at this example using some tables, which hopefully will help give you an idea of the intuition behind DD, as well as some of its identifying assumptions.3 Assume that the intervention\n3 You\u2019ll sometimes see acronyms for difference-in-differences like DD, DiD, Diff-in-\nTable 70. Compared to what? Different companies.\nTable 71. Compared to what? Before and after.\nis clean water, which I\u2019ll write as D, and our objective is to estimate D\u2019s causal effect on cholera deaths. Let cholera deaths be represented by the variable Y. Can we identify the causal effect of D if we just compare the post-treatment 1854 Lambeth cholera death values to that of the 1854 Southwark and Vauxhall values? This is in many ways an obvious choice, and in fact, it is one of the more common naive approaches to causal inference. After all, we have a control group, don\u2019t we? Why can\u2019t we just compare a treatment group to a control group? Let\u2019s look and see.\nOne of the things we immediately must remember is that the simple difference in outcomes, which is all we are doing here, only collapsed to the ATE if the treatment had been randomized. But it is never randomized in the real world where most choices if not all choices made by real people is endogenous to potential outcomes. Let\u2019s represent now the differences between Lambeth and Southwark and Vauxhall with \ufb01xed level differences, or \ufb01xed effects, represented by L and SV. Both are unobserved, unique to each company, and \ufb01xed over time.", "start_char_idx": 11199, "end_char_idx": 15646, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f60c31a5-6d4a-4f95-a61e-cc26358a8662": {"__data__": {"id_": "f60c31a5-6d4a-4f95-a61e-cc26358a8662", "embedding": null, "metadata": {"page number": "443 - 461", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "30d6cc4b-0fe1-4dd7-afdd-66923a155732", "node_type": "4", "metadata": {"page number": "443 - 461", "chapter": "Difference in Differences"}, "hash": "23176a9c9027b7e413409ef6e069bd177c677b7fbb8e90bb7dd3671848d45880", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc223f22-48e6-4f9e-8605-cf8f5d56652b", "node_type": "1", "metadata": {"page number": "443 - 461", "chapter": "Difference in Differences"}, "hash": "5255ed0bcff8b52fdebbb7b8968eb1f7e9d8f6a2fd00588a20cd910ce2fabf76", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "455352be-d4d9-44d3-b260-cba3062cdd10", "node_type": "1", "metadata": {}, "hash": "a124be5928062447f24689e7448c43d5d543fce4e33bcb2cf98b03fc4af85685", "class_name": "RelatedNodeInfo"}}, "text": "This is in many ways an obvious choice, and in fact, it is one of the more common naive approaches to causal inference. After all, we have a control group, don\u2019t we? Why can\u2019t we just compare a treatment group to a control group? Let\u2019s look and see.\nOne of the things we immediately must remember is that the simple difference in outcomes, which is all we are doing here, only collapsed to the ATE if the treatment had been randomized. But it is never randomized in the real world where most choices if not all choices made by real people is endogenous to potential outcomes. Let\u2019s represent now the differences between Lambeth and Southwark and Vauxhall with \ufb01xed level differences, or \ufb01xed effects, represented by L and SV. Both are unobserved, unique to each company, and \ufb01xed over time. What these \ufb01xed effects mean is that even if Lam- beth hadn\u2019t changed its water source there, would still be something determining cholera deaths, which is just the time-invariant unique dif- ferences between the two companies as it relates to cholera deaths in 1854.\nTable 72. Compared to what? Difference in each company\u2019s differences.\nWhen we make a simple comparison between Lambeth and South- wark and Vauxhall, we get an estimated causal effect equalling D + (L \u2212 SV). Notice the second term, L \u2212 SV. We\u2019ve seen this before. It\u2019s the selection bias we found from the decomposition of the simple difference in outcomes from earlier in the book.\nOkay, so say we realize that we cannot simply make cross- sectional comparisons between two units because of selection bias. Surely, though, we can compare a unit to itself? This is sometimes called an interrupted time series. Let\u2019s consider that simple before-and- after difference for Lambeth now.\nWhile this procedure successfully eliminates the Lambeth \ufb01xed it doesn\u2019t give me an effect (unlike the cross-sectional difference), unbiased estimate of D because differences can\u2019t eliminate the natu- ral changes in the cholera deaths over time. Recall, these events were oscillating in waves. I can\u2019t compare Lambeth before and after (T + D) because of T, which is an omitted variable.\nThe intuition of the DD strategy is remarkably simple: combine these two simpler approaches so the selection bias and the effect in turns, eliminated. Let\u2019s look at it in the following of time are, table.\nThe \ufb01rst difference, D1, does the simple before-and-after differ- ence. This ultimately eliminates the unit-speci\ufb01c \ufb01xed effects. Then, once those differences are made, we difference the differences (hence the name) to get the unbiased estimate of D.\nBut there\u2019s a a key assumption with a DD design, and that assump- tion is discernible even in this table. We are assuming that there is no time-variant company speci\ufb01c unobservables. Nothing unobserved in Lambeth households that is changing between these two periods that also determines cholera deaths. This is equivalent to assuming that T is the same for all units. And we call this the parallel trends assumption. We will discuss this assumption repeatedly as the chapter proceeds, as it is the most important assumption in the design\u2019s engine. If you can buy off on the parallel trends assumption, then DD will identify the causal effect.\nDD is a powerful, yet amazingly simple design. Using repeated observations on a treatment and control unit (usually several units), we can eliminate the unobserved heterogeneity to provide a credible estimate of the average treatment effect on the treated (ATT) by trans- forming the data in very speci\ufb01c ways. But when and why does this process yield the correct answer? Turns out, there is more to it than meets the eye. And it is imperative on the front end that you under- stand what\u2019s under the hood so that you can avoid conceptual errors about this design.\nThe simple 2 \u00d7 2 DD. The cholera case is a particular kind of DD design that Goodman-Bacon [2019] calls the 2 \u00d7 2 DD design. The 2 \u00d7 2 DD design has a treatment group k and untreated group U. There is a pre- period for the treatment group, pre(k); a post-period for the treatment group, post(k); a pre-treatment period for the untreated group, pre(U); and a post-period for the untreated group, post(U) So:\nwhere(cid:12)\u03b4kU is the estimated ATT for group k, and y is the sample mean for that particular group in a particular time period.", "start_char_idx": 14856, "end_char_idx": 19185, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "455352be-d4d9-44d3-b260-cba3062cdd10": {"__data__": {"id_": "455352be-d4d9-44d3-b260-cba3062cdd10", "embedding": null, "metadata": {"page number": "443 - 461", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "30d6cc4b-0fe1-4dd7-afdd-66923a155732", "node_type": "4", "metadata": {"page number": "443 - 461", "chapter": "Difference in Differences"}, "hash": "23176a9c9027b7e413409ef6e069bd177c677b7fbb8e90bb7dd3671848d45880", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f60c31a5-6d4a-4f95-a61e-cc26358a8662", "node_type": "1", "metadata": {"page number": "443 - 461", "chapter": "Difference in Differences"}, "hash": "a150e6f238f161c74448e46db805229c798738c40e7e7f7fbef7381b0ed64864", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "00944c39-88e6-4f7d-87d4-a7ab32606956", "node_type": "1", "metadata": {}, "hash": "1d937b4204e8087ad22251b06ccd9140eddbcc0f1fa382e6b9d10076a72eff1f", "class_name": "RelatedNodeInfo"}}, "text": "But when and why does this process yield the correct answer? Turns out, there is more to it than meets the eye. And it is imperative on the front end that you under- stand what\u2019s under the hood so that you can avoid conceptual errors about this design.\nThe simple 2 \u00d7 2 DD. The cholera case is a particular kind of DD design that Goodman-Bacon [2019] calls the 2 \u00d7 2 DD design. The 2 \u00d7 2 DD design has a treatment group k and untreated group U. There is a pre- period for the treatment group, pre(k); a post-period for the treatment group, post(k); a pre-treatment period for the untreated group, pre(U); and a post-period for the untreated group, post(U) So:\nwhere(cid:12)\u03b4kU is the estimated ATT for group k, and y is the sample mean for that particular group in a particular time period. The \ufb01rst paragraph dif- ferences the treatment group, k, after minus before, the second para- graph differences the untreated group, U, after minus before. And once those quantities are obtained, we difference the second term from the \ufb01rst.\nBut this is simply the mechanics of calculations. What exactly is this estimated parameter mapping onto? To understand that, we must convert these sample averages into conditional expectations of\npotential outcomes. But that is easy to do when working with sample averages, as we will see here. First let\u2019s rewrite this as a conditional expectation. (cid:8)\nNow let\u2019s use the switching equation, which transforms historical quantities of Y into potential outcomes. As we\u2019ve done before, we\u2019ll do a little trick where we add zero to the right-hand side so that we can use those terms to help illustrate something important.\n\u2212 (cid:24)(cid:25) Switching equation\nNow we simply rearrange these terms to get the decomposition of\nthe 2 \u00d7 2 DD in terms of conditional expected potential outcomes.\n\u2212 (cid:24)(cid:25) Non-parallel trends bias in 2 \u00d7 2 case\nNow, let\u2019s study this last term closely. This simple 2 \u00d7 2 difference- in-differences will isolate the ATT (the \ufb01rst term) if and only if the second term zeroes out. But why would this second term be zero? It would equal zero if the \ufb01rst difference involving the treatment group, k, equaled the second difference involving the untreated group, U.\nBut notice the term in the second line. Notice anything strange about it? The object of interest is Y0, which is some outcome in a world without the treatment. But it\u2019s the post period, and in the post period, Y = Y1 not Y0 by the switching equation. Thus, the \ufb01rst term is counterfactual. And as we\u2019ve said over and over, counterfactuals are not observable. This bottom line is often called the parallel trends assumption and it is by de\ufb01nition untestable since we cannot observe this counterfactual conditional expectation. We will return to this again, but for now I simply present it for your consideration.\nDD and the Minimum Wage. Now I\u2019d like to talk about more explicit economic content, and the minimum wage is as good a topic as any. The modern use of DD was brought into the social sciences through esteemed labor economist Orley Ashenfelter [1978]. His study was no doubt in\ufb02uential to his advisee, David Card, arguably the greatest labor economist of his generation. Card would go on to use the method in several pioneering studies, such as Card [1990]. But I will focus on one in particular\u2014his now-classic minimum wage study [Card and Krueger, 1994].\nCard and Krueger [1994] is an infamous study both because of its use of an explicit counterfactual for estimation, and because the study challenges many people\u2019s common beliefs about the negative effects of the minimum wage. It lionized a massive back-and-forth minimum- wage literature that continues to this day.4 So controversial was this study that James Buchanan, the Nobel Prize winner, called those in\ufb02u- enced by Card and Krueger [1994] \u201ccamp following whores\u201d in a letter to the editor of the Wall Street Journal [Buchanan, 1996].5\nSuppose you are interested in the effect of minimum wages on employment. Theoretically, you might expect that in competitive labor markets, an increase in the minimum wage would move us up a downward-sloping demand curve, causing employment to fall. But in labor markets characterized by monopsony, minimum wages can increase employment.", "start_char_idx": 18395, "end_char_idx": 22667, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "00944c39-88e6-4f7d-87d4-a7ab32606956": {"__data__": {"id_": "00944c39-88e6-4f7d-87d4-a7ab32606956", "embedding": null, "metadata": {"page number": "443 - 461", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "30d6cc4b-0fe1-4dd7-afdd-66923a155732", "node_type": "4", "metadata": {"page number": "443 - 461", "chapter": "Difference in Differences"}, "hash": "23176a9c9027b7e413409ef6e069bd177c677b7fbb8e90bb7dd3671848d45880", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "455352be-d4d9-44d3-b260-cba3062cdd10", "node_type": "1", "metadata": {"page number": "443 - 461", "chapter": "Difference in Differences"}, "hash": "b7ee80347cdeffcbaeeead226bdcbe02bf4ff4128beb0147093e3799608eb4fa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3f23f710-7643-47b6-84fa-aed98b366026", "node_type": "1", "metadata": {}, "hash": "a2fb8d95b4683e988ff00adfbd6d847ba8d759a3f48aae4e36c7a1f3072b8d3a", "class_name": "RelatedNodeInfo"}}, "text": "Card and Krueger [1994] is an infamous study both because of its use of an explicit counterfactual for estimation, and because the study challenges many people\u2019s common beliefs about the negative effects of the minimum wage. It lionized a massive back-and-forth minimum- wage literature that continues to this day.4 So controversial was this study that James Buchanan, the Nobel Prize winner, called those in\ufb02u- enced by Card and Krueger [1994] \u201ccamp following whores\u201d in a letter to the editor of the Wall Street Journal [Buchanan, 1996].5\nSuppose you are interested in the effect of minimum wages on employment. Theoretically, you might expect that in competitive labor markets, an increase in the minimum wage would move us up a downward-sloping demand curve, causing employment to fall. But in labor markets characterized by monopsony, minimum wages can increase employment. Therefore, there are strong theoretical reasons to believe that the effect of the minimum wage on employment is ultimately an empirical question depending on many local contextual factors. This is where Card and Krueger [1994] entered. Could they uncover whether minimum wages were ultimately harmful or helpful in some local economy?\n4 That literature is too extensive to cite here, but one can \ufb01nd reviews of a great\n5 James Buchanan won the Nobel Prize for his pioneering work on the theory of\npublic choice. He was not, though, a labor economist, and to my knowledge did not\nhave experience estimating causal effects using explicit counterfactuals with observa-\ntional data. A Google Scholar search for \u201cJames Buchanan minimum wage\u201d returned only one hit, the previously mentioned Wall Street Journal letter to the editor. I consider his criticism to be ideologically motivated ad hominem and as such unhelpful in this debate.\nIt\u2019s always useful to start these questions with a simple thought experiment: if you had a billion dollars, complete discretion and could run a randomized experiment, how would you test whether minimum wages increased or decreased employment? You might go across the hundreds of local labor markets in the United States and \ufb02ip a coin\u2014 heads, you raise the minimum wage; tails, you keep it at the status quo. As we\u2019ve done before, these kinds of thought experiments are useful for clarifying both the research design and the causal question.\nLacking a randomized experiment, Card and Krueger [1994] decided on a next-best solution by comparing two neighboring states before and after a minimum-wage increase. It was essentially the same strategy that Snow used in his cholera study and a strategy that economists continue to use, in one form or another, to this day [Dube et al., 2010].\nNew Jersey was set to experience an increase in the state min- imum wage from $4.25 to $5.05 in November 1992, but neighbor- ing Pennsylvania\u2019s minimum wage was staying at $4.25. Realizing they had an opportunity to evaluate the effect of the minimum-wage increase by comparing the two states before and after, they \ufb01elded a survey of about four hundred fast-food restaurants in both states\u2014 once in February 1992 (before) and again in November (after). The responses from this survey were then used to measure the outcomes they cared about (i.e., employment). As we saw with Snow, we see again here that shoe leather is as important as any statistical technique in causal inference.\nLet\u2019s look at whether the minimum-wage hike in New Jersey in fact raised the minimum wage by examining the distribution of wages in the fast food stores they surveyed. Figure 54 shows the distribution of wages in November 1992 after the minimum-wage hike. As can be seen, the minimum-wage hike was binding, evidenced by the mass of wages at the minimum wage in New Jersey.\nAs a caveat, notice how effective this is at convincing the reader that the minimum wage in New Jersey was binding. This piece of data visualization is not a trivial, or even optional, strategy to be taken in studies such as this. Even John Snow presented carefully designed maps of the distribution of cholera deaths throughout London. Beauti- ful pictures displaying the \u201c\ufb01rst stage\u201d effect of the intervention on the\nFigure 54. Distribution of wages for NJ and PA in November 1992. Reprinted from\nCard, D. and Krueger, A. (1994). \u201cMinimum Wages and Employment: A Case Study of the Fast-Food Industry in New Jersey and Pennsylvania.\u201d American Economic Review, 84:772\u2013793. Reprinted with permission from authors.\ntreatment are crucial in the rhetoric of causal inference, and few have done it as well as Card and Krueger.", "start_char_idx": 21789, "end_char_idx": 26365, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3f23f710-7643-47b6-84fa-aed98b366026": {"__data__": {"id_": "3f23f710-7643-47b6-84fa-aed98b366026", "embedding": null, "metadata": {"page number": "443 - 461", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "30d6cc4b-0fe1-4dd7-afdd-66923a155732", "node_type": "4", "metadata": {"page number": "443 - 461", "chapter": "Difference in Differences"}, "hash": "23176a9c9027b7e413409ef6e069bd177c677b7fbb8e90bb7dd3671848d45880", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "00944c39-88e6-4f7d-87d4-a7ab32606956", "node_type": "1", "metadata": {"page number": "443 - 461", "chapter": "Difference in Differences"}, "hash": "850c4a94851b510440b82b515aceccc69bfbffc64defc4adab55a0c4f335a0c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "24ee121b-0432-49c9-a326-f06b15dacbf5", "node_type": "1", "metadata": {}, "hash": "d7228a7355f367a84642ea80934f9463e8b78f4ba994cb0945024dae720aa2bd", "class_name": "RelatedNodeInfo"}}, "text": "As a caveat, notice how effective this is at convincing the reader that the minimum wage in New Jersey was binding. This piece of data visualization is not a trivial, or even optional, strategy to be taken in studies such as this. Even John Snow presented carefully designed maps of the distribution of cholera deaths throughout London. Beauti- ful pictures displaying the \u201c\ufb01rst stage\u201d effect of the intervention on the\nFigure 54. Distribution of wages for NJ and PA in November 1992. Reprinted from\nCard, D. and Krueger, A. (1994). \u201cMinimum Wages and Employment: A Case Study of the Fast-Food Industry in New Jersey and Pennsylvania.\u201d American Economic Review, 84:772\u2013793. Reprinted with permission from authors.\ntreatment are crucial in the rhetoric of causal inference, and few have done it as well as Card and Krueger.\nLet\u2019s remind ourselves what we\u2019re after\u2014the average causal effect of the minimum-wage hike on employment, or the ATT. Using our decomposition of the 2 \u00d7 2 DD from earlier, we can write it out as:\nAgain, we see the key assumption: the parallel-trends assumption, which is represented by the \ufb01rst difference in the second line. Inso- far as parallel trends holds in this situation, then the second term goes to zero, and the 2 \u00d7 2 DD collapses to the ATT.\nThe 2 \u00d7 2 DD requires differencing employment in NJ and PA, then differencing those \ufb01rst differences. This set of steps estimates the true ATT so long as the parallel-trends bias is zero. When that is true, (cid:12)\u03b42\u00d72\nTable 73. Simple DD using sample averages on full-time employment.\nis equal to \u03b4ATT. If this bottom line is not zero, though, then simple 2 \u00d7 2 suffers from unknown bias\u2014could bias it upwards, could bias it down- wards, could \ufb02ip the sign entirely. Table 73 shows the results of this exercise from Card and Krueger [1994].\nHere you see the result that surprised many people. Card and Krueger [1994] estimate an ATT of +2.76 additional mean full-time- equivalent employment, as opposed to some negative value which would be consistent with competitive input markets. Herein we get Buchanan\u2019s frustration with the paper, which is based mainly on a par- ticular model he had in mind, rather than a criticism of the research design the authors used.\nWhile differences in sample averages will identify the ATT under the parallel assumption, we may want to use multivariate regression instead. For instance, if you need to avoid omitted variable bias through controlling for endogenous covariates that vary over time, then you may want to use regression. Such strategies are another way of say- ing that you will need to close some known critical backdoor. Another reason for the equation is that by controlling for more appropriate covariates, you can reduce residual variance and improve the precision of your DD estimate.\nUsing the switching equation, and assuming a constant state \ufb01xed effect and time \ufb01xed effect, we can write out a simple regression model estimating the causal effect of the minimum wage on employment, Y.\nThis simple 2 \u00d7 2 is estimated with the following equation:\nYits = \u03b1 + \u03b3 NJs + \u03bbDt + \u03b4(NJ \u00d7 D)st + \u03b5its\nNJ is a dummy equal to 1 if the observation is from NJ, and D is a dummy equal to 1 if the observation is from November (the post period). This equation takes the following values, which I will list in order according to setting the dummies equal to one and/or zero:\n1. PA Pre: \u03b1 2. PA Post: \u03b1 + \u03bb 3. NJ Pre: \u03b1 + \u03b3 4. NJ Post: \u03b1 + \u03b3 + \u03bb + \u03b4\nWe can visualize the 2 \u00d7 2 DD parameter in Figure 55.\nNow before we hammer the parallel trends assumption for the bil- lionth time, I wanted to point something out here which is a bit subtle. But do you see the \u03b4 parameter \ufb02oating in the air above the November line in the Figure 55? This is the difference between a counterfactual level of employment (the bottom black circle in November on the neg- atively sloped dashed line) and the actual level of employment (the above black circle in November on the positively sloped solid line) for New Jersey. It is therefore the ATT, because the ATT is equal to\nwherein the \ufb01rst is observed (because Y = Y1 in the post period) and the latter is unobserved for the same reason.", "start_char_idx": 25543, "end_char_idx": 29728, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "24ee121b-0432-49c9-a326-f06b15dacbf5": {"__data__": {"id_": "24ee121b-0432-49c9-a326-f06b15dacbf5", "embedding": null, "metadata": {"page number": "443 - 461", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "30d6cc4b-0fe1-4dd7-afdd-66923a155732", "node_type": "4", "metadata": {"page number": "443 - 461", "chapter": "Difference in Differences"}, "hash": "23176a9c9027b7e413409ef6e069bd177c677b7fbb8e90bb7dd3671848d45880", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3f23f710-7643-47b6-84fa-aed98b366026", "node_type": "1", "metadata": {"page number": "443 - 461", "chapter": "Difference in Differences"}, "hash": "847b5a0a75259fcb1af574f1c82b58dda29290007bc647c2e6ee14ae3917af47", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "61674ede-9040-4047-b628-71b25973adba", "node_type": "1", "metadata": {}, "hash": "343bbf30e0887ec904ccd143a4853d8f2f8d21ee1a66e314aa6ae6e8dea2fea0", "class_name": "RelatedNodeInfo"}}, "text": "NJ Pre: \u03b1 + \u03b3 4. NJ Post: \u03b1 + \u03b3 + \u03bb + \u03b4\nWe can visualize the 2 \u00d7 2 DD parameter in Figure 55.\nNow before we hammer the parallel trends assumption for the bil- lionth time, I wanted to point something out here which is a bit subtle. But do you see the \u03b4 parameter \ufb02oating in the air above the November line in the Figure 55? This is the difference between a counterfactual level of employment (the bottom black circle in November on the neg- atively sloped dashed line) and the actual level of employment (the above black circle in November on the positively sloped solid line) for New Jersey. It is therefore the ATT, because the ATT is equal to\nwherein the \ufb01rst is observed (because Y = Y1 in the post period) and the latter is unobserved for the same reason.\nNow here\u2019s the kicker: OLS will always estimate that \u03b4 line even if the counterfactual slope had been something else. That\u2019s because OLS uses Pennsylvania\u2019s change over time to project a point starting at New Jersey\u2019s pre-treatment value. When OLS has \ufb01lled in that missing amount, the parameter estimate is equal to the difference between the observed post-treatment value and that projected value based on the slope of Pennsylvania regardless of whether that Pennsylvania slope was the correct benchmark for measuring New Jersey\u2019s counterfac- tual slope. OLS always estimates an effect size using the slope of the untreated group as the counterfactual, regardless of whether that slope is in fact the correct one.\nBut, see what happens when Pennsylvania\u2019s slope is equal to New Jersey\u2019s counterfactual slope? Then that Pennsylvania slope used in regression will mechanically estimate the ATT. In other words, only when the Pennsylvania slope is the counterfactual slope for New Jer- sey will OLS coincidentally identify that true effect. Let\u2019s see that here in Figure 56.\nNotice the two \u03b4 listed: on the left is the true parameter \u03b4ATT. On the right is the one estimated by OLS, (cid:12)\u03b4OLS. The falling solid line is the observed Pennsylvania change, whereas the falling solid line labeled \u201cobserved NJ\u201d is the change in observed employment for New Jersey between the two periods.\nThe true causal effect, \u03b4ATT, is the line from the \u201cobserved NJ\u201d point and the \u201ccounterfactual NJ\u201d point. But OLS does not estimate this line. Instead, OLS uses the falling Pennsylvania line to draw a par- allel line from the February NJ point, which is shown in thin gray. And\nOLS simply estimates the vertical line from the observed NJ point to the post NJ point, which as can be seen underestimates the true causal effect.\nHere we see the importance of the parallel trends assumption. The only situation under which the OLS estimate equals the ATT is when the counterfactual NJ just coincidentally lined up with the gray OLS line, which is a line parallel to the slope of the Pennsylvania line. Herein lies the source of understandable skepticism of many who have been paying attention: why should we base estimation on this belief in a coincidence? After all, this is a counterfactual trend, and therefore it is unobserved, given it never occurred. Maybe the counterfactual would\u2019ve been the gray line, but maybe it would\u2019ve been some other unknown line. It could\u2019ve been anything\u2014we just don\u2019t know.\nThis is why I like to tell people that the parallel trends assumption is actually just a restatement of the strict exogeneity assumption we discussed in the panel chapter. What we are saying when we appeal to parallel trends is that we have found a control group who approximates the traveling path of the treatment group and that the treatment is not\nendogenous. If it is endogenous, then parallel trends is always vio- lated because in counterfactual the treatment group would\u2019ve diverged anyway, regardless of the treatment.\nBefore we see the number of tests that economists have devised to provide some reasonable con\ufb01dence in the belief of the parallel trends, I\u2019d like to quickly talk about standard errors in a DD design.", "start_char_idx": 28968, "end_char_idx": 32947, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "61674ede-9040-4047-b628-71b25973adba": {"__data__": {"id_": "61674ede-9040-4047-b628-71b25973adba", "embedding": null, "metadata": {"page number": "461 - 463", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "60aada48-38f9-4299-b5a9-9fcac7886e83", "node_type": "4", "metadata": {"page number": "461 - 463", "chapter": "Difference in Differences"}, "hash": "f7855c877f6df4fac384481cd0ae5423ed06838b97b47180a26d21a242be71a9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "24ee121b-0432-49c9-a326-f06b15dacbf5", "node_type": "1", "metadata": {"page number": "443 - 461", "chapter": "Difference in Differences"}, "hash": "66af4f5c3d889644d136756c5f50596b4993d4cadac7e19971f28e3faa37f98b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "02eafcdc-f5a5-46ef-bdd9-d9b6cb906847", "node_type": "1", "metadata": {}, "hash": "d7f1a2cb839fec34dd0652094e3f4acf3fb7bc4f375d90030b7d8cd682ffbb17", "class_name": "RelatedNodeInfo"}}, "text": "Inference:\nMany studies employing DD strategies use data from many years\u2014 not just one pre-treatment and one post-treatment period like Card and Krueger [1994]. The variables of interest in many of these setups only vary at a group level, such as the state, and outcome variables are often serially correlated. In Card and Krueger [1994], it is very likely for instance that employment in each state is not only correlated within the state but also serially correlated. Bertrand et al. [2004] point out that the conventional standard errors often severely understate the stan- dard deviation of the estimators, and so standard errors are biased downward, \u201ctoo small,\u201d and therefore overreject the null hypothesis. Bertrand et al. [2004] propose the following solutions:\n1. Block bootstrapping standard errors. 2. Aggregating the data into one pre and one post period. 3. Clustering standard errors at the group level.\nBlock bootstrapping. If the block is a state, then you simply sample states with replacement for bootstrapping. Block bootstrap is straight- forward and only requires a little programming involving loops and storing the estimates. As the mechanics are similar to that of random- ization inference, I leave it to the reader to think about how they might tackle this.\nAggregation. This approach ignores the time-series dimensions alto- gether, and if there is only one pre and post period and one untreated it\u2019s as simple as it sounds. You simply average the groups group, into one pre and post period, and conduct difference-in-differences\nendogenous. If it is endogenous, then parallel trends is always vio- lated because in counterfactual the treatment group would\u2019ve diverged anyway, regardless of the treatment.\nBefore we see the number of tests that economists have devised to provide some reasonable con\ufb01dence in the belief of the parallel trends, I\u2019d like to quickly talk about standard errors in a DD design.\nMany studies employing DD strategies use data from many years\u2014 not just one pre-treatment and one post-treatment period like Card and Krueger [1994]. The variables of interest in many of these setups only vary at a group level, such as the state, and outcome variables are often serially correlated. In Card and Krueger [1994], it is very likely for instance that employment in each state is not only correlated within the state but also serially correlated. Bertrand et al. [2004] point out that the conventional standard errors often severely understate the stan- dard deviation of the estimators, and so standard errors are biased downward, \u201ctoo small,\u201d and therefore overreject the null hypothesis. Bertrand et al. [2004] propose the following solutions:\n1. Block bootstrapping standard errors. 2. Aggregating the data into one pre and one post period. 3. Clustering standard errors at the group level.\nBlock bootstrapping. If the block is a state, then you simply sample states with replacement for bootstrapping. Block bootstrap is straight- forward and only requires a little programming involving loops and storing the estimates. As the mechanics are similar to that of random- ization inference, I leave it to the reader to think about how they might tackle this.\nAggregation. This approach ignores the time-series dimensions alto- gether, and if there is only one pre and post period and one untreated it\u2019s as simple as it sounds. You simply average the groups group, into one pre and post period, and conduct difference-in-differences\non those aggregated. But if you have differential timing, it\u2019s a bit unusual because you will need to partial out state and year \ufb01xed effects before turning the analysis into an analysis involving residualization. Essentially, for those common situations where you have multiple treatment time periods (which we discuss later in greater detail), you would regress the outcome onto panel unit and time \ufb01xed effects and any covariates. You\u2019d then obtain the residuals for only the treatment group. You then divide the residuals only into a pre and post period; you are essentially at this point ignoring the never-treated groups. And then you regress the residuals on the after dummy. It\u2019s a strange procedure, and does not recover the original point estimate, so I focus instead on the third.\nClustering. Correct treatment of standard errors sometimes makes the number of groups very small: in Card and Krueger [1994], the number of groups is only two. More common than not, researchers will use the third option (clustering the standard errors by group). I have only one time seen someone do all three of these; it\u2019s rare though.", "start_char_idx": 0, "end_char_idx": 4609, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "02eafcdc-f5a5-46ef-bdd9-d9b6cb906847": {"__data__": {"id_": "02eafcdc-f5a5-46ef-bdd9-d9b6cb906847", "embedding": null, "metadata": {"page number": "461 - 463", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "60aada48-38f9-4299-b5a9-9fcac7886e83", "node_type": "4", "metadata": {"page number": "461 - 463", "chapter": "Difference in Differences"}, "hash": "f7855c877f6df4fac384481cd0ae5423ed06838b97b47180a26d21a242be71a9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "61674ede-9040-4047-b628-71b25973adba", "node_type": "1", "metadata": {"page number": "461 - 463", "chapter": "Difference in Differences"}, "hash": "14be5612a5910cf41ec40818dcb3fc513d0f67a89b0d78836461dd93bdab2228", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f1898914-b4cb-48b1-99f9-f39ebd7e5924", "node_type": "1", "metadata": {}, "hash": "41ea070a396b49aa9c943f3728169b4916328f83a055e698b1a1b2bfae1ed24a", "class_name": "RelatedNodeInfo"}}, "text": "Essentially, for those common situations where you have multiple treatment time periods (which we discuss later in greater detail), you would regress the outcome onto panel unit and time \ufb01xed effects and any covariates. You\u2019d then obtain the residuals for only the treatment group. You then divide the residuals only into a pre and post period; you are essentially at this point ignoring the never-treated groups. And then you regress the residuals on the after dummy. It\u2019s a strange procedure, and does not recover the original point estimate, so I focus instead on the third.\nClustering. Correct treatment of standard errors sometimes makes the number of groups very small: in Card and Krueger [1994], the number of groups is only two. More common than not, researchers will use the third option (clustering the standard errors by group). I have only one time seen someone do all three of these; it\u2019s rare though. Most people will present just the clustering solution\u2014most likely because it requires minimal programming.\nFor clustering, there is no programming required, as most soft- ware packages allow for it already. You simply adjust standard errors by clustering at the group level, as we discussed in the earlier chapter, or the level of treatment. For state-level panels, that would mean clus- tering at the state level, which allows for arbitrary serial correlation in errors within a state over time. This is the most common solution employed.\nInference in a panel setting is independently an interesting area. When the number of clusters is small, then simple solutions like clus- tering the standard errors no longer su\ufb03ce because of a growing false positive problem. In the extreme case with only one treatment unit, the over-rejection rate at a signi\ufb01cance of 5% can be as high as 80% in simulations even using the wild bootstrap technique which has been suggested for smaller numbers of clusters [Cameron et al., 2008; MacKinnon and Webb, 2017]. In such extreme cases where there is only one treatment group, I have preferred to use randomization inference following Buchmueller et al. [2011].", "start_char_idx": 3694, "end_char_idx": 5804, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f1898914-b4cb-48b1-99f9-f39ebd7e5924": {"__data__": {"id_": "f1898914-b4cb-48b1-99f9-f39ebd7e5924", "embedding": null, "metadata": {"page number": "464 - 472", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0c99ade6-1a9e-484d-8673-341f7e9766eb", "node_type": "4", "metadata": {"page number": "464 - 472", "chapter": "Difference in Differences"}, "hash": "dafc7bf539eb4d2efb447b08a40fdbd7ad2de91c2743ebba8cc201bd028195dc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "02eafcdc-f5a5-46ef-bdd9-d9b6cb906847", "node_type": "1", "metadata": {"page number": "461 - 463", "chapter": "Difference in Differences"}, "hash": "c2aa04f0c443c1504c2e7d19350d707ccec20a78341b947bc424160656f258ca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2586316-5fd1-4e58-864d-628420b5ae2f", "node_type": "1", "metadata": {}, "hash": "3430d51218c3bb2df1849688afd2b19f3c88629f659fff21972b95ee4f02864b", "class_name": "RelatedNodeInfo"}}, "text": "Providing Evidence for Parallel Trends Through Event Studies and Parallel Leads:\nA redundant rant about parallel pre-treatment DD coe\ufb03cients (because I\u2019m worried one was not enough). Given the critical importance of the parallel trends assumption in identifying causal effects with the DD design, and given that one of the observations needed to evaluate the parallel-trends assumption is not available to the researcher, one might throw up their hands in despair. But economists are stubborn, and they have spent decades devising ways to test whether it\u2019s reasonable to believe in parallel trends. We now discuss the obligatory test for any DD design\u2014the event study. Let\u2019s rewrite the decomposition of the 2 \u00d7 2 DD again.\nWe are interested in the \ufb01rst term, ATT, but it is contaminated by selection bias when the second term does not equal zero. Since eval- uating the second term requires the counterfactual, E[Y0 | Post], we k are unable to do so directly. What economists typically do, instead, is compare placebo pre-treatment leads of the DD coe\ufb03cient. If DD coe\ufb03cients in the pre-treatment periods are statistically zero, then the difference-in-differences between treatment and control groups fol- lowed a similar trend prior to treatment. And here\u2019s the rhetorical art of the design: if they had been similar before, then why wouldn\u2019t they continue to be post-treatment?\nBut notice that this rhetoric is a kind of proof by assertion. Just because they were similar before does not logically require they be the same after. Assuming that the future is like the past is a form of the gambler\u2019s fallacy called the \u201creverse position.\u201d Just because a coin came up heads three times in a row does not mean it will come up heads the fourth time\u2014not without further assumptions. Like- wise, we are not obligated to believe that that counterfactual trends\nwould be the same post-treatment because they had been similar pre- treatment without further assumptions about the predictive power of pre-treatment trends. But to make such assumptions is again to make untestable assumptions, and so we are back where we started.\nOne situation where parallel trends would be obviously violated is if the treatment itself was endogenous. In such a scenario, the assign- ment of the treatment status would be directly dependent on poten- tial outcomes, and absent the treatment, potential outcomes would\u2019ve changed regardless. Such traditional endogeneity requires more than merely lazy visualizations of parallel leads. While the test is impor- tant, technically pre-treatment similarities are neither necessary nor su\ufb03cient to guarantee parallel counterfactual trends [Kahn-Lang and Lang, 2019]. The assumption is not so easily proven. You can never stop being diligent in attempting to determine whether groups of units endogenously selected into treatment, the presence of omitted vari- able biases, various sources of selection bias, and open backdoor paths. When the structural error term in a dynamic regression model is uncorrelated with the treatment variable, you have strict exogeneity, and that is what gives you parallel trends, and that is what makes you able to make meaningful statements about your estimates.\nChecking the pre-treatment balance between treatment and control groups. Now with that pessimism out of the way, let\u2019s discuss event study plots because though they are not direct tests of the parallel trends assumption, they have their place because they show that the two groups of units were comparable on dynamics in the pre-treatment period.6 Such conditional independence concepts have been used pro\ufb01tably throughout this book, and we do so again now.\nAuthors have tried showing the differences between treatment and control groups a few different ways. One way is to simply show the raw data, which you can do if you have a set of groups who received the treatment at the same point in time. Then you would just visually\n6 Financial economics also has a procedure called the event study [Binder, 1998],\nbut the way that event study is often used in contemporary causal inference is nothing\ndummy, you saturate a model with leads and lags based on the timing of treatment.\ninspect whether the pre-treatment dynamics of the treatment group differed from that of the control group units.\nBut what if you do not have a single treatment date? What if instead you have differential timing wherein groups of units adopt the treat- ment at different points? Then the concept of pre-treatment becomes complex. If New Jersey raised its minimum wage in 1992 and New York raised its minimum wage in 1994, but Pennsylvania never raised its minimum wage, the pre-treatment period is de\ufb01ned for New Jersey (1991) and New York (1993), but not Pennsylvania.", "start_char_idx": 0, "end_char_idx": 4763, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d2586316-5fd1-4e58-864d-628420b5ae2f": {"__data__": {"id_": "d2586316-5fd1-4e58-864d-628420b5ae2f", "embedding": null, "metadata": {"page number": "464 - 472", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0c99ade6-1a9e-484d-8673-341f7e9766eb", "node_type": "4", "metadata": {"page number": "464 - 472", "chapter": "Difference in Differences"}, "hash": "dafc7bf539eb4d2efb447b08a40fdbd7ad2de91c2743ebba8cc201bd028195dc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f1898914-b4cb-48b1-99f9-f39ebd7e5924", "node_type": "1", "metadata": {"page number": "464 - 472", "chapter": "Difference in Differences"}, "hash": "ea4bb057f1b5910eb71d40750b900935859d8bce0416add95c054b75b418123d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e846af91-e4bc-468b-8bf7-5670f6d27e99", "node_type": "1", "metadata": {}, "hash": "6aac213da6582b1589844e4d621a3f08f2e78cfc8f12b482ba173f909fbbe1cf", "class_name": "RelatedNodeInfo"}}, "text": "Then you would just visually\n6 Financial economics also has a procedure called the event study [Binder, 1998],\nbut the way that event study is often used in contemporary causal inference is nothing\ndummy, you saturate a model with leads and lags based on the timing of treatment.\ninspect whether the pre-treatment dynamics of the treatment group differed from that of the control group units.\nBut what if you do not have a single treatment date? What if instead you have differential timing wherein groups of units adopt the treat- ment at different points? Then the concept of pre-treatment becomes complex. If New Jersey raised its minimum wage in 1992 and New York raised its minimum wage in 1994, but Pennsylvania never raised its minimum wage, the pre-treatment period is de\ufb01ned for New Jersey (1991) and New York (1993), but not Pennsylvania. Thus, how do we go about testing for pre-treatment differences in that case? People have done it in a variety of ways.\nOne possibility is to plot the raw data, year by year, and simply eye- ball. You would compare the treatment group with the never-treated, for instance, which might require a lot of graphs and may also be awk- ward looking. Cheng and Hoekstra [2013] took this route, and created a separate graph comparing treatment groups with an untreated group for each different year of treatment. The advantage is its transparent display of the raw unadjusted data. No funny business. The disadvan- tage of this several-fold. First, it may be cumbersome when the number of treatment groups is large, making it practically impossible. Second, it may not be beautiful. But third, this necessarily assumes that the only control group is the never-treated group, which in fact is not true given what Goodman-Bacon [2019] has shown. Any DD is a combination of a comparison between the treatment and the never treated, an early treated compared to a late treated, and a late treated compared to an early treated. Thus only showing the comparison with the never treated is actually a misleading presentation of the underlying mechanization of identi\ufb01cation using an twoway \ufb01xed-effects model with differential timing.\nAnderson et al. [2013] took an alternative, creative approach to show the comparability of states with legalized medical marijuana and states without. As I said, the concept of a pre-treatment period for a control state is unde\ufb01ned when pre-treatment is always in reference to a speci\ufb01c treatment date which varies across groups. So, the authors construct a recentered time path of tra\ufb03c fatality rates for the con- trol states by assigning random treatment dates to all control counties and then plotting the average tra\ufb03c fatality rates for each group in\nyears leading up to treatment and beyond. This approach has a few advantages. First, it plots the raw data, rather than coe\ufb03cients from a regression (as we will see next). Second, it plots that data against controls. But its weakness is that technically, the control series is not in fact true. It is chosen so as to give a comparison, but when regres- sions are eventually run, it will not be based on this series. But the main main shortcoming is that technically it is not displaying any of the con- trol groups that will be used for estimation Goodman-Bacon [2019]. It is not displaying a comparison between the treated and the never treated; it is not a comparison between the early and late treated; it is not a comparison between the late and early treated. While a creative attempt to evaluate the pre-treatment differences in leads, it does not in fact technically show that.\nThe current way in which authors evaluate the pre-treatment dynamics between a treatment and control group with differential tim- ing is to estimate a regression model that includes treatment leads and lags. I \ufb01nd that it is always useful to teach these concepts in the con- text of an actual paper, so let\u2019s review an interesting working paper by Miller et al. [2019].\nAffordable Care Act, expanding Medicaid and population mortality. A provocative new study by Miller et al. [2019] examined the expansion of Medicaid under the Affordable Care Act. They were primarily interested in the effect that this expansion had on population mortality. Earlier work had cast doubt on Medicaid\u2019s effect on mortality [Baicker et al., 2013; Finkelstein et al., 2012], so revisiting the question with a larger sample size had value.\nLike Snow before them, the authors link data sets on deaths with a large-scale federal survey data, thus showing that shoe leather often goes hand in hand with good design.", "start_char_idx": 3915, "end_char_idx": 8516, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e846af91-e4bc-468b-8bf7-5670f6d27e99": {"__data__": {"id_": "e846af91-e4bc-468b-8bf7-5670f6d27e99", "embedding": null, "metadata": {"page number": "464 - 472", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0c99ade6-1a9e-484d-8673-341f7e9766eb", "node_type": "4", "metadata": {"page number": "464 - 472", "chapter": "Difference in Differences"}, "hash": "dafc7bf539eb4d2efb447b08a40fdbd7ad2de91c2743ebba8cc201bd028195dc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2586316-5fd1-4e58-864d-628420b5ae2f", "node_type": "1", "metadata": {"page number": "464 - 472", "chapter": "Difference in Differences"}, "hash": "aeb6d5bfc6561bffa56c00d60a98d2181904062d943efc6329363ace8cb6b5dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0e70b4b1-9f47-410b-86d2-5592ed7a4e35", "node_type": "1", "metadata": {}, "hash": "81cbda9c75ccd2c1bc29d6148de9145ac27c6b680dc0d6a18b1d562e81970349", "class_name": "RelatedNodeInfo"}}, "text": "I \ufb01nd that it is always useful to teach these concepts in the con- text of an actual paper, so let\u2019s review an interesting working paper by Miller et al. [2019].\nAffordable Care Act, expanding Medicaid and population mortality. A provocative new study by Miller et al. [2019] examined the expansion of Medicaid under the Affordable Care Act. They were primarily interested in the effect that this expansion had on population mortality. Earlier work had cast doubt on Medicaid\u2019s effect on mortality [Baicker et al., 2013; Finkelstein et al., 2012], so revisiting the question with a larger sample size had value.\nLike Snow before them, the authors link data sets on deaths with a large-scale federal survey data, thus showing that shoe leather often goes hand in hand with good design. They use these data to evalu- ate the causal impact of Medicaid enrollment on mortality using a DD design. Their focus is on the near-elderly adults in states with and with- out the Affordable Care Act Medicaid expansions and they \ufb01nd a 0.13- percentage-point decline in annual mortality, which is a 9.3% reduction over the sample mean, as a result of the ACA expansion. This effect is a result of a reduction in disease-related deaths and gets larger over time. Medicaid, in this estimation, saved a non-trivial number of lives.\nAs with many contemporary DD designs, Miller et al. [2019] eval- uate the pre-treatment leads instead of plotting the raw data by treat- ment and control. Post-estimation, they plotted regression coe\ufb03cients with 95% con\ufb01dence intervals on their treatment leads and lags. Includ- ing leads and lags into the DD model allowed the reader to check both the degree to which the post-treatment treatment effects were dynamic, and whether the two groups were comparable on outcome dynamics pre-treatment. Models like this one usually follow a form like:\nYits = \u03b3s + \u03bbt +\nTreatment occurs in year 0. You include q leads or anticipatory effects and m lags or post-treatment effects.\nMiller et al. [2019] produce four event studies that when taken together tell the main parts of the story of their paper. This is, quite frankly, the art of the rhetoric of causal inference\u2014visualization of key estimates, such as \u201c\ufb01rst stages\u201d as well as outcomes and placebos. The event study plots are so powerfully persuasive, they will make you a bit jealous, since oftentimes yours won\u2019t be nearly so nice. Let\u2019s look at the \ufb01rst three. State expansion of Medicaid under the Afford- able Care Act increased Medicaid eligibility (Figure 57), which is not altogether surprising. It also caused an increase in Medicaid coverage (Figure 58), and as a consequence reduced the percentage of the unin- sured population (Figure 59). All three of these are simply showing that the ACA Medicaid expansion had \u201cbite\u201d\u2014people enrolled and became insured.\nThere are several features of these event studies that should catch your eye. First, look at Figure 57. The pre-treatment coe\ufb03cients are nearly on the zero line itself. Not only are they nearly zero in their point estimate, but their standard errors are very small. This means these are very precisely estimated zero differences between individuals in the two groups of states prior to the expansion.\nThe second thing you see, though, is the elephant in the room. Post-treatment, the probability that someone becomes eligible for Medicaid immediately shoots up to 0.4 and while not as precise as the pre-treatment coe\ufb03cients, the authors can rule out effects as low as 0.3 to 0.35. These are large increases in eligibility, and the fact that\nFigure 57. Estimates of Medicaid expansion\u2019s effects on eligibility using leads and lags\nin an event-study model. Miller, S., Altekruse, S., Johnson, N., and Wherry,\nL. R. (2019). Medicaid and mortality: New evidence from linked survey and\nadministrative data. Working Paper No. 6081, National Bureau of Economic\nResearch, Cambridge, MA. Reprinted with permission from authors.\nthe coe\ufb03cients prior to the treatment are basically zero, we \ufb01nd it easy to believe that the risen coe\ufb03cients post-treatment were caused by the ACA\u2019s expansion of Medicaid in states.", "start_char_idx": 7732, "end_char_idx": 11870, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0e70b4b1-9f47-410b-86d2-5592ed7a4e35": {"__data__": {"id_": "0e70b4b1-9f47-410b-86d2-5592ed7a4e35", "embedding": null, "metadata": {"page number": "464 - 472", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0c99ade6-1a9e-484d-8673-341f7e9766eb", "node_type": "4", "metadata": {"page number": "464 - 472", "chapter": "Difference in Differences"}, "hash": "dafc7bf539eb4d2efb447b08a40fdbd7ad2de91c2743ebba8cc201bd028195dc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e846af91-e4bc-468b-8bf7-5670f6d27e99", "node_type": "1", "metadata": {"page number": "464 - 472", "chapter": "Difference in Differences"}, "hash": "80e26f11254cbf1cabcd32fab6d816a660ff49daa19cca09a7e9442955fdc722", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b07f6633-44d0-4636-b615-65c3b71474e7", "node_type": "1", "metadata": {}, "hash": "a1ce3200c533a5a3c39578fc42306a6ffec76cb8ded5020eb26f5a354fa74992", "class_name": "RelatedNodeInfo"}}, "text": "These are large increases in eligibility, and the fact that\nFigure 57. Estimates of Medicaid expansion\u2019s effects on eligibility using leads and lags\nin an event-study model. Miller, S., Altekruse, S., Johnson, N., and Wherry,\nL. R. (2019). Medicaid and mortality: New evidence from linked survey and\nadministrative data. Working Paper No. 6081, National Bureau of Economic\nResearch, Cambridge, MA. Reprinted with permission from authors.\nthe coe\ufb03cients prior to the treatment are basically zero, we \ufb01nd it easy to believe that the risen coe\ufb03cients post-treatment were caused by the ACA\u2019s expansion of Medicaid in states.\nOf course, I would not be me if I did not say that technically the zeroes pre-treatment do not therefore mean that the post-treatment difference between counterfactual trends and observed trends are zero, but doesn\u2019t it seem compelling when you see it? Doesn\u2019t it com- pel you, just a little bit, that the changes in enrollment and insurance status were probably caused by the Medicaid expansion? I daresay a table of coe\ufb03cients with leads, lags, and standard errors would proba- bly not be as compelling even though it is the identical information. Also, it is only fair that the skeptic refuse these patterns with new evidence of what it is other than the Medicaid expansion. It is not enough to merely hand wave a criticism of omitted variable bias; the critic must be as engaged in this phenomenon as the authors them- selves, which is how empiricists earn the right to critique someone else\u2019s work.\nSimilar graphs are shown for coverage\u2014prior to treatment, the two groups of individuals in treatment and control were similar with regards\nFigure 58. Estimates of Medicaid expansion\u2019s effects on coverage using leads and lags\nin an event-study model. Miller, S., Altekruse, S., Johnson, N., and Wherry,\nL. R. (2019). Medicaid and mortality: New evidence from linked survey and\nadministrative data. Working Paper No. 6081, National Bureau of Economic\nResearch, Cambridge, MA. Reprinted with permission from authors.\nto their coverage and uninsured rate. But post-treatment, they diverge dramatically. Taken together, we have the \u201c\ufb01rst stage,\u201d which means we can see that the Medicaid expansion under the ACA had \u201cbite.\u201d Had the authors failed to \ufb01nd changes in eligibility, coverage, or unin- sured rates, then any evidence from the secondary outcomes would have doubt built in. This is the reason it is so important that you exam- ine the \ufb01rst stage (treatment\u2019s effect on usage), as well as the second stage (treatment\u2019s effect on the outcomes of interest).\nBut now let\u2019s look at the main result\u2014what effect did this have on population mortality itself? Recall, Miller et al. [2019] linked adminis- trative death records with a large-scale federal survey. So they actually know who is on Medicaid and who is not. John Snow would be proud of this design, the meticulous collection of high-quality data, and all the shoeleather the authors showed.\nThis event study is presented in Figure 60. A graph like this is the contemporary heart and soul of a DD design, both because it conveys key information regarding the comparability of the treatment and con- trol groups in their dynamics just prior to treatment, and because such\nFigure 59. Estimates of Medicaid expansion\u2019s effects on the uninsured state using\nleads and lags in an event-study model. Miller, S., Altekruse, S., Johnson,\nN., and Wherry, L. R. (2019). Medicaid and mortality: New evidence from\nlinked survey and administrative data. Working Paper No. 6081, National\nBureau of Economic Research, Cambridge, MA. Reprinted with permission\nstrong data visualization of main effects are powerfully persuasive. It\u2019s quite clear looking at it that there was no difference between the trend- ing tendencies of the two sets of state prior to treatment, making the subsequent divergence all the more striking.\nBut a picture like this is only as important as the thing that it is studying, and it is worth summarizing what Miller et al. [2019] have revealed here. The expansion of ACA Medicaid led to large swaths of people becoming eligible for Medicaid. In turn, they enrolled in Medicaid, which caused the uninsured rate to drop considerably.", "start_char_idx": 11250, "end_char_idx": 15470, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b07f6633-44d0-4636-b615-65c3b71474e7": {"__data__": {"id_": "b07f6633-44d0-4636-b615-65c3b71474e7", "embedding": null, "metadata": {"page number": "464 - 472", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0c99ade6-1a9e-484d-8673-341f7e9766eb", "node_type": "4", "metadata": {"page number": "464 - 472", "chapter": "Difference in Differences"}, "hash": "dafc7bf539eb4d2efb447b08a40fdbd7ad2de91c2743ebba8cc201bd028195dc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0e70b4b1-9f47-410b-86d2-5592ed7a4e35", "node_type": "1", "metadata": {"page number": "464 - 472", "chapter": "Difference in Differences"}, "hash": "ab03f0f6aefd98463690ba20e3894fd14abf791da87e83e70edc30977b2d71d3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ee05de7f-f805-4355-afd2-cbe4812c6263", "node_type": "1", "metadata": {}, "hash": "b6a261dc9d7fd10c86249b95ed34fe678a578d248ebe170b802fb88b21cbf785", "class_name": "RelatedNodeInfo"}}, "text": "Miller, S., Altekruse, S., Johnson,\nN., and Wherry, L. R. (2019). Medicaid and mortality: New evidence from\nlinked survey and administrative data. Working Paper No. 6081, National\nBureau of Economic Research, Cambridge, MA. Reprinted with permission\nstrong data visualization of main effects are powerfully persuasive. It\u2019s quite clear looking at it that there was no difference between the trend- ing tendencies of the two sets of state prior to treatment, making the subsequent divergence all the more striking.\nBut a picture like this is only as important as the thing that it is studying, and it is worth summarizing what Miller et al. [2019] have revealed here. The expansion of ACA Medicaid led to large swaths of people becoming eligible for Medicaid. In turn, they enrolled in Medicaid, which caused the uninsured rate to drop considerably. The authors then \ufb01nd amazingly using linked administrative data on death records that the expansion of ACA Medicaid led to a 0.13 percentage point decline in annual mortality, which is a 9.3 percent reduction over the mean. They go on to try and understand the mechanism (another key feature of this high-quality study) by which such amazing effects may have occurred, and conclude that Medicaid caused near-elderly individuals to receive treatment for life-threatening illnesses. I suspect we will be hearing about this study for many years.\nFigure 60. Estimates of Medicaid expansion\u2019s effects on on annual mortality using\nleads and lags in an event study model. Miller, S., Altekruse, S., Johnson,\nN., and Wherry, L. R. (2019). Medicaid and mortality: New evidence from\nlinked survey and administrative data. Working Paper No. 6081, National\nBureau of Economic Research, Cambridge, MA. Reprinted with permission", "start_char_idx": 14622, "end_char_idx": 16385, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ee05de7f-f805-4355-afd2-cbe4812c6263": {"__data__": {"id_": "ee05de7f-f805-4355-afd2-cbe4812c6263", "embedding": null, "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "83d6e205-d5f0-4291-802f-85c79a899db8", "node_type": "4", "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "hash": "67c25ce71063b06809b4b6ddca5b8e8fc47c015726d3cb1dda1af9ff2c428e85", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b07f6633-44d0-4636-b615-65c3b71474e7", "node_type": "1", "metadata": {"page number": "464 - 472", "chapter": "Difference in Differences"}, "hash": "7ce7fa95a4a714c8502e4bcf0089db662def91cdf4c3b42835ca145af010ae90", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "66732e29-5597-4743-8462-ff2c51405ddf", "node_type": "1", "metadata": {}, "hash": "8de78513d779a0f36ad80ccef9b09d06c4a9bb4ee5d5ee03c537cb0df808284d", "class_name": "RelatedNodeInfo"}}, "text": "The Importance of Placebos in DD:\nThere are several tests of the validity of a DD strategy. I have already discussed one\u2014comparability between treatment and control groups on observable pre-treatment dynamics. Next, I will discuss other credible ways to evaluate whether estimated causal effects are credible by emphasizing the use of placebo falsi\ufb01cation.\nThe idea of placebo falsi\ufb01cation is simple. Say that you are \ufb01nding some negative effect of the minimum wage on low-wage employment. Is the hypothesis true if we \ufb01nd evidence in favor? Maybe, maybe not. Maybe what would really help, though, is if you had in mind an alter- native hypothesis and then tried to test that alternative hypothesis. If you cannot reject the null on the alternative hypothesis, then it provides some credibility to your original analysis. For instance, maybe you are picking up something spurious, like cyclical factors or other unobserv- ables not easily captured by a time or state \ufb01xed effects. So what can you do?\nOne candidate placebo falsi\ufb01cation might simply be to use data for an alternative type of worker whose wages would not be affected\nFigure 60. Estimates of Medicaid expansion\u2019s effects on on annual mortality using\nleads and lags in an event study model. Miller, S., Altekruse, S., Johnson,\nN., and Wherry, L. R. (2019). Medicaid and mortality: New evidence from\nlinked survey and administrative data. Working Paper No. 6081, National\nBureau of Economic Research, Cambridge, MA. Reprinted with permission\nThere are several tests of the validity of a DD strategy. I have already discussed one\u2014comparability between treatment and control groups on observable pre-treatment dynamics. Next, I will discuss other credible ways to evaluate whether estimated causal effects are credible by emphasizing the use of placebo falsi\ufb01cation.\nThe idea of placebo falsi\ufb01cation is simple. Say that you are \ufb01nding some negative effect of the minimum wage on low-wage employment. Is the hypothesis true if we \ufb01nd evidence in favor? Maybe, maybe not. Maybe what would really help, though, is if you had in mind an alter- native hypothesis and then tried to test that alternative hypothesis. If you cannot reject the null on the alternative hypothesis, then it provides some credibility to your original analysis. For instance, maybe you are picking up something spurious, like cyclical factors or other unobserv- ables not easily captured by a time or state \ufb01xed effects. So what can you do?\nOne candidate placebo falsi\ufb01cation might simply be to use data for an alternative type of worker whose wages would not be affected\nby the binding minimum wage. For instance, minimum wages affect employment and earnings of low-wage workers as these are the work- ers who literally are hired based on the market wage. Without some serious general equilibrium gymnastics, the minimum wage should not affect the employment of higher wage workers, because the minimum wage is not binding on high wage workers. Since high- and low-wage workers are employed in very different sectors, they are unlikely to be substitutes. This reasoning might lead us to consider the possibility that higher wage workers might function as a placebo.\nThere are two ways you can go about incorporating this idea into our analysis. Many people like to be straightforward and sim- ply \ufb01t the same DD design using high wage employment as the out- come. If the coe\ufb03cient on minimum wages is zero when using high wage worker employment as the outcome, but the coe\ufb03cient on min- imum wages for low wage workers is negative, then we have pro- vided stronger evidence that complements the earlier analysis we did when on the low wage workers. But there is another method that uses the within-state placebo for identi\ufb01cation called the difference- in-differences-in-differences (\u201ctriple differences\u201d). I will discuss that design now.\nTriple differences. In our earlier analysis, we assumed that the only thing that happened to New Jersey after it passed the minimum wage was a common shock, T, but what if there were state-speci\ufb01c time shocks such as NJt or PAt? Then even DD cannot recover the treat- ment effect. Let\u2019s see for ourselves using a modi\ufb01cation of the simple minimum-wage table from earlier, which will include the within-state workers who hypothetically were untreated by the minimum wage\u2014the \u201chigh-wage workers.\u201d\nBefore the minimum-wage increase, low- and high-wage employ- ment in New Jersey is determined by a group-speci\ufb01c New Jersey \ufb01xed effect (e.g., NJh). The same is true for Pennsylvania.", "start_char_idx": 0, "end_char_idx": 4548, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "66732e29-5597-4743-8462-ff2c51405ddf": {"__data__": {"id_": "66732e29-5597-4743-8462-ff2c51405ddf", "embedding": null, "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "83d6e205-d5f0-4291-802f-85c79a899db8", "node_type": "4", "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "hash": "67c25ce71063b06809b4b6ddca5b8e8fc47c015726d3cb1dda1af9ff2c428e85", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ee05de7f-f805-4355-afd2-cbe4812c6263", "node_type": "1", "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "hash": "46514dd53f0007f70426f04989c4a596a466d2126e5d4f35528e5fa679cf4c88", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d7ecb51-5ac1-44e2-bedc-2f7ee8f0438b", "node_type": "1", "metadata": {}, "hash": "67ca529419f1cb34fbeb816de28098831723b96731e1768193c8453b8cac12f2", "class_name": "RelatedNodeInfo"}}, "text": "But there is another method that uses the within-state placebo for identi\ufb01cation called the difference- in-differences-in-differences (\u201ctriple differences\u201d). I will discuss that design now.\nTriple differences. In our earlier analysis, we assumed that the only thing that happened to New Jersey after it passed the minimum wage was a common shock, T, but what if there were state-speci\ufb01c time shocks such as NJt or PAt? Then even DD cannot recover the treat- ment effect. Let\u2019s see for ourselves using a modi\ufb01cation of the simple minimum-wage table from earlier, which will include the within-state workers who hypothetically were untreated by the minimum wage\u2014the \u201chigh-wage workers.\u201d\nBefore the minimum-wage increase, low- and high-wage employ- ment in New Jersey is determined by a group-speci\ufb01c New Jersey \ufb01xed effect (e.g., NJh). The same is true for Pennsylvania. But after the minimum-wage hike, four things change in New Jersey: national trends cause employment to change by T; New Jersey-speci\ufb01c time shocks change employment by NJt; generic trends in low-wage work- ers change employment by lt; and the minimum-wage has some unknown effect D. We have the same setup in Pennsylvania except\nLow-wage After NJl + T + NJt + lt + D T + NJt+ workers Before NJl High-wage After NJh + T + NJt + ht workers Before NJh Low-wage After workers High-wage After workers\n(lt \u2212 ht) + D D\nthere is no minimum wage, and Pennsylvania experiences its own time shocks.\nNow if we take \ufb01rst differences for each set of states, we only eliminate the state \ufb01xed effect. The \ufb01rst difference estimate for New Jersey includes the minimum-wage effect, D, but is also hopelessly contaminated by confounders (i.e., T + NJt + lt). So we take a second difference for each state, and doing so, we eliminate two of the con- founders: T disappears and NJt disappears. But while this DD strategy has eliminated several confounders, it has also introduced new ones (i.e., (lt \u2212 ht)). This is the \ufb01nal source of selection bias that triple dif- ferences are designed to resolve. But, by differencing Pennsylvania\u2019s second difference from New Jersey, the (lt \u2212 ht) is deleted and the minimum-wage effect is isolated.\nNow, this solution is not without its own set of unique parallel- trends assumptions. But one of the parallel trends here I\u2019d like you to see is the lt \u2212 ht term. This parallel trends assumption states that the effect can be isolated if the gap between high- and low-wage employ- ment would\u2019ve evolved similarly in the treatment state counterfactual as it did in the historical control states. And we should probably pro- vide some credible evidence that this is true with leads and lags in an event study as before.\nState-mandated maternity bene\ufb01ts. The triple differences design was \ufb01rst introduced by Gruber [1994] in a study of state-level policies pro- viding maternity bene\ufb01ts. I present his main results in Table 75. Notice\nthat he uses as his treatment group married women of childbearing age in treatment and control states, but he also uses a set of placebo units (older women and single men 20\u201340) as within-state controls. He then goes through the differences in means to get the difference- in-differences for each set of groups, after which he calculates the DDD as the difference between these two difference-in-differences.\nIdeally when you do a DDD estimate, the causal effect estimate will come from changes in the treatment units, not changes in the control units. That\u2019s precisely what we see in Gruber [1994]: the action comes from changes in the married women age 20\u201340 (\u22120.062); there\u2019s little movement among the placebo units (\u22120.008). Thus when we calcu- late the DDD, we know that most of that calculation is coming from the \ufb01rst DD, and not so much from the second. We emphasize this because\nDDD is really just another falsi\ufb01cation exercise, and just as we would expect no effect had we done the DD on this placebo group, we hope that our DDD estimate is also based on negligible effects among the control group.\nWhat we have done up to now is show how to use sample analogs and simple differences in means to estimate the treatment effect using DDD. But we can also use regression to control for additional covari- ates that perhaps are necessary to close backdoor paths and so forth. What does that regression equation look like?", "start_char_idx": 3680, "end_char_idx": 8023, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4d7ecb51-5ac1-44e2-bedc-2f7ee8f0438b": {"__data__": {"id_": "4d7ecb51-5ac1-44e2-bedc-2f7ee8f0438b", "embedding": null, "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "83d6e205-d5f0-4291-802f-85c79a899db8", "node_type": "4", "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "hash": "67c25ce71063b06809b4b6ddca5b8e8fc47c015726d3cb1dda1af9ff2c428e85", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "66732e29-5597-4743-8462-ff2c51405ddf", "node_type": "1", "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "hash": "4000834f48ae41a05c943e7618221287051116120463a1261779ed414d307ff2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "27160782-c7dc-4f06-bb51-4a7843652f5d", "node_type": "1", "metadata": {}, "hash": "be4ded4c57c355740fb65697ced3b11ea026bb8afe197416b7309d97ea689ebe", "class_name": "RelatedNodeInfo"}}, "text": "Thus when we calcu- late the DDD, we know that most of that calculation is coming from the \ufb01rst DD, and not so much from the second. We emphasize this because\nDDD is really just another falsi\ufb01cation exercise, and just as we would expect no effect had we done the DD on this placebo group, we hope that our DDD estimate is also based on negligible effects among the control group.\nWhat we have done up to now is show how to use sample analogs and simple differences in means to estimate the treatment effect using DDD. But we can also use regression to control for additional covari- ates that perhaps are necessary to close backdoor paths and so forth. What does that regression equation look like? Both the regression itself, and the data structure upon which the regression is based, are complicated because of the stacking of different groups and the sheer number of interactions involved. Estimating a DDD model requires estimating the following regression:\n+ \u03b25(\u03c4 \u00d7 D)ti + \u03b26(\u03b4 \u00d7 D)ij + \u03b27(\u03b4 \u00d7 \u03c4 \u00d7 D)ijt + \u03b5ijt\nwhere the parameter of interest is \u03b27. First, notice the additional sub- j. This j indexes whether it\u2019s the main category of interest script, (e.g., low-wage employment) or the within-state comparison group (e.g., high-wage employment). This requires a stacking of the data into a panel structure by group, as well as state. Second, the DDD model requires that you include all possible interactions across the group dummy \u03b4j, the post-treatment dummy \u03c4t and the treatment state dummy Di. The regression must include each dummy independently, each individual interaction, and the triple differences interaction. One of these will be dropped due to multicollinearity, but I include them in the equation so that you can visualize all the factors used in the product of these terms.\nAbortion legalization and long-term gonorrhea incidence. Now that we know a little about the DD design, it would probably be bene\ufb01cial to replicate a paper. And since the DDD requires reshaping panel data multiple times, that makes working through a detailed replication even more important. The study we will be replicating is Cunningham and Cornwell [2013], one of my \ufb01rst publications and the third chapter of my dissertation. Buckle up, as this will be a bit of a roller-coaster ride.\nGruber et al. [1999] was the beginning of what would become a controversial literature in reproductive health. They wanted to know the characteristics of the marginal child aborted had that child reached their teen years. The authors found that the marginal counterfactual child aborted was 60% more likely to grow up in a single-parent house- hold, 50% more likely to live in poverty, and 45% more likely to be a welfare recipient. Clearly there were strong selection effects related to early abortion whereby it selected on families with fewer resources.\nTheir \ufb01nding about the marginal child led John Donohue and Steven Levitt to wonder if there might be far-reaching effects of abor- tion legalization given the strong selection associated with its usage in the early 1970s. In Donohue and Levitt [2001], the authors argued that they had found evidence that abortion legalization had also led to massive declines in crime rates. Their interpretation of the results was that abortion legalization had reduced crime by removing high-risk individuals from a birth cohort, and as that cohort aged, those counter- factual crimes disappeared. Levitt [2004] attributed as much as 10% of the decline in crime between 1991 and 2001 to abortion legalization in the 1970s.\nThis study was, not surprisingly, incredibly controversial\u2014some of it warranted but some unwarranted. For instance, some attacked the paper on ethical grounds and argued the paper was revitalizing the pseudoscience of eugenics. But Levitt was careful to focus only on the scienti\ufb01c issues and causal effects and did not offer policy advice based on his own private views, whatever those may be.\nBut some of the criticism the authors received was legitimate pre- cisely because it centered on the research design and execution itself. Joyce [2004], Joyce [2009], and Foote and Goetz [2008] disputed the abortion-crime \ufb01ndings\u2014some through replication exercises using dif- ferent data, some with different research designs, and some through the discovery of key coding errors and erroneous variable construction. One study in particular challenged the whole enterprise of estimat- ing longrun improvements due to abortion legalization.", "start_char_idx": 7325, "end_char_idx": 11808, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "27160782-c7dc-4f06-bb51-4a7843652f5d": {"__data__": {"id_": "27160782-c7dc-4f06-bb51-4a7843652f5d", "embedding": null, "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "83d6e205-d5f0-4291-802f-85c79a899db8", "node_type": "4", "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "hash": "67c25ce71063b06809b4b6ddca5b8e8fc47c015726d3cb1dda1af9ff2c428e85", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d7ecb51-5ac1-44e2-bedc-2f7ee8f0438b", "node_type": "1", "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "hash": "f76d3b81f41d871c994798ed1c2dbd28b1ad87ae72d17a52834a26757e23b8ca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "525f5141-f421-40e9-a2e4-6d17ba4bc958", "node_type": "1", "metadata": {}, "hash": "232ee92f707d40071a528c995cb997b481057abd5e98d67e13b8eaf620456bd4", "class_name": "RelatedNodeInfo"}}, "text": "This study was, not surprisingly, incredibly controversial\u2014some of it warranted but some unwarranted. For instance, some attacked the paper on ethical grounds and argued the paper was revitalizing the pseudoscience of eugenics. But Levitt was careful to focus only on the scienti\ufb01c issues and causal effects and did not offer policy advice based on his own private views, whatever those may be.\nBut some of the criticism the authors received was legitimate pre- cisely because it centered on the research design and execution itself. Joyce [2004], Joyce [2009], and Foote and Goetz [2008] disputed the abortion-crime \ufb01ndings\u2014some through replication exercises using dif- ferent data, some with different research designs, and some through the discovery of key coding errors and erroneous variable construction. One study in particular challenged the whole enterprise of estimat- ing longrun improvements due to abortion legalization. For instance, Ted Joyce, an expert on reproductive health, cast doubt on the abortion-crime hypothesis using a DDD design [Joyce, 2009]. In addi- tion to challenging Donohue and Levitt [2001], Joyce also threw down a gauntlet. He argued that if abortion legalization had such extreme\nnegative selection as claimed by by Gruber et al. [1999] and Donohue and Levitt [2001], then it shouldn\u2019t show up just in crime. It should show up everywhere. Joyce writes:\nIf abortion lowers homicide rates by 20\u201330%, then it is likely to have affected an entire spectrum of outcomes associated with well-being: infant health, child development, schooling, earn- ings and marital status. Similarly, the policy implications are broader than abortion. Other interventions that affect fertility control and that lead to fewer unwanted births\u2014contraception In short, a or sexual abstinence\u2014have huge potential payoffs. causal relationship between legalized abortion and crime has such signi\ufb01cant rami\ufb01cations for social policy and at the same time is so controversial, that further assessment of the identify- ing assumptions and their robustness to alternative strategies is warranted. [112]\nCunningham and Cornwell [2013] took up Joyce\u2019s challenge. Our study estimated the effects of abortion legalization on long-term gon- orrhea incidence. Why gonorrhea? For one, single-parent households are a risk factor that lead to earlier sexual activity and unprotected sex, and Levine et al. [1999] found that abortion legalization caused teen childbearing to fall by 12%. Other risky outcomes had been found by numerous authors. Charles and Stephens [2006] reported that children exposed in utero to a legalized abortion regime were less likely to use illegal substances, which is correlated with risky sexual behavior.\nMy research design differed from Donohue and Levitt [2001] in that they used state-level lagged values of an abortion ratio, whereas I used difference-in-differences. My design exploited the early repeal of abor- tion in \ufb01ve states in 1970 and compared those states to the states that were legalized under Roe v. Wade in 1973. To do this, I needed cohort- speci\ufb01c data on gonorrhea incidence by state and year, but as those data are not collected by the CDC, I had to settle for second best. That second best was the CDC\u2019s gonorrhea data broken into \ufb01ve-year age categories (e.g., age 15\u201319, age 20\u201324). But this might still be use- ful because even with aggregate data, it might be possible to test the model I had in mind.\nTo understand this next part, which I consider the best part of my study, you must \ufb01rst accept a basic view of science that good theo- ries make very speci\ufb01c falsi\ufb01able hypotheses. The more speci\ufb01c the hypothesis, the more convincing the theory, because if we \ufb01nd evi- dence exactly where the theory predicts, a Bayesian is likely to update her beliefs towards accepting the theory\u2019s credibility. Let me illustrate what I mean with a brief detour involving Albert Einstein\u2019s theory of relativity.\nEinstein\u2019s theory made several falsi\ufb01able hypotheses. One of them involved a precise prediction of the warping of light as it moved past a large object, such as a star. The problem was that testing this the- ory involved observing distance between stars at night and comparing it to measurements made during the day as the starlight moved past the sun. Problem was, the sun is too bright in the daytime to see the stars, so those critical measurements can\u2019t be made.", "start_char_idx": 10875, "end_char_idx": 15293, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "525f5141-f421-40e9-a2e4-6d17ba4bc958": {"__data__": {"id_": "525f5141-f421-40e9-a2e4-6d17ba4bc958", "embedding": null, "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "83d6e205-d5f0-4291-802f-85c79a899db8", "node_type": "4", "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "hash": "67c25ce71063b06809b4b6ddca5b8e8fc47c015726d3cb1dda1af9ff2c428e85", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "27160782-c7dc-4f06-bb51-4a7843652f5d", "node_type": "1", "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "hash": "fde1d4a4db40bdb372400da3c354c5fbedc8e37ebe8438f2d81d6afb1ebe1de8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4f25183f-6f0a-4e20-a718-0f7f30225b20", "node_type": "1", "metadata": {}, "hash": "ddbd0289cc2a8d8d0088a49ef65e606bb179ed8745357df6dcac9095e761de00", "class_name": "RelatedNodeInfo"}}, "text": "The more speci\ufb01c the hypothesis, the more convincing the theory, because if we \ufb01nd evi- dence exactly where the theory predicts, a Bayesian is likely to update her beliefs towards accepting the theory\u2019s credibility. Let me illustrate what I mean with a brief detour involving Albert Einstein\u2019s theory of relativity.\nEinstein\u2019s theory made several falsi\ufb01able hypotheses. One of them involved a precise prediction of the warping of light as it moved past a large object, such as a star. The problem was that testing this the- ory involved observing distance between stars at night and comparing it to measurements made during the day as the starlight moved past the sun. Problem was, the sun is too bright in the daytime to see the stars, so those critical measurements can\u2019t be made. But Andrew Crommelin and Arthur Eddington realized the measurements could be made using an ingenious natural experiment. That natural experiment was an eclipse. They shipped telescopes to different parts of the world under the eclipse\u2019s path so that they had multiple chances to make the measurements. They decided to measure the distances of a large cluster of stars passing by the sun when it was dark and then immedi- ately during an eclipse (Figure 61). That test was over a decade after Einstein\u2019s work was \ufb01rst published [Coles, 2019]. Think about it for a second\u2014Einstein\u2019s theory by deduction is making predictions about phenomena that no one had ever really observed before. If this phe- nomena turned out to exist, then how couldn\u2019t the Bayesian update her beliefs and accept that the theory was credible? Incredibly, Einstein was right\u2014just as he predicted, the apparent position of these stars shifted when moving around the sun. Incredible!\nSo what does that have to do with my study of abortion legaliza- tion and gonorrhea? The theory of abortion legalization having strong selection effects on cohorts makes very speci\ufb01c predictions about the shape of observed treatment effects. And if we found evidence for that shape, we\u2019d be forced to take the theory seriously. So what what were these unusual yet testable predictions exactly?\nThe testable prediction from the staggered adoption of abortion legalization concerned the age-year-state pro\ufb01le of gonorrhea. The\nFigure 61. Light bending around the sun, predicted by Einstein, and con\ufb01rmed in a\nnatural experiment involving an eclipse. Artwork by Seth Hahne \u00a92020.\nearly repeal of abortion by \ufb01ve states three years before the rest of the country predicts lower incidence among 15- to 19-year-olds in the repeal states only during the 1986\u20131992 period relative to their Roe counterparts as the treated cohorts aged. That\u2019s not really all that special a prediction though. Maybe something happens in those same states \ufb01fteen to nineteen years later that isn\u2019t controlled for, for instance. What else?\nThe abortion legalization theory also predicted the shape of the observed treatment effects in this particular staggered adoption. Speci\ufb01cally, we should observe nonlinear treatment effects. These treatment effects should be increasingly negative from 1986 to 1989, plateau from 1989 to 1991, then gradually dissipate until 1992. In other words, the abortion legalization hypothesis predicts a parabolic treat- ment effect as treated cohorts move through the age distribution. All coe\ufb03cients on the DD coe\ufb03cients beyond 1992 should be zero and/or statistically insigni\ufb01cant.\nI illustrate these predictions in Figure 62. The top horizontal axis shows the year of the panel, the vertical axis shows the age in calendar years, and the cells show the cohort for a given person of a certain age in that given year. For instance, consider a 15-year-old in 1985. She\nrhea incidence. Reprinted from Cunningham, S. and Cornwell, C. (2013). \u201cThe Long-Run Effect of Abortion on Sexually Transmitted Infections.\u201d Amer- ican Law and Economics Review, 15(1):381\u2013407. Permission from Oxford University Press.\nwas born in 1970. A 15-year-old in 1986 was born in 1971. A 15-year-old in 1987 was born in 1972, and so forth. I mark the cohorts who were treated by either repeal or Roe in different shades of gray.\nThe theoretical predictions of the staggered rollout is shown at the bottom of Figure 62. In 1986, only one cohort (the 1971 cohort) was treated and only in the repeal states.", "start_char_idx": 14511, "end_char_idx": 18830, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4f25183f-6f0a-4e20-a718-0f7f30225b20": {"__data__": {"id_": "4f25183f-6f0a-4e20-a718-0f7f30225b20", "embedding": null, "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "83d6e205-d5f0-4291-802f-85c79a899db8", "node_type": "4", "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "hash": "67c25ce71063b06809b4b6ddca5b8e8fc47c015726d3cb1dda1af9ff2c428e85", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "525f5141-f421-40e9-a2e4-6d17ba4bc958", "node_type": "1", "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "hash": "1d93754b1f467785d090a1874fb97a6519382f610c0381b279db25c2f155ae0c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6595c7bf-b400-42f1-b382-63b4fb774af6", "node_type": "1", "metadata": {}, "hash": "3aeec85f1841b3ea72e2ba59db420fa16265e8073937ea28d91af7d1375ff188", "class_name": "RelatedNodeInfo"}}, "text": "For instance, consider a 15-year-old in 1985. She\nrhea incidence. Reprinted from Cunningham, S. and Cornwell, C. (2013). \u201cThe Long-Run Effect of Abortion on Sexually Transmitted Infections.\u201d Amer- ican Law and Economics Review, 15(1):381\u2013407. Permission from Oxford University Press.\nwas born in 1970. A 15-year-old in 1986 was born in 1971. A 15-year-old in 1987 was born in 1972, and so forth. I mark the cohorts who were treated by either repeal or Roe in different shades of gray.\nThe theoretical predictions of the staggered rollout is shown at the bottom of Figure 62. In 1986, only one cohort (the 1971 cohort) was treated and only in the repeal states. Therefore, we should see small declines in gonorrhea incidence among 15-year-olds in 1986 relative to Roe states. In 1987, two cohorts in our data are treated in the repeal states relative to Roe, so we should see larger effects in absolute value than we saw in 1986. But from 1988 to 1991, we should at most see only three net treated cohorts in the repeal states because starting in 1988, the Roe state cohorts enter and begin erasing those differences. Starting in 1992, the effects should get smaller in absolute value until\nFigure 63. Differences in gonorrhea incidence among black females between repeal and Roe cohorts expressed as coe\ufb03cient plots. Reprinted from Cunningham, S. and Cornwell, C. (2013). \u201cThe Long-Run Effect of Abortion on Sexually Transmitted Infections.\u201d American Law and Economics Review, 15(1):381\u2013 407. Permission from Oxford University Press.\n1992, beyond which there should be no difference between repeal and Roe states.\nIt is interesting that something so simple as a staggered policy roll- out should provide two testable hypotheses that together can provide some insight into whether there is credibility to the negative selection in abortion legalization story. If we cannot \ufb01nd evidence for a nega- tive parabola during this speci\ufb01c, narrow window, then the abortion legalization hypothesis has one more nail in its co\ufb03n.\nA simple graphic for gonorrhea incidence among black 15- to 19- year-olds can help illustrate our \ufb01ndings. Remember, a picture is worth a thousand words, and whether it\u2019s RDD or DD, it\u2019s helpful to show pictures like these to prepare the reader for the table after table of regression coe\ufb03cients. So notice what the raw data looks like in Figure 63.\nFirst let\u2019s look at the raw data. I have shaded the years correspond- ing to the window where we expect to \ufb01nd effects. In Figure 63, we\nsee the dynamics that will ultimately be picked up in the regression coe\ufb03cients\u2014the Roe states experienced a large and sustained gonor- rhea epidemic that only waned once the treated cohorts emerged and overtook the entire data series.\nNow let\u2019s look at regression coe\ufb03cients. Our estimating equation\nis as follows:\nYst = \u03b21Repeals + \u03b22DTt + \u03b23tRepeals \u00d7 DTt + Xst\u03c8 + \u03b1sDSs + \u03b5st\nwhere Y is the log number of new gonorrhea cases for 15- to 19-year- olds (per 100,000 of the population); Repeals equals 1 if the state legal- ized abortion prior to Roe; DTt is a year dummy; DSs is a state dummy; t is a time trend; X is a matrix of covariates. In the paper, I sometimes included state-speci\ufb01c linear trends, but for this analysis, I present the simpler model. Finally, \u03b5st is a structural error term assumed to be conditionally independent of the regressors. All standard errors, fur- thermore, were clustered at the state level allowing for arbitrary serial correlation.\nI present the plotted coe\ufb03cients from this regression for simplicity (and because pictures can be so powerful) in Figure 64. As can be seen in Figure 64, there is a negative effect during the window where Roe has not fully caught up, and that negative effect forms a parabola\u2014just as our theory predicted.\nNow, a lot of people might be done, but if you are reading this book, you have revealed that you are not like a lot of people.", "start_char_idx": 18170, "end_char_idx": 22080, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6595c7bf-b400-42f1-b382-63b4fb774af6": {"__data__": {"id_": "6595c7bf-b400-42f1-b382-63b4fb774af6", "embedding": null, "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "83d6e205-d5f0-4291-802f-85c79a899db8", "node_type": "4", "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "hash": "67c25ce71063b06809b4b6ddca5b8e8fc47c015726d3cb1dda1af9ff2c428e85", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4f25183f-6f0a-4e20-a718-0f7f30225b20", "node_type": "1", "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "hash": "f077bf40477acaf03cc117ec37fbd548ec6d051b7bf89d46c62ef18fa5d72abc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e9c76c9e-8fc2-4769-920a-7ea830fe05e4", "node_type": "1", "metadata": {}, "hash": "63950d4f48c5240f83509e1606cd63ad7e126c4f2ab7d606a2ca7d1c657efa1f", "class_name": "RelatedNodeInfo"}}, "text": "In the paper, I sometimes included state-speci\ufb01c linear trends, but for this analysis, I present the simpler model. Finally, \u03b5st is a structural error term assumed to be conditionally independent of the regressors. All standard errors, fur- thermore, were clustered at the state level allowing for arbitrary serial correlation.\nI present the plotted coe\ufb03cients from this regression for simplicity (and because pictures can be so powerful) in Figure 64. As can be seen in Figure 64, there is a negative effect during the window where Roe has not fully caught up, and that negative effect forms a parabola\u2014just as our theory predicted.\nNow, a lot of people might be done, but if you are reading this book, you have revealed that you are not like a lot of people. Credibly identi\ufb01ed causal effects requires both \ufb01nding effects, and ruling out alternative explanations. This is necessary because the fundamental problem of causal inference keeps us blind to the truth. But one way to alleviate some of that doubt is through rigorous placebo analysis. Here I present evidence from a triple difference in which an untreated cohort is used as a within-state control.\nWe chose the 25- to 29-year-olds in the same states as within- state comparison groups instead of 20- to 24-year-olds after a lot of thought. Our reasoning was that we needed an age group that was close enough to capture common trends but far enough so as not to violate SUTVA. Since 15- to 19-year-olds were more likely than 25- to 29-year-olds to have sex with 20- to 24-year-olds, we chose the\nslightly older group as the within-stage control. But there\u2019s a trade- off here. Choose a group too close and you get SUTVA violations. Choose a group too far and they no longer can credibly soak up the heterogeneities you\u2019re worried about. The estimating equation for this regression is:\nwhere the DDD parameter we are estimating is \u03b44t\u2014the full interaction. In case this wasn\u2019t obvious, there are 7 separate dummies because our DDD parameter has all three interactions. Thus since there are eight combinations, we had to drop one as the omitted group, and control separately for the other seven. Here we present the table of coe\ufb03- cients. Note that the effect should be concentrated only among the treatment years as before, and second, it should form a parabola. The results are presented in Figure 65.\nHere we see the prediction start to break down. Though there are negative effects for years 1986 to 1990, the 1991 and 1992 coe\ufb03cients are positive, which is not consistent with our hypothesis. Furthermore, only the \ufb01rst four coe\ufb03cients are statistically signi\ufb01cant. Nevertheless, given the demanding nature of DDD, perhaps this is a small victory in favor of Gruber et al. [1999] and Donohue and Levitt [2001]. Perhaps the theory that abortion legalization had strong selection effects on cohorts has some validity.\nPutting aside whether you believe the results, it is still valuable to replicate the results based on this staggered design. Recall that I said the DDD design requires stacking the data, which may seem like a bit of a black box, so I\u2019d like to examine these data now.7\n7 In the original Cunningham and Cornwell [2013], we estimated models with multi-\nway clustering correction, but the package for this in Stata is no longer supported.\nTherefore, we will estimate the same models as in Cunningham and Cornwell [2013] using cluster robust standard errors. In all prior analysis, I clustered the standard errors\nat the state level so as to maintain consistency with this code.", "start_char_idx": 21320, "end_char_idx": 24874, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e9c76c9e-8fc2-4769-920a-7ea830fe05e4": {"__data__": {"id_": "e9c76c9e-8fc2-4769-920a-7ea830fe05e4", "embedding": null, "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "83d6e205-d5f0-4291-802f-85c79a899db8", "node_type": "4", "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "hash": "67c25ce71063b06809b4b6ddca5b8e8fc47c015726d3cb1dda1af9ff2c428e85", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6595c7bf-b400-42f1-b382-63b4fb774af6", "node_type": "1", "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "hash": "7342d63282621654c088ae7cd4431f2ad65c239c6842c7271fa57b660ab4a3d5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3187ef9b-b450-49aa-a908-21ed310a945d", "node_type": "1", "metadata": {}, "hash": "2a812a5b35b15f289256e38018615a9ee7f3f75f2d3b541f1aaea4b112f7506e", "class_name": "RelatedNodeInfo"}}, "text": "[1999] and Donohue and Levitt [2001]. Perhaps the theory that abortion legalization had strong selection effects on cohorts has some validity.\nPutting aside whether you believe the results, it is still valuable to replicate the results based on this staggered design. Recall that I said the DDD design requires stacking the data, which may seem like a bit of a black box, so I\u2019d like to examine these data now.7\n7 In the original Cunningham and Cornwell [2013], we estimated models with multi-\nway clustering correction, but the package for this in Stata is no longer supported.\nTherefore, we will estimate the same models as in Cunningham and Cornwell [2013] using cluster robust standard errors. In all prior analysis, I clustered the standard errors\nat the state level so as to maintain consistency with this code.\n1 * DD estimate of 15-19 year olds in repeal states vs Roe states 2 use https://github.com/scunning1975/mixtape/raw/master/abortion.dta, clear 3 xi: reg lnr i.repeal*i.year i.fip acc ir pi alcohol crack poverty income ur if bf15==1\n4 5 * ssc install parmest, replace 6 7 parmest, label for(estimate min95 max95 %8.2f) li(parm label estimate min95\nmax95) saving(bf15_DD.dta, replace)\n8 9 use ./bf15_DD.dta, replace 10 11 keep in 17/31 12 13 gen 14 replace year=1987 in 2 15 replace year=1988 in 3 16 replace year=1989 in 4 17 replace year=1990 in 5 18 replace year=1991 in 6 19 replace year=1992 in 7 20 replace year=1993 in 8 21 replace year=1994 in 9 22 replace year=1995 in 10 23 replace year=1996 in 11 24 replace year=1997 in 12 25 replace year=1998 in 13 26 replace year=1999 in 14 27 replace year=2000 in 15 28 29 sort year 30 31 twoway (scatter estimate year, mlabel(year) mlabsize(vsmall) msize(tiny)) (rcap min95 max95 year, msize(vsmall)), ytitle(Repeal x year estimated coefficient) yscale(titlegap(2)) yline(0, lwidth(vvvthin) lcolor(black)) xtitle(Year) xline(1986 1987 1988 1989 1990 1991 1992, lwidth(vvvthick) lpattern(solid) lcolor(ltblue)) xscale(titlegap(2)) title(Estimated effect of abortion legalization on gonorrhea) subtitle(Black females 15-19 year-olds) note(Whisker plots are estimated coefficients of DD estimator from Column b of Table 2.) legend(off)\n1 #-- DD estimate of 15-19 year olds in repeal states vs Roe states 2 library(tidyverse) 3 library(haven) 4 library(estimatr) 5 6 read_data <- function(df) 7 { 8 9 10 11 12 } 13 14 abortion <- read_data(\"abortion.dta\") %>% 15 mutate( 16 17 18 19 20 21 22 reg <- abortion %>% 23 24\nfilter(bf15 == 1) %>% lm_robust(lnr ~ repeal*year + fip + acc + ir + pi + alcohol+ crack + poverty+ (cid:12)\u2192\ndf, sep = \"\")\n25 26 27 abortion_plot <- tibble( 28 sd = reg$std.error[-1:-75], 29 mean = reg$coefficients[-1:-75], 30 31 32 abortion_plot %>% 33 34\nggplot(aes(x = year, y = mean)) + geom_rect(aes(xmin=1986, xmax=1992, ymin=-Inf, ymax=Inf), fill = \"cyan\", alpha (cid:12)\u2192\ndata = ., weights = totpop, clusters = fip)\ngeom_point()+ geom_text(aes(label = year), hjust=-0.002, vjust = -0.03)+ geom_hline(yintercept = 0) + geom_errorbar(aes(ymin = mean - sd*1.96, ymax = mean + sd*1.96), width = 0.2,\nThe second line estimates the regression equation.", "start_char_idx": 24057, "end_char_idx": 27192, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3187ef9b-b450-49aa-a908-21ed310a945d": {"__data__": {"id_": "3187ef9b-b450-49aa-a908-21ed310a945d", "embedding": null, "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "83d6e205-d5f0-4291-802f-85c79a899db8", "node_type": "4", "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "hash": "67c25ce71063b06809b4b6ddca5b8e8fc47c015726d3cb1dda1af9ff2c428e85", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e9c76c9e-8fc2-4769-920a-7ea830fe05e4", "node_type": "1", "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "hash": "1b37b3fd6e913a11d991d4ee50db8ad428026a87da809476d25f11a2df371432", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2601e591-7ae0-41f7-b8c6-01ec634fb4a3", "node_type": "1", "metadata": {}, "hash": "f5e719604a183a18592775a481a6cc89c129527e9a75200cb0faf95a1fabdddf", "class_name": "RelatedNodeInfo"}}, "text": "The dynamic DD coe\ufb03cients are captured by the repeal-year interactions. These are the coe\ufb03cients we used to create box plots in Figure 64. You can check these yourself.\nNote, for simplicity, I only estimated this for the black females (bf15==1) but you could estimate for the black males (bm15==1), white females (wf15==1), or white males (wm15==1). We do all four in the paper, but here we only focus on the black females aged 15\u201319 because the purpose of this section is to help you understand the esti- mation. I encourage you to play around with this model to see how robust the effects are in your mind using only this linear estimation.\nBut now I want to show you the code for estimating a triple differ- ence model. Some reshaping had to be done behind the scenes for this data structure, but it would take too long to post that here. For now, I will simply produce the commands that produce the black female result, and I encourage you to explore the panel data structure so as to familiarize yourself with the way in which the data are organized.\nNotice that some of these were already interactions (e.g., yr), which was my way to compactly include all of the interactions. I did this pri- marily to give myself more control over what variables I was using. But I encourage you to study the data structure itself so that when you need to estimate your own DDD, you\u2019ll have a good handle on what form the data must be in in order to execute so many interactions.\n1 use https://github.com/scunning1975/mixtape/raw/master/abortion.dta, clear 2 3 * DDD estimate for 15-19 year olds vs. 20-24 year olds in repeal vs Roe states 4 gen yr=(repeal) & (younger==1) 5 gen wm=(wht==1) & (male==1) 6 gen wf=(wht==1) & (male==0) 7 gen bm=(wht==0) & (male==1) 8 gen bf=(wht==0) & (male==0) 9 char year[omit] 1985 10 char repeal[omit] 0 11 char younger[omit] 0\n(continued)\nSTATA (continued)\n11 char fip[omit] 1 12 char fa[omit] 0 13 char yr[omit] 0 14 xi: reg lnr i.repeal*i.year i.younger*i.repeal i.younger*i.year i.yr*i.year i.fip*t acc pi ir alcohol crack poverty income ur if bf==1 & (age==15 | age==25) [aweight=totpop], cluster(fip)\n15 16 parmest, label for(estimate min95 max95 %8.2f) li(parm label estimate min95\nmax95) saving(bf15_DDD.dta, replace)\n17 18 use ./bf15_DDD.dta, replace 19 20 keep in 82/96 21 22 gen 23 replace year=1987 in 2 24 replace year=1988 in 3 25 replace year=1989 in 4 26 replace year=1990 in 5 27 replace year=1991 in 6 28 replace year=1992 in 7 29 replace year=1993 in 8 30 replace year=1994 in 9 31 replace year=1995 in 10 32 replace year=1996 in 11 33 replace year=1997 in 12 34 replace year=1998 in 13 35 replace year=1999 in 14 36 replace year=2000 in 15 37 38 sort year 39 40 twoway (scatter estimate year, mlabel(year) mlabsize(vsmall) msize(tiny)) (rcap min95 max95 year, msize(vsmall)), ytitle(Repeal x 20-24yo x year estimated coefficient) yscale(titlegap(2)) yline(0, lwidth(vvvthin) lcolor(black)) xtitle(Year) xline(1986 1987 1988 1989 1990 1991 1992, lwidth(vvvthick) lpattern(solid) lcolor(ltblue)) xscale(titlegap(2)) title(Estimated effect of abortion legalization on gonorrhea) subtitle(Black females 15-19 year-olds) note(Whisker plots are estimated coefficients of DDD estimator from Column b of Table 2.)", "start_char_idx": 27193, "end_char_idx": 30444, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2601e591-7ae0-41f7-b8c6-01ec634fb4a3": {"__data__": {"id_": "2601e591-7ae0-41f7-b8c6-01ec634fb4a3", "embedding": null, "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "83d6e205-d5f0-4291-802f-85c79a899db8", "node_type": "4", "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "hash": "67c25ce71063b06809b4b6ddca5b8e8fc47c015726d3cb1dda1af9ff2c428e85", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3187ef9b-b450-49aa-a908-21ed310a945d", "node_type": "1", "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "hash": "c4a007cf7e2a873cee1bd33d2179345e89f6c1eac50350a36ea74245c8302be1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e11ef6db-88b9-4c78-8a5e-3c6501758d55", "node_type": "1", "metadata": {}, "hash": "640a248bb49cc8389355a9bae83afbfb84b36a663100826c8cbe4b0e55878238", "class_name": "RelatedNodeInfo"}}, "text": "legend(off)\n1 library(tidyverse) 2 library(haven) 3 library(estimatr) 4 5 read_data <- function(df) 6 { 7 8 9 10 11 } 12 13 abortion <- read_data(\"abortion.dta\") %>% 14 mutate( 15 16 17 18 19 20 21 22 23 24 25 26 27 28 regddd <- lm_robust(lnr ~ repeal*year + younger*repeal + younger*year + yr*year\nrepeal = as_factor(repeal), year = as_factor(year), fip fa younger = as_factor(younger), yr wm = as_factor(case_when(wht == 1 & male == 1 ~ 1, TRUE ~ 0)), = as_factor(case_when(wht == 1 & male == 0 ~ 1, TRUE ~ 0)), wf bm = as_factor(case_when(wht == 0 & male == 1 ~ 1, TRUE ~ 0)), bf ) %>% filter(bf == 1 & (age == 15 | age == 25))\ndf, sep = \"\")\n29 30 31 abortion_plot <- tibble( 32 sd = regddd$std.error[110:124], 33 mean = regddd$coefficients[110:124], 34 35\n+ fip*t + acc + ir + pi + alcohol + crack + poverty + income + ur, data = abortion, weights = totpop, clusters = fip)\n(continued)\nR (continued)\nggplot(aes(x = year, y = mean)) + geom_rect(aes(xmin=1986, xmax=1992, ymin=-Inf, ymax=Inf), fill = \"cyan\", alpha (cid:12)\u2192\ngeom_point()+ geom_text(aes(label = year), hjust=-0.002, vjust = -0.03)+ geom_hline(yintercept = 0) + geom_errorbar(aes(ymin = mean-sd*1.96, ymax = mean+sd*1.96), width = 0.2,\nGoing beyond Cunningham and Cornwell [2013]. The US experience with abortion legalization predicted a parabola from 1986 to 1992 for 15- to 19-year-olds, and using a DD design, that\u2019s what I found. I also esti- mated the effect using a DDD design, and while the effects weren\u2019t as pretty as what I found with DD, there appeared to be something going on in the general vicinity of where the model predicted. So boom goes the dynamite, right? Can\u2019t we be done \ufb01nally? Not quite.\nWhereas my original study stopped there, I would like to go a little farther. The reason can be seen in the following Figure 66. This is a modi\ufb01ed version of Figure 62, with the main difference being I have created a new parabola for the 20- to 24-year-olds.\nLook carefully at Figure 66. Insofar as the early 1970s cohorts were treated in utero with abortion legalization, then we should see not just a parabola for the 15- to 19-year-olds for 1986 to 1992 but also for the 20- to 24-year-olds for years 1991 to 1997 as the cohorts continued to age.8\nI did not examine the 20- to 24-year-old cohort when I \ufb01rst wrote this paper because at that time I doubted that the selection effects for risky sex would persist into adulthood given that youth display consid- erable risk-taking behavior. But with time come new perspectives, and these days I don\u2019t have strong priors that the selection effects would necessarily vanish after teenage years. So I\u2019d like to conduct that addi- tional analysis here and now for the \ufb01rst time. Let\u2019s estimate the same DD model as before, only for Black females aged 20\u201324.", "start_char_idx": 30445, "end_char_idx": 33228, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e11ef6db-88b9-4c78-8a5e-3c6501758d55": {"__data__": {"id_": "e11ef6db-88b9-4c78-8a5e-3c6501758d55", "embedding": null, "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "83d6e205-d5f0-4291-802f-85c79a899db8", "node_type": "4", "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "hash": "67c25ce71063b06809b4b6ddca5b8e8fc47c015726d3cb1dda1af9ff2c428e85", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2601e591-7ae0-41f7-b8c6-01ec634fb4a3", "node_type": "1", "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "hash": "666b7a6091eb4b02d4c6ec20721be2ed65050473c58c10c06a0b76939de604b9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c2452a17-c720-4357-a07d-f00870f891f8", "node_type": "1", "metadata": {}, "hash": "041c57a8ebdc91e9afb3dcd49ad24bcd8e22252c0fdc6ab7031524646ebd6e74", "class_name": "RelatedNodeInfo"}}, "text": "But with time come new perspectives, and these days I don\u2019t have strong priors that the selection effects would necessarily vanish after teenage years. So I\u2019d like to conduct that addi- tional analysis here and now for the \ufb01rst time. Let\u2019s estimate the same DD model as before, only for Black females aged 20\u201324.\n8 There is a third prediction on the 25- to 29-year-olds, but for the sake of space, I\n1 use https://github.com/scunning1975/mixtape/raw/master/abortion.dta, clear 2 3 * Second DD model for 20-24 year old black females 4 char year[omit] 1985 5 xi: reg lnr i.repeal*i.year i.fip acc ir pi alcohol crack poverty income ur if (race==2\n& sex==2 & age==20) [aweight=totpop], cluster(fip)\n1 library(tidyverse) 2 library(haven) 3 library(estimatr) 4 5 read_data <- function(df) 6 { 7 8 9 10 11 } 12 13 abortion <- read_data(\"abortion.dta\") %>% 14 mutate( 15 16 17 18 19 20 21 reg <- abortion %>% 22 23\nfilter(race == 2 & sex == 2 & age == 20) %>% lm_robust(lnr ~ repeal*year + fip + acc + ir + pi + alcohol+ crack + poverty+ (cid:12)\u2192\ndf, sep = \"\")\ndata = ., weights = totpop, clusters = fip)\nAs before, we will focus just on the coe\ufb03cient plots. We show that in Figure 67. There are a couple of things about this regression output that are troubling. First, there is a negative parabola showing up where there wasn\u2019t necessarily one predicted\u2014the 1986\u20131992 period. Note that is the period where only the 15- to 19-year-olds were the treated\ncohorts, suggesting that our 15- to 19-year-old analysis was picking up something other than abortion legalization. But that was also the justi\ufb01cation for using DDD, as clearly something else is going on in the repeal versus Roe states during those years that we cannot adequately control for with our controls and \ufb01xed effects.\nThe second thing to notice is that there is no parabola in the treat- ment window for the treatment cohort. The effect sizes are negative in the beginning, but shrink in absolute value when they should be grow- ing. In fact, the 1991 to 1997 period is one of convergence to zero, not divergence between these two sets of states.\nBut as before, maybe there are strong trending unobservables for all groups masking the abortion legalization effect. To check, let\u2019s use my DDD strategy with the 25- to 29-year-olds as the within- state control group. We can implement this by using the Stata code, abortion_ddd2.do and abortion_ddd2.R.", "start_char_idx": 32916, "end_char_idx": 35325, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c2452a17-c720-4357-a07d-f00870f891f8": {"__data__": {"id_": "c2452a17-c720-4357-a07d-f00870f891f8", "embedding": null, "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "83d6e205-d5f0-4291-802f-85c79a899db8", "node_type": "4", "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "hash": "67c25ce71063b06809b4b6ddca5b8e8fc47c015726d3cb1dda1af9ff2c428e85", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e11ef6db-88b9-4c78-8a5e-3c6501758d55", "node_type": "1", "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "hash": "8f5bfcc550267b5b12e6c2757f32f2f7d766b0b3a8c93e68e534442f469b8178", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f3746d27-f600-48bd-89e4-7b1cc757f9f6", "node_type": "1", "metadata": {}, "hash": "3a7d3a7795f78221333ab42927f86d0c3b161c740abfe1edd3c929a38479138d", "class_name": "RelatedNodeInfo"}}, "text": "But that was also the justi\ufb01cation for using DDD, as clearly something else is going on in the repeal versus Roe states during those years that we cannot adequately control for with our controls and \ufb01xed effects.\nThe second thing to notice is that there is no parabola in the treat- ment window for the treatment cohort. The effect sizes are negative in the beginning, but shrink in absolute value when they should be grow- ing. In fact, the 1991 to 1997 period is one of convergence to zero, not divergence between these two sets of states.\nBut as before, maybe there are strong trending unobservables for all groups masking the abortion legalization effect. To check, let\u2019s use my DDD strategy with the 25- to 29-year-olds as the within- state control group. We can implement this by using the Stata code, abortion_ddd2.do and abortion_ddd2.R.\n4 gen younger2 = 0 5 replace younger2 = 1 if age == 20 6 gen yr2=(repeal==1) & (younger2==1) 7 gen wm=(wht==1) & (male==1) 8 gen wf=(wht==1) & (male==0) 9 gen bm=(wht==0) & (male==1) 10 gen bf=(wht==0) & (male==0) 11 char year[omit] 1985 12 char repeal[omit] 0 13 char younger2[omit] 0 14 char fip[omit] 1 15 char fa[omit] 0 16 char yr2[omit] 0 17 xi: reg lnr i.repeal*i.year i.younger2*i.repeal i.younger2*i.year i.yr2*i.year i.fip*t\nacc pi ir alcohol crack poverty income ur if bf==1 & (age==20 | age==25) [aweight=totpop], cluster(fip)\n1 library(tidyverse) 2 library(haven) 3 library(estimatr) 4 5 read_data <- function(df) 6 { 7 8 9 10 11 } 12 13 abortion <- read_data(\"abortion.dta\") %>% 14 mutate( 15 16 17 18 19 20 21 22 23 24 25 26 27 regddd <- abortion %>% 28 29\nfilter(bf == 1 & (age == 20 | age ==25)) %>% lm_robust(lnr ~ repeal*year + acc + ir + pi + alcohol + crack + poverty + income (cid:12)\u2192\nrepeal = as_factor(repeal), year fip fa younger2 = case_when(age == 20 ~ 1, TRUE ~ 0), yr2 wm wf bm bf\ndf, sep = \"\")\ndata = ., weights = totpop, clusters = fip)\nFigure 68 shows the DDD estimated coe\ufb03cients for the treated cohort relative to a slightly older 25- to 29-year-old cohort. It\u2019s possible that the 25- to 29-year-old cohort is too close in age to function as a satisfactory within-state control; if those age 20\u201324 have sex with those who are age 25\u201329, for instance, then SUTVA is violated. There are other age groups, though, that you can try in place of the 25- to 29- year-olds, and I encourage you to do it for both the experience and the insights you might gleam.\nBut let\u2019s back up and remember the big picture. The abortion legal- ization hypothesis made a series of predictions about where negative parabolic treatment effects should appear in the data. And while we found some initial support, when we exploited more of those predic- tions, the results fell apart. A fair interpretation of this exercise is that our analysis does not support the abortion legalization hypothesis. Figure 68 shows several point estimates at nearly zero, and standard errors so large as to include both positive and negative values for these interactions.\nI included this analysis because I wanted to show you the power of a theory with numerous unusual yet testable predictions. Imagine for a moment if a parabola had showed up for all age groups in precisely the years predicted by the theory. Wouldn\u2019t we have to update our priors about the abortion legalization selection hypothesis? With predictions so narrow, what else could be causing it? It\u2019s precisely because the pre- dictions are so speci\ufb01c, though, that we are able to reject the abortion legalization hypothesis, at least for gonorrhea.", "start_char_idx": 34480, "end_char_idx": 38034, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f3746d27-f600-48bd-89e4-7b1cc757f9f6": {"__data__": {"id_": "f3746d27-f600-48bd-89e4-7b1cc757f9f6", "embedding": null, "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "83d6e205-d5f0-4291-802f-85c79a899db8", "node_type": "4", "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "hash": "67c25ce71063b06809b4b6ddca5b8e8fc47c015726d3cb1dda1af9ff2c428e85", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c2452a17-c720-4357-a07d-f00870f891f8", "node_type": "1", "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "hash": "5a6a5446854ad45d868e4a0ed495d70f46d5665bafd090820b0c047592460586", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ec5e20de-09c1-45a4-a676-71feea16ceef", "node_type": "1", "metadata": {}, "hash": "db0587e105769485e2fcaf621305ee240c42a25d46b25b90478a7c9a91edcb3a", "class_name": "RelatedNodeInfo"}}, "text": "And while we found some initial support, when we exploited more of those predic- tions, the results fell apart. A fair interpretation of this exercise is that our analysis does not support the abortion legalization hypothesis. Figure 68 shows several point estimates at nearly zero, and standard errors so large as to include both positive and negative values for these interactions.\nI included this analysis because I wanted to show you the power of a theory with numerous unusual yet testable predictions. Imagine for a moment if a parabola had showed up for all age groups in precisely the years predicted by the theory. Wouldn\u2019t we have to update our priors about the abortion legalization selection hypothesis? With predictions so narrow, what else could be causing it? It\u2019s precisely because the pre- dictions are so speci\ufb01c, though, that we are able to reject the abortion legalization hypothesis, at least for gonorrhea.\nPlacebos as critique. Since the fundamental problem of causal infer- ence blocks our direct observation of causal effects, we rely on many direct and indirect pieces of evidence to establish credible causality. And as I said in the previous section on DDD, one of those indirect pieces of evidence is placebo analysis. The reasoning goes that if we \ufb01nd, using our preferred research design, effects where there shouldn\u2019t be, then maybe our original \ufb01ndings weren\u2019t credible in the \ufb01rst place. Using placebo analysis within your own work has become an essential part of empirical work for this reason.\nBut another use of placebo analysis is to evaluate the credibility of popular estimation strategies themselves. This kind of use helps improve a literature by uncovering \ufb02aws in a research design which can then help stimulate the creation of stronger methods and models. Let\u2019s take two exemplary studies that accomplished this well: Auld and Grootendorst [2004] and Cohen-Cole and Fletcher [2008].\nTo say that the Becker and Murphy [1988] \u201crational addiction\u201d model has been in\ufb02uential would be an understatement. It has over 4,000 cites and has become one of the most common frameworks in health economics. It created a cottage industry of empirical studies that persists to this day. Alcohol, tobacco, gambling, even sports, have all been found to be \u201crationally addictive\u201d commodities and activities using various empirical approaches.\nBut some researchers cautioned the research community about these empirical studies. Rogeberg [2004] critiqued the theory on its own grounds, but I\u2019d like to focus on the empirical studies based on the theory. Rather than talk about any speci\ufb01c paper, I\u2019d like to provide a quote from Melberg [2008], who surveyed researchers who had written on rational addiction:\nA majority of [our] respondents believe the literature is a success story that demonstrates the power of economic reasoning. At the same time, they also believe the empirical evidence is weak, and they disagree both on the type of evidence that would validate the theory and the policy implications. Taken together, this points to an interesting gap. On the one hand, most of the respondents claim that the theory has valuable real world implications. On the other hand, they do not believe the theory has received empirical support. [1]\nRational addiction should be held to the same empirical standards as in theory. The strength of the model has always been based on the economic reasoning, which economists obviously \ufb01nd compelling. But were the empirical designs \ufb02awed? How could we know?\nAuld and Grootendorst [2004] is not a test of the rational addic- tion model. On the contrary, it is an \u201canti-test\u201d of the empirical rational addiction models common at the time. Their goal was not to evaluate the theoretical rational addiction model, in other words, but rather the empirical rational addiction models themselves. How do they do this? Auld and Grootendorst [2004] used the empirical rational addiction model to evaluate commodities that could not plausibly be considered addictive, such as eggs, milk, orange, and apples. They found that the empirical rational addiction model implied milk was extremely addic- tive, perhaps one of the most addictive commodities studied.9 Is it credible to believe that eggs and milk are \u201crationally addictive\u201d or is it more likely the research designs used to evaluate the rational addiction model were \ufb02awed? Auld and Grootendorst [2004] study cast doubt on the empirical rational addiction model, not the theory.\nAnother problematic literature was the peer-effects literature. Esti- mating peer effects is notoriously hard.", "start_char_idx": 37106, "end_char_idx": 41710, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ec5e20de-09c1-45a4-a676-71feea16ceef": {"__data__": {"id_": "ec5e20de-09c1-45a4-a676-71feea16ceef", "embedding": null, "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "83d6e205-d5f0-4291-802f-85c79a899db8", "node_type": "4", "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "hash": "67c25ce71063b06809b4b6ddca5b8e8fc47c015726d3cb1dda1af9ff2c428e85", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f3746d27-f600-48bd-89e4-7b1cc757f9f6", "node_type": "1", "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "hash": "ef35568299de17bb0e84c8385c9a3f02c89cbe4992afb89fcdac813653fe4c8f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bacc47b3-ffef-4437-bddf-6f4b6dfb5681", "node_type": "1", "metadata": {}, "hash": "43a93f647b80bbefd8ae7ea0f5b1862b0f160e00a5cf1e34406529fcb4e8f85e", "class_name": "RelatedNodeInfo"}}, "text": "Their goal was not to evaluate the theoretical rational addiction model, in other words, but rather the empirical rational addiction models themselves. How do they do this? Auld and Grootendorst [2004] used the empirical rational addiction model to evaluate commodities that could not plausibly be considered addictive, such as eggs, milk, orange, and apples. They found that the empirical rational addiction model implied milk was extremely addic- tive, perhaps one of the most addictive commodities studied.9 Is it credible to believe that eggs and milk are \u201crationally addictive\u201d or is it more likely the research designs used to evaluate the rational addiction model were \ufb02awed? Auld and Grootendorst [2004] study cast doubt on the empirical rational addiction model, not the theory.\nAnother problematic literature was the peer-effects literature. Esti- mating peer effects is notoriously hard. Manski [1993] said that the deep endogeneity of social interactions made the identi\ufb01cation of peer effects di\ufb03cult and possibly even impossible. He called this problem the \u201cmirroring\u201d problem. If \u201cbirds of a feather \ufb02ock together,\u201d then iden- tifying peer effects in observational settings may just be impossible due to the profound endogeneities at play.\nSeveral studies found signi\ufb01cant network effects on outcomes like obesity, smoking, alcohol use, and happiness. This led many researchers to conclude that these kinds of risk behaviors were \u201ccon- tagious\u201d through peer effects [Christakis and Fowler, 2007]. But these studies did not exploit randomized social groups. The peer groups were purely endogenous. Cohen-Cole and Fletcher [2008] showed using similar models and data that even attributes that couldn\u2019t be transmitted between peers\u2014acne, height, and headaches\u2014appeared \u201ccontagious\u201d in observational data using the Christakis and Fowler\n9 Milk is ironically my favorite drink, even over IPAs, so I am not persuaded by this\n[2007] model for estimation. Note, Cohen-Cole and Fletcher [2008] does not reject the idea of theoretical contagions. Rather, they point out that the Manski critique should guide peer effect analysis if social interactions are endogenous. They provide evidence for this indirectly using placebo analysis.10\nCompositional change within repeated cross-sections. DD can be applied to repeated cross-sections, as well as panel data. But one of the risks of working with the repeated cross-sections is that unlike panel data (e.g., individual-level panel data), repeated cross-sections run the risk of compositional changes. Hong [2013] used repeated cross-sectional data from the Consumer Expenditure Survey (CEX) containing music expenditure and internet use for a random sample of households. The author\u2019s study exploited the emergence and immense popularity of Napster, the \ufb01rst \ufb01le-sharing software widely used by Internet users, in June 1999 as a natural experiment. The study com- pared Internet users and Internet non-users before and after the emer- gence of Napster. At \ufb01rst glance, they found that as Internet diffusion increased from 1996 to 2001, spending on music for Internet users fell faster than that for non-Internet users. This was initially evidence that Napster was responsible for the decline, until this was investigated more carefully.\nBut when we look at Table 76, we see evidence of compositional changes. While music expenditure fell over the treatment period, the demographics of the two groups also changed over this period. For instance, the age of Internet users grew while income fell. If older peo- ple are less likely to buy music in the \ufb01rst place, then this could indepen- dently explain some of the decline. This kind of compositional change is a like an omitted variable bias built into the sample itself caused by time-variant unobservables. Diffusion of the Internet appears to be related to changing samples as younger music fans are early adopters.\n10 Breakthroughs in identifying peer effects eventually emerged, but only from\nstudies that serendipitously had randomized peer groups such as Sacerdote [2001], Lyle\n[2009], Carrell et al. [2019], Kofoed and McGovney [2019], and several others. Many of\nthese papers either used randomized roommates or randomized companies at military academies. Such natural experiments are rare opportunities for studying peer effects\nfor their ability to overcome the mirror problem.\nIdenti\ufb01cation of causal effects would need for the treatment itself to be exogenous to such changes in the composition.\nFinal thoughts. There are a few other caveats I\u2019d like to make before moving on.", "start_char_idx": 40812, "end_char_idx": 45403, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bacc47b3-ffef-4437-bddf-6f4b6dfb5681": {"__data__": {"id_": "bacc47b3-ffef-4437-bddf-6f4b6dfb5681", "embedding": null, "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "83d6e205-d5f0-4291-802f-85c79a899db8", "node_type": "4", "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "hash": "67c25ce71063b06809b4b6ddca5b8e8fc47c015726d3cb1dda1af9ff2c428e85", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ec5e20de-09c1-45a4-a676-71feea16ceef", "node_type": "1", "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}, "hash": "215d57bbd36291efa4a2b120f426d68033153fe21409c1e2812aa011152f840d", "class_name": "RelatedNodeInfo"}}, "text": "Diffusion of the Internet appears to be related to changing samples as younger music fans are early adopters.\n10 Breakthroughs in identifying peer effects eventually emerged, but only from\nstudies that serendipitously had randomized peer groups such as Sacerdote [2001], Lyle\n[2009], Carrell et al. [2019], Kofoed and McGovney [2019], and several others. Many of\nthese papers either used randomized roommates or randomized companies at military academies. Such natural experiments are rare opportunities for studying peer effects\nfor their ability to overcome the mirror problem.\nIdenti\ufb01cation of causal effects would need for the treatment itself to be exogenous to such changes in the composition.\nFinal thoughts. There are a few other caveats I\u2019d like to make before moving on. First, it is important to remember the concepts we learned in the early DAG chapter. In choosing covariates in a DD design, you must resist the temptation to simply load the regression up with a kitchen sink of regressors. You should resist if only because in so doing, you may inadvertently include a collider, and if a collider is condi- tioned on, it introduces strange patterns that may mislead you and your audience. There is unfortunately no way forward except, again, deep institutional familiarity with both the factors that determined treatment assignment on the ground, as well as economic theory itself. Second, another issue I skipped over entirely is the question of how the out- come is modeled. Very little thought if any is given to how exactly we should model some outcome. Just to take one example, should we use the log or the levels themselves? Should we use the quartic root? Should we use rates? These, it turns, out are critically impor- tant because for many of them, the parallel trends assumption needed for identi\ufb01cation will not be achieved\u2014even though it will be achieved under some other unknown transformation. It is for this reason that you can think of many DD designs as having a parametric element because you must make strong commitments about the functional form itself. I cannot provide guidance to you on this, except that maybe using the pre-treatment leads as a way of \ufb01nding parallelism could be a useful guide.", "start_char_idx": 44623, "end_char_idx": 46857, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"942b2e0c-e53a-4648-8a9f-c6040b2c6ff8": {"doc_hash": "438b277466d240c7db0ead53e42ab8aa97441e47a10e9747f8467944bc1b867d", "ref_doc_id": "b43c985c-9291-4546-8396-43fef2089134"}, "cdf8fa84-273e-48fb-ac38-f565003d24c5": {"doc_hash": "491873f8c2d6096943ceed015e3f9fe2eccc400abaedc083237d249cd8d7e348", "ref_doc_id": "b43c985c-9291-4546-8396-43fef2089134"}, "13a903af-57c2-4317-be27-f8f3b31bc6ea": {"doc_hash": "c066899586a634f1b0593ded09135f98559809cde48c35c07b7c8fa7f0b6ceef", "ref_doc_id": "0a516dbb-2de4-488a-b681-0e73fd98c9a7"}, "19023b39-bf25-4b31-88dd-ab8251717e2c": {"doc_hash": "f85ddf250b7a24ad429289454576a814f0ef544002fdc214f37f513db93a5fef", "ref_doc_id": "0a516dbb-2de4-488a-b681-0e73fd98c9a7"}, "45c746f2-1658-4d10-8798-23f5c4d8e162": {"doc_hash": "0bc8f5f6817858cf2e29cd8bb8bb24ffc3c4d6bed0f8654a7cc8bbb324a44e3a", "ref_doc_id": "52827240-61a7-40e1-8078-45bacdcb7d9d"}, "9156c820-862b-4eda-84c7-7b7632de30b4": {"doc_hash": "8f1da4e11a0dec1201fe7b4530e57dc45dbd2215c26422597e69533ac827af2b", "ref_doc_id": "717c74f0-f551-4617-bf30-e276298d1582"}, "4f3291a2-2015-44bc-80c8-79c563ea570b": {"doc_hash": "bbeb2abb302e9c64a6d6f49ce54cf0c8f6c6a70ef3c74929b5f73b7f665a2f67", "ref_doc_id": "717c74f0-f551-4617-bf30-e276298d1582"}, "2f729636-0b2a-4a18-a03b-0fca3c13baac": {"doc_hash": "66be530eb69a5f82c4a8b815609461b5251373845686960f26cc6e74859fd366", "ref_doc_id": "7c3a6c55-9360-4422-9ac1-87a0145579d6"}, "a9ce38c9-acc2-4f68-af34-ade27cdb27f8": {"doc_hash": "719770660a421140c9bcec9bf213a3e7114fc9a76e49487135c8daef39133eac", "ref_doc_id": "7c3a6c55-9360-4422-9ac1-87a0145579d6"}, "3a909d11-e83f-465f-8b8d-3bfd44717207": {"doc_hash": "623a43272ae9b9cf8d72c6fcbefc4780478f185feb7518e6eb691b42f782fc17", "ref_doc_id": "314f3329-a54b-4e56-8822-20973e9f563f"}, "1371dc4d-518d-4ff8-a820-1ceea4b8e222": {"doc_hash": "cb1d05e4623aab33d0d6fa52fbf2047c257f6f2e05470ff9c425d4fd2e2cc20e", "ref_doc_id": "314f3329-a54b-4e56-8822-20973e9f563f"}, "26a865ee-5356-492d-8e0e-cf9ece65d9a0": {"doc_hash": "67ebe3563d9acbf94604e8acad56993f7ad25ddbf1704524eeb0dee5c046a53f", "ref_doc_id": "bb4056ec-7ec0-47f8-97e4-4af53e34c7a3"}, "23448f74-5eef-4900-a80d-b7221740d012": {"doc_hash": "02cb95b5431a07f6206112465bd557e2b3760139ab82fc2dc35a1b9bdadf9ba3", "ref_doc_id": "0e501736-f232-4e9b-99cd-511596548668"}, "191c5854-0afe-43e5-b645-c69381dc7791": {"doc_hash": "e07b77d8922dc278d5e083538c98c95de7cb93f85946dd21ae3680b204138881", "ref_doc_id": "0e501736-f232-4e9b-99cd-511596548668"}, "b38b4017-cade-407c-be15-4aab4ea8e80e": {"doc_hash": "a87d3d3feb04c84a73e467e489432d8e48564b888ef05a5e6d9a63ddaa928eda", "ref_doc_id": "3dc624e4-a621-459b-9343-5c9d0651aa5e"}, "6361436a-9595-4826-98a4-0d4a8fe63e6a": {"doc_hash": "ee003d97319a4a5ed9fb2ac20d0dbeb2c3d42aa8ecaea6c3eb3a5c007090ab94", "ref_doc_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac"}, "9f8f892c-762f-4cfc-a2c6-0d93ade12309": {"doc_hash": "00ada188524e021de27d1954cbabadc7cb4aa19fd27132e08f3c2179cc4b0727", "ref_doc_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac"}, "5ec549e9-4256-4d62-ab5e-27a400c61620": {"doc_hash": "cf67c41e95c9ba564d61e0bde612773eeea6d309af38dc0c75bf992cba6fb8f5", "ref_doc_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac"}, "ae7fb1c9-03ef-41a0-a8ff-fbb0200e9ab8": {"doc_hash": "22282a63411cb9223ab2ee2ea9595b9f6feabef72fe78806841376b5cf503e24", "ref_doc_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac"}, "00220262-ff96-47a7-94ce-47181c19cd06": {"doc_hash": "05d517cc84e11a0a6667578b790ef86dea46ad4fab9f2308f61d1913d1c0c00d", "ref_doc_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac"}, "871d9c76-eb20-42bf-82eb-854ca19267ad": {"doc_hash": "4eabc8fbd8b0bfdeaa725fe089919a36d6fed3b3a45c0e2b20e58654d4c09f0f", "ref_doc_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac"}, "41de045d-dcd8-4859-beaa-a5b245df59ab": {"doc_hash": "04770e862145813cdd8527fabc2bc064ab24aab4d4f80ea5ff319645f4e637bb", "ref_doc_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac"}, "e6324ae8-415b-4378-92cf-d521c070a73a": {"doc_hash": "ec06b91f0e893ba405e9cc36e1a5170352137cfd9e217c83561c75724993e122", "ref_doc_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac"}, "a4ccd7f7-bb4c-41af-9d03-49777119f215": {"doc_hash": "de5267998ca41f4ca3e6619a71d5d32c08e207fc1f6572bf39b239bdb3e9481f", "ref_doc_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac"}, "23f4472d-5730-4497-8f25-f1d3a2b8935b": {"doc_hash": "150a6722f19f9c9d84228e94e9cc5eb29dc601d90da623402fe7a52cc2fad518", "ref_doc_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac"}, "b57161ab-f09b-4efe-bdd0-33f8c6b5b3bb": {"doc_hash": "8885501e88529de1fe8da4de278d6cab51714c2ff0b6b81964024d94deae5f35", "ref_doc_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac"}, "4e8aa477-bec7-431f-bceb-a78000db563b": {"doc_hash": "a81f3c7f08f9402dc78b5f5ec75a18291ad4bfe03a972e08777338ecc5ad27b1", "ref_doc_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac"}, "842c2091-a976-448e-8902-e9069d66d7d4": {"doc_hash": "e43600bf27b9e0a64d4b6cb0463d6fe3f6bbad4ae34ab9786bc0bf2fdae0d979", "ref_doc_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac"}, "239192fa-bcca-47cb-8d57-8b18fe4ff79e": {"doc_hash": "92223c2bdd5ef194d81bf5bae1aa1550d759c5771e9dc6c7e216e0286c38565b", "ref_doc_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac"}, "c6495d9b-d117-4e65-87b6-cc8a4ac65f3b": {"doc_hash": "bb0e5d70f7b0f41976beb83a1c4c0e5bad47aa1d9cb539095d53308b34d7d679", "ref_doc_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac"}, "6c2354b8-13d3-4794-bcc3-fc867663d61a": {"doc_hash": "11a6cac002115c3acb51cef45f47af48ff07287ed1c6499cbfbab6d41ac86802", "ref_doc_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac"}, "b59b7994-6a71-477e-b62a-f32f5d971d9e": {"doc_hash": "7dd0505cf61a61eab78000e6a4608d38ea71d54c6ba54e6675ca79ef7b991d7e", "ref_doc_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac"}, "0baf671a-06b2-40fb-9d52-b47aa81b097b": {"doc_hash": "0fa9266381b678122b3fa9e5cd88046fcd91f466a67495e9fe5f05901e082cd0", "ref_doc_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac"}, "e476494e-51c6-4d8d-a91d-7a0d245b3b45": {"doc_hash": "e2365c393c47126c276ccdff081112a4c3953146341f1ce9395811bde40dc008", "ref_doc_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac"}, "48635a48-89aa-42ca-a979-c9fbc2fc90ef": {"doc_hash": "3ec361ffa5d86c9d9e5e1f6364f61162f79d111ae0798c53ee9b3fb05f1df07f", "ref_doc_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac"}, "0688844b-95cc-4293-acca-fab7610e0ee7": {"doc_hash": "92a8646e0fc92476b8f58207413a51cd64f27c88c2ae02e7b269c7262629c93c", "ref_doc_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac"}, "0cc7cfbf-9128-4a51-bb5c-d2be612c1a91": {"doc_hash": "a19b4cd29ae47785daf6000b137319cd5b2cbeb190942f3ea3b9a3b3bd253621", "ref_doc_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac"}, "d5ae99f1-0b74-4749-a7f2-2a07423f145d": {"doc_hash": "477c8679a8909b2d74e1af6ce72ea8d76dc07e4128516ce2da92cd0189e1d7fd", "ref_doc_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac"}, "b55d4b25-7e28-4c65-af3a-bc83aa6e492c": {"doc_hash": "6e3cd989abf113a6407d53cc53ecd3172b5a6316a2416652fc5f910b22a5fe23", "ref_doc_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac"}, "6ca22f0a-0101-4f58-99b7-03f60a42796f": {"doc_hash": "5d1c7f9d10bbbbc8bd451a23d4db93fa6ea5bfaba15622d7d55b8002f8c27c9c", "ref_doc_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac"}, "cf428dc9-624f-4ad5-8b8c-42bac12cdb94": {"doc_hash": "ee0c090284995c06c462773a3db7dc0c3c80a5577ab5cc2bea07ccbe89252203", "ref_doc_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac"}, "809d651a-50e5-4947-80c3-a06e1dcf8121": {"doc_hash": "0c8b1db92ffdeebe3832fd01cefbe167cf206ec8cac5bc3fa119787496f27916", "ref_doc_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac"}, "9ce80a1a-9b2a-4fa1-915e-93878bf37a06": {"doc_hash": "e2eef37bdfba2621ec1f9c571d92f564f305b93b5f8344ad32670ddba53b525f", "ref_doc_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac"}, "080e709e-2c6d-4206-ac48-2aac0cccbbd4": {"doc_hash": "4deffacaf13775fd511d7c793cf118175ec5d0366de8f4400d1f7b4f9e24b7ca", "ref_doc_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac"}, "6d916272-dd5b-4712-93ff-eb8347135643": {"doc_hash": "29d5833b3948f44034880c30711c60d46bb006add287c7a953c7247f6395dc47", "ref_doc_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac"}, "1faed875-be71-4f56-9041-1c4934363585": {"doc_hash": "6ac4e351c92c0dea0ca577c6931835d9f9e86c120bf150a67b9c671d8e2c2eda", "ref_doc_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac"}, "48922572-24b8-43ce-95d0-f146f47a9671": {"doc_hash": "af571f704719ac96287a4251f7b3d516df5b996b781ac56a9da15374a0a749bf", "ref_doc_id": "86ad906d-8465-40ae-80ce-78a9d964b8ac"}, "36a0c3a4-c944-4edd-9e06-bc216346c954": {"doc_hash": "b7b90cb06013993e4455d7745620108454cda3303a59f8c5ff3fd143ec8576d0", "ref_doc_id": "b54e1c91-a86f-40ac-afbe-d6b1e34d2c64"}, "9924e15b-0480-4bdf-adaf-e3481237fcc6": {"doc_hash": "c53545c80ce1e1370f0fc6a591356ed863daa1a97337b9ab2fcac2d6df1d1ab0", "ref_doc_id": "955ddc19-f5ce-411b-b542-1a22027b6054"}, "e4561bb4-b35d-454e-81e6-42023f5607c2": {"doc_hash": "0d1332a9e59c8afaeadbc7b8ea6f964f22a2ae6c4dd5bc4d97b9b47f4d8d78d5", "ref_doc_id": "955ddc19-f5ce-411b-b542-1a22027b6054"}, "8d2ee2a9-4e5d-46ec-a55f-7b509822f836": {"doc_hash": "0bc69eb09ba728c29e7f8cfd6d1bf1960b34498bfc745ef644c0bec47834b836", "ref_doc_id": "955ddc19-f5ce-411b-b542-1a22027b6054"}, "a3f93cad-494e-4d42-a10a-033da91d5748": {"doc_hash": "5e895851e429f3e06c8089e4cdd9652785c7f397697ec5232b95db512e8c54f5", "ref_doc_id": "955ddc19-f5ce-411b-b542-1a22027b6054"}, "9a42df33-55b6-41a0-936a-ab2f5d38e7ab": {"doc_hash": "444c38d9b48307bb80a81c7b54b856141545bad399c8d3d7977a619c6e89b7ce", "ref_doc_id": "955ddc19-f5ce-411b-b542-1a22027b6054"}, "b5b94589-66b6-4b37-b50d-7137fecda024": {"doc_hash": "890d3d41b26e6d6107888589ba6835c54fc7efd231f0c6b24693c547f0bcb7b1", "ref_doc_id": "955ddc19-f5ce-411b-b542-1a22027b6054"}, "1935f891-ec9c-4857-8676-c136c62a55bf": {"doc_hash": "073abd621047316ace80755692f1e3c79b7f1766bd258a32dc7ae1c007a19a97", "ref_doc_id": "955ddc19-f5ce-411b-b542-1a22027b6054"}, "788f5d9b-7c3f-4118-9b18-299ff807945c": {"doc_hash": "52674f3949c8017557832d9b8709f0c9847923721f39481a8f6884113fd63777", "ref_doc_id": "955ddc19-f5ce-411b-b542-1a22027b6054"}, "5b4862c9-e4e7-4859-92a5-2eee96de8b97": {"doc_hash": "d18d949b0d4f944d6839d0167f91c745267a043873d71634ec0924a26e47275d", "ref_doc_id": "955ddc19-f5ce-411b-b542-1a22027b6054"}, "106a1df9-dcf7-43cd-ae1a-386da9e48ac4": {"doc_hash": "51a5fd4547f797090b3e4c459f75b5b230b321271db0508f9d9366b4baf5b07a", "ref_doc_id": "955ddc19-f5ce-411b-b542-1a22027b6054"}, "e24931e6-26d5-4850-bfc2-b49bb797df92": {"doc_hash": "cd49b9a3b10e602a9e67a4038b4777ed2bee44e953a373e7a743c2519cf76ce8", "ref_doc_id": "955ddc19-f5ce-411b-b542-1a22027b6054"}, "90421679-c6e1-45dc-97a2-4375725556a2": {"doc_hash": "55f235de4ca4488fbf158abf0e44809d23248a04cd34831fe8f7ca67631d1b21", "ref_doc_id": "a3e020a5-0f41-4b6c-b676-b0179dc36958"}, "fe066021-8699-4c1c-a112-8d73aa445280": {"doc_hash": "3c39fc6cc4cf5374241cd869d35868cf308a3c669137f875d855f1c5baf227b2", "ref_doc_id": "a3e020a5-0f41-4b6c-b676-b0179dc36958"}, "edfaf9aa-0b12-4c71-9aab-8764f8f98138": {"doc_hash": "2d9f2df841560521da2d161bcfc3a2391a4e23c8a618a184f7b8d71dd3e07c12", "ref_doc_id": "39a1e882-b5dd-4076-870d-98cfa39f2345"}, "56408816-91e0-4bf6-a309-dcbf6ee05392": {"doc_hash": "d9e238c0b208334ca9cb43fa48c16ab3a69d4988ac5a8ead83ce7b1dad9dd3f4", "ref_doc_id": "39a1e882-b5dd-4076-870d-98cfa39f2345"}, "165ae155-f6cb-4585-a40c-38f2851203b2": {"doc_hash": "6d9a0bb7e310695a0e1123a74c3d31ef5b9a1bceeed66768cd4fe156047c469d", "ref_doc_id": "39a1e882-b5dd-4076-870d-98cfa39f2345"}, "8c632b4c-15e8-4e1a-a2c8-40bcb3d6f102": {"doc_hash": "5b7da65e72186fb64a228492021ccc031f9015cd7d1ea10b1390d88e5d75c0a7", "ref_doc_id": "39a1e882-b5dd-4076-870d-98cfa39f2345"}, "bbfbff1d-d29c-4036-bc98-4670e198a78d": {"doc_hash": "015cb38e0c9aa378960b5e7e42697c21be730a1159c3e6b86efa86a08de42c67", "ref_doc_id": "39a1e882-b5dd-4076-870d-98cfa39f2345"}, "bfc12990-1ab9-4538-b340-a3935271b8d5": {"doc_hash": "f66e7a786ce86eb662cdd1109dce9f7c9841d3b38e4e59f382ba6553e6062089", "ref_doc_id": "39a1e882-b5dd-4076-870d-98cfa39f2345"}, "65264c56-b76e-4031-902c-82487cb4cd35": {"doc_hash": "b061d5b8df21a585184e9fec849f04b712f3229c397c4b3fc284c410ca86ee4a", "ref_doc_id": "39a1e882-b5dd-4076-870d-98cfa39f2345"}, "816ae97f-aa5a-4c2b-a7c9-083b296c106e": {"doc_hash": "d54f68c10f7e2ed14d89e24be389037225cdaf8715cb976eea403cf61b47287a", "ref_doc_id": "39a1e882-b5dd-4076-870d-98cfa39f2345"}, "588b4611-c557-4839-938c-045601ea7e35": {"doc_hash": "b1847e3bce372f16862c56c8ad6fc499ae59d4b9b5a7d33a9f040493e8ca8e40", "ref_doc_id": "39a1e882-b5dd-4076-870d-98cfa39f2345"}, "6de4c20e-a4c0-4969-8bc6-6af20801f02e": {"doc_hash": "cb6dec948eb3c00a6f18f2ff377e46480ea0ce9f28358bd1728d82f6d52edf3b", "ref_doc_id": "39a1e882-b5dd-4076-870d-98cfa39f2345"}, "62a91a12-8022-4894-bc99-a0863296d34c": {"doc_hash": "065faeff29327440c6c0f1ef7f6d7fde5e1a393b8679df8dcfce7af0d80d1db9", "ref_doc_id": "39a1e882-b5dd-4076-870d-98cfa39f2345"}, "2a264822-5315-4642-be39-1ba8e6061c0a": {"doc_hash": "c21fbd6c1420c39ad7c205537bb49b14a3db906f0ed63331006d578ad5527b86", "ref_doc_id": "39a1e882-b5dd-4076-870d-98cfa39f2345"}, "0b521db6-b74d-474a-b18b-e4539facdd44": {"doc_hash": "825481c4974c9652803b0dadaf057759b3be4dc13415baaeea84ab4c12a798bd", "ref_doc_id": "39a1e882-b5dd-4076-870d-98cfa39f2345"}, "90eaf067-33f6-4902-b983-c897fefb509a": {"doc_hash": "d486dc222a573e976ddb7d1519ca75f24d2cd56ecd652d40bf8c849218e42922", "ref_doc_id": "81eef3b9-e34d-4b00-8a19-890225e31bcc"}, "96ae19bb-0997-4812-9dd3-540861e336a7": {"doc_hash": "5afca38c7723ca46464c6d53462ba097598005ca4547cd163faa21471024e968", "ref_doc_id": "81eef3b9-e34d-4b00-8a19-890225e31bcc"}, "5b543a75-b019-4279-9d72-786a979d53d7": {"doc_hash": "8fa1d1bb61e1a7716442fb588b66634d2fcdcf63708d6108b246ee7bfc1747e2", "ref_doc_id": "81eef3b9-e34d-4b00-8a19-890225e31bcc"}, "c4161d9b-ee14-4926-9310-548c93d4db5d": {"doc_hash": "2da864ba47fceaa94bfa6c49ec54f2bda7c2fc3225d77634afb9318cceea5051", "ref_doc_id": "81eef3b9-e34d-4b00-8a19-890225e31bcc"}, "afb18742-996d-4b30-9677-dfbbd7f1e195": {"doc_hash": "ba495850d706cfe46207c695b7c6a8c8dd9d539f54cf0f8d28b74755b0257e72", "ref_doc_id": "81eef3b9-e34d-4b00-8a19-890225e31bcc"}, "44c4979b-985c-44f2-b655-e7837bb6ce69": {"doc_hash": "6fed0a59b40840870bc948c027caa41a068cbade7e9bbed78e1ddfd96ed5ed53", "ref_doc_id": "81eef3b9-e34d-4b00-8a19-890225e31bcc"}, "63f4da4a-0e49-4ec9-a37d-a358a3932413": {"doc_hash": "329c336a7bcae7699c4e7c8c741cd2a4be177338730211ae0168acd4c682fa78", "ref_doc_id": "81eef3b9-e34d-4b00-8a19-890225e31bcc"}, "6ecbfb9b-4afe-4c7a-915e-57c481930f05": {"doc_hash": "0a7ef525d3f782edf56ee9f19470c11aef0ed7d87620b75908ef7b18f0610172", "ref_doc_id": "81eef3b9-e34d-4b00-8a19-890225e31bcc"}, "e434a416-d6d3-4936-a7c4-47bf549262e2": {"doc_hash": "4e2b5e6072fe9978b65cd478b226af5e2342b24ca4d02a2fe56cad38b3b9f3c1", "ref_doc_id": "81eef3b9-e34d-4b00-8a19-890225e31bcc"}, "733067d5-ebd3-4a19-952f-bf27c4b036d2": {"doc_hash": "5619ba0d7f520f7a08c9de536952f809e2fab2ec78a9689b54ec33e6279c014e", "ref_doc_id": "81eef3b9-e34d-4b00-8a19-890225e31bcc"}, "40a56963-2e6f-4f30-8808-e31221e3364b": {"doc_hash": "0a37ce18b176d2c0a1892573d964e7d212d289963b181a0f8a8441ca49ecedd8", "ref_doc_id": "81eef3b9-e34d-4b00-8a19-890225e31bcc"}, "248147f2-b673-4b06-8a61-65686e1305ab": {"doc_hash": "5c2650a53c88af5baf07fc209c51182d35b32f212950d52ee4478bf7f24ff80a", "ref_doc_id": "81eef3b9-e34d-4b00-8a19-890225e31bcc"}, "760e4741-bcd4-4cfd-84bf-528fa6a66d64": {"doc_hash": "8bd81a3b3e9f82dc4a7b59d6094cd5ec29184732a04ad130b84b4a87df6b5881", "ref_doc_id": "81eef3b9-e34d-4b00-8a19-890225e31bcc"}, "b86ab68d-9922-4b5b-9843-e558c00322da": {"doc_hash": "07ccfbbc03323cd19eb649f02e0296c97646dc412bb0ce5ca9d358fe517b6f62", "ref_doc_id": "81eef3b9-e34d-4b00-8a19-890225e31bcc"}, "405f61bc-c0a2-456f-96b7-36932ba297d2": {"doc_hash": "d0f8a31813b492b6315db3b480278d727c870a22b06b7695b8413cc197d1c2b3", "ref_doc_id": "81eef3b9-e34d-4b00-8a19-890225e31bcc"}, "21cb16d8-a7e9-4e0d-8a4f-faa13387ea80": {"doc_hash": "3287a4df9957e1191dfdae0707e2c8791b5df6d5d693f43078c4f6d8a5683e18", "ref_doc_id": "81eef3b9-e34d-4b00-8a19-890225e31bcc"}, "e460fcaf-a992-4f29-8166-98955e5462a7": {"doc_hash": "75ca5329627a99ec2c1131ece5d1ebee2f3dfeef35c0c27cb5e7f42cd0c3f304", "ref_doc_id": "81eef3b9-e34d-4b00-8a19-890225e31bcc"}, "ea07c07b-5454-40cd-b8e4-6d522296be88": {"doc_hash": "b9fde3f4ad333a6b3821ac2ebf1958939d582fb9c081334f06e81062d9e24fd1", "ref_doc_id": "81eef3b9-e34d-4b00-8a19-890225e31bcc"}, "a598177b-2d6f-4508-bf88-f27d648c2172": {"doc_hash": "91e1d84f8449289f36c55802659906f1a5b72060c4b01bed3814dfe87f2fc741", "ref_doc_id": "81eef3b9-e34d-4b00-8a19-890225e31bcc"}, "37c04e80-b693-4a58-8652-8a639ea99a8f": {"doc_hash": "8918c3487246cb7e4858586a015dfe66ccce57673522e8dac972a097330aa8d3", "ref_doc_id": "7a2d50d1-1130-4536-8364-423c157b6059"}, "2bcd3c56-c03c-4451-ad1a-3440c181186b": {"doc_hash": "2d71e498521939579f8f279619194d7cf28c46062ac772193daa5216dd52150e", "ref_doc_id": "7a2d50d1-1130-4536-8364-423c157b6059"}, "59353d60-5952-4f90-997a-e48e7388a3ad": {"doc_hash": "0158c7d4ebbd6fe8199b1ab7e2966f0ef4ef2c6dcdefd2b82ba6d5f1bf75732e", "ref_doc_id": "7a2d50d1-1130-4536-8364-423c157b6059"}, "d582b2ca-1e32-4551-8b24-06f85b4dbdd9": {"doc_hash": "2a1b7b41125ff3ce41e464d6581fa5362431f27257dd2b0ef8ee218bf46067cc", "ref_doc_id": "9fd33664-e6ea-4fe1-ae3b-699b771c4008"}, "b6ec7c3d-3467-4c30-936f-3c0cf47b3100": {"doc_hash": "87192cce0712c6ca22ae6f7694046883153d47b3b2332ce2a32d3a7d79782d79", "ref_doc_id": "9fd33664-e6ea-4fe1-ae3b-699b771c4008"}, "cd0e0e8a-1fba-4d5a-ad7f-af50ba2c1e87": {"doc_hash": "c86db9a3118952bcf3613991e87e903db684ecd49cffe82ab264483e65cb745e", "ref_doc_id": "9fd33664-e6ea-4fe1-ae3b-699b771c4008"}, "f054d891-0ddf-4d24-984d-0388df46402b": {"doc_hash": "35145e0ba58f0130598839b2b3414c4b9f8f731add484bbb1e6690423aefb083", "ref_doc_id": "9fd33664-e6ea-4fe1-ae3b-699b771c4008"}, "3d7e0001-caaa-4e4e-9a63-045bfcb861a7": {"doc_hash": "1f1fe9b2efd198fcd7f18aa50c5c0e30a57f4084da6e65cb036053f6ddc78ed3", "ref_doc_id": "9fd33664-e6ea-4fe1-ae3b-699b771c4008"}, "47ff8b58-7f29-45b3-82a1-5fcf6ec0deda": {"doc_hash": "fd3f81017de344223fd4c93b753790d3f3ba4c748d93b123c00470f2fdfa1a4c", "ref_doc_id": "9fd33664-e6ea-4fe1-ae3b-699b771c4008"}, "85654770-e510-4a32-824b-64794e31ba4b": {"doc_hash": "091b946ba636c7bc752606dbcd0f7daccd2d7f7e64b157e5818f578c2f2b229e", "ref_doc_id": "9fd33664-e6ea-4fe1-ae3b-699b771c4008"}, "76eb42df-45b6-4072-a2f8-049c7ddd3c01": {"doc_hash": "b1051b3f03800468bc160826a711b2d2de9ba728867ed0d7d3c2902a792a7a41", "ref_doc_id": "9fd33664-e6ea-4fe1-ae3b-699b771c4008"}, "4b3ed90a-1690-4ae1-ac0e-d8ade0bf9689": {"doc_hash": "f38b69f1197c622b9a5f34dd364827aa8527bae720c68fe0a0f63e86a35b879c", "ref_doc_id": "9fd33664-e6ea-4fe1-ae3b-699b771c4008"}, "22d8ac8c-fbe5-47e2-bdd0-993692c9aed8": {"doc_hash": "ec845bb4424be8cd0fb392afa40cc0167b023890bc40313ae200553c942c4fbc", "ref_doc_id": "9fd33664-e6ea-4fe1-ae3b-699b771c4008"}, "3a358362-e01f-4432-b134-1863e105b843": {"doc_hash": "bbcf03b8d332cfe60e27419f139906c9a68b810cae5a11092ac7db2574b325c3", "ref_doc_id": "9fd33664-e6ea-4fe1-ae3b-699b771c4008"}, "2100053e-99f0-4312-99e3-8641692d42f2": {"doc_hash": "9031d8e3a80c0cd7ddac88bcfab00b02297d7797461697271ee1d2989ea04d60", "ref_doc_id": "9fd33664-e6ea-4fe1-ae3b-699b771c4008"}, "44f90f56-7618-406b-ac7d-9b60e535bfee": {"doc_hash": "3e60537d0b112173d055a8739c08ab6bb22f3419db860c24558ab4cef92040cf", "ref_doc_id": "9fd33664-e6ea-4fe1-ae3b-699b771c4008"}, "5e8e915e-65f2-4b90-961c-0ebdb6d58c35": {"doc_hash": "67821c51c317e8f122aae5ed9f31fae2d8d61af6ba332ec0f4eb8e29c2e6f651", "ref_doc_id": "9fd33664-e6ea-4fe1-ae3b-699b771c4008"}, "a14a8db0-fd67-4dda-99c1-c6fb16340558": {"doc_hash": "11f874b6780986b3d4a0c2ba077522595aa94513a5d3c20cc1c28188ccd51e0f", "ref_doc_id": "9fd33664-e6ea-4fe1-ae3b-699b771c4008"}, "dd5016ef-f821-422c-8809-4f03d2f6a652": {"doc_hash": "8802b9ec8a083cff7d7832b920f931ab709bee66e8c6f6f65e9424d57504d4e5", "ref_doc_id": "9fd33664-e6ea-4fe1-ae3b-699b771c4008"}, "afdf4866-84de-44a2-95d3-1ab79cc77502": {"doc_hash": "02aabf23c152146dea5d0f9b6bb2a3bd4c264aa431218002a44ad735ee8c18be", "ref_doc_id": "9fd33664-e6ea-4fe1-ae3b-699b771c4008"}, "4f4c6529-934a-4f73-bd45-2f1b69e92f10": {"doc_hash": "ccce67cc59b0caa5978c507b4fcc677cc13b9eec12fd3ba05eb5519a19f9145a", "ref_doc_id": "9fd33664-e6ea-4fe1-ae3b-699b771c4008"}, "17ca8703-e1d5-43ef-83e9-453a59278a17": {"doc_hash": "66f6903ebe90a2a369b0b0d777c07b1dc3746b016154255ea36d77ac2f020ede", "ref_doc_id": "9fd33664-e6ea-4fe1-ae3b-699b771c4008"}, "f295f988-3d16-4de6-b1c5-6b009099f0a9": {"doc_hash": "9b0dfa37abe4d66277b5ddaf9ab2ad0fbe5d27d53b8bab18a45da88f2da35ced", "ref_doc_id": "3f6fab02-0aa2-4ef7-a94c-950957c48764"}, "fd244716-6b65-423a-a321-a377abd5f8ac": {"doc_hash": "e53286ed95d576b4b3b24a589a7fe1747ff087a459cb324bd438ad1c8698d70c", "ref_doc_id": "3d8b6c2b-0a22-45a8-bbb1-03011dbcff45"}, "920e24fe-4a2e-4074-ac1a-ea33b04ed8f1": {"doc_hash": "a2f4a8aa02eefee40da775b2db0ad0aa351ae52c2273019347027c584c72cd67", "ref_doc_id": "3d8b6c2b-0a22-45a8-bbb1-03011dbcff45"}, "ed91555d-571b-4339-9bd1-97891f68a8a8": {"doc_hash": "b4ddc5d3b957d8d7ab1b633a455b9d18634673587c2a1bfe0b218542e78816f2", "ref_doc_id": "3d8b6c2b-0a22-45a8-bbb1-03011dbcff45"}, "524756d1-9882-496d-b338-067374826496": {"doc_hash": "f45f17f0a78db12c3268cf227de3bb868a3022ea486ba94b049679fb64134bda", "ref_doc_id": "3d8b6c2b-0a22-45a8-bbb1-03011dbcff45"}, "305a7aac-41cd-437d-9711-aab926ecee25": {"doc_hash": "658528689b84976919e5e99e421b249cedf985eeb68234d1ccc25237a00a380d", "ref_doc_id": "3d8b6c2b-0a22-45a8-bbb1-03011dbcff45"}, "b5bd2545-bd5c-4c98-92d9-7543ca302bb5": {"doc_hash": "e815688188853a6746b905f9572a20e15bb5e6bac8a95b3d1c0252ed8c435e87", "ref_doc_id": "3d8b6c2b-0a22-45a8-bbb1-03011dbcff45"}, "51f7e9f5-47c0-41cf-a990-091928260203": {"doc_hash": "3a8b0a700d981906ee0e6d63062f928f4c9b172cce2a056b29f5403864277354", "ref_doc_id": "3d8b6c2b-0a22-45a8-bbb1-03011dbcff45"}, "d4e3a577-aac6-4049-b886-2d6f85cc7ff3": {"doc_hash": "7cc878bf8d5461809e526de079b9947c7b500ae46985482fd5bc78694e4f8bef", "ref_doc_id": "3895c9fb-cef4-4c38-bd5f-7c8889fe5b0a"}, "9de6615f-8624-4a84-a47f-cd10b2a4281b": {"doc_hash": "5fa5190536fbf39debbf2bcea6b79ed06416ef1df8578bd18a35afc0ce105412", "ref_doc_id": "3895c9fb-cef4-4c38-bd5f-7c8889fe5b0a"}, "bb7920af-060a-40f5-8cb5-0c95ea6f0475": {"doc_hash": "766913fc6fbb017560faf58bde49eade0c9a2b0cc53793356faeade005257b7d", "ref_doc_id": "3895c9fb-cef4-4c38-bd5f-7c8889fe5b0a"}, "3e644237-3be4-4fdc-90e7-940ce46568d8": {"doc_hash": "530d451570a5a9cf17f00c636d5783cc7fb5d683f4cfc0a0111201af9a680051", "ref_doc_id": "3895c9fb-cef4-4c38-bd5f-7c8889fe5b0a"}, "9e52a776-c8be-488d-bdd1-190b802a1ac8": {"doc_hash": "bb71b32d0eff32e44793780ce240717bf2ead209952a02cefcd6f838776d6363", "ref_doc_id": "3895c9fb-cef4-4c38-bd5f-7c8889fe5b0a"}, "e32eecd1-7222-4cf8-887b-901e4cbc0038": {"doc_hash": "8375192f73b4a41d714aeb3b1dcd6bead877be1ea277d79a9525629f0039227b", "ref_doc_id": "3895c9fb-cef4-4c38-bd5f-7c8889fe5b0a"}, "b97a5047-0c37-42bc-80ef-75cd77e75c2f": {"doc_hash": "eb487dfd49d4c8abf857a3b099ed233bf56e7e96f7336b05b77b3926d60faeb8", "ref_doc_id": "3895c9fb-cef4-4c38-bd5f-7c8889fe5b0a"}, "85f49d42-1483-439c-843b-18182d6d83de": {"doc_hash": "ddcb79a03b9b230532ab2e953faf0e385d84fb6688314e505a798297552fd1f0", "ref_doc_id": "3895c9fb-cef4-4c38-bd5f-7c8889fe5b0a"}, "21e6acb9-5275-4094-afff-afca52fa7d76": {"doc_hash": "0f05989d918bb9ad2efffe988eeb247ac4d28c3af4506ff1b95bba745247becb", "ref_doc_id": "3895c9fb-cef4-4c38-bd5f-7c8889fe5b0a"}, "63ac8d68-383b-4152-92df-05c654f60bf9": {"doc_hash": "0ff246dca7464e91d312cef3072b2bac8e27733564fef0727608a8620e0e0711", "ref_doc_id": "3895c9fb-cef4-4c38-bd5f-7c8889fe5b0a"}, "adf69572-0d0c-4089-a81c-3fd8fe96f1c1": {"doc_hash": "b3f12ba10095c802d213912c69b34bdf6265f39380e71be4f77515997c17d2cc", "ref_doc_id": "3895c9fb-cef4-4c38-bd5f-7c8889fe5b0a"}, "085e07b6-61bd-403a-8481-9bab8ade3fb8": {"doc_hash": "91497fa7e9f66d49460326160dfe70364e17d75a6a3e667a66e0a5c65f1bc6e6", "ref_doc_id": "3895c9fb-cef4-4c38-bd5f-7c8889fe5b0a"}, "4404019b-30d8-4d16-9da4-c398059583b9": {"doc_hash": "950a2d8098ee35263a7ca12ec308cd5de4af4c56c60c6ed45a01656e05565f27", "ref_doc_id": "3895c9fb-cef4-4c38-bd5f-7c8889fe5b0a"}, "5e1fa5c4-248d-4abc-9c06-5d66e0a5615c": {"doc_hash": "2b2e221951766dff5c06c468a00ea9dc04f23370572661fac8fdca550c1104db", "ref_doc_id": "3895c9fb-cef4-4c38-bd5f-7c8889fe5b0a"}, "c1837d1d-a042-4678-8007-3202df1d5158": {"doc_hash": "e4421d25b64ce862cdbb339f7f05e577f6176832521d4ff30015cb98d79c972f", "ref_doc_id": "3895c9fb-cef4-4c38-bd5f-7c8889fe5b0a"}, "d6371224-5c5b-4442-8a5b-ddcb321b33f4": {"doc_hash": "91cf92e15e9b3f6297439ac73f083d05543efa394aa3722351a1527b92c96d1f", "ref_doc_id": "3895c9fb-cef4-4c38-bd5f-7c8889fe5b0a"}, "94cf0326-16f6-4a44-9abc-93ac34e29e2d": {"doc_hash": "eebe30dba468dc5a2695de2f6e4aad9089aaaf36db7c6fef901725a91de46762", "ref_doc_id": "3895c9fb-cef4-4c38-bd5f-7c8889fe5b0a"}, "86c248d8-643f-40a0-9121-4226ec37da1f": {"doc_hash": "802292a7de1222706e800b33f99532e8c4297f5017c1eabceba424c559dcc489", "ref_doc_id": "4f33b2b1-e0b1-4436-80fb-33767c564527"}, "8dc5e05a-6302-47ce-a15e-1e1c5e0ac640": {"doc_hash": "0a46a550b721d7e77fbbd08789ddc07c221d753bf7274a83d98aaca8f874542f", "ref_doc_id": "bb1a07a4-6944-47b1-832e-5dff9fe341c4"}, "9cb2841a-3c5d-44b8-98bd-5f3b318f2c07": {"doc_hash": "04a156074a6fd08dbc22ce02bd0e3a292c575eb2b59a5f1917ef6cbf195fd46c", "ref_doc_id": "bb1a07a4-6944-47b1-832e-5dff9fe341c4"}, "b9edad27-d956-4d95-9c9f-e2bddd52e1e8": {"doc_hash": "d8c110dfdc50c213fa1bc44f231e6e2fe1048df116fd8074ce21dc690548759a", "ref_doc_id": "bb1a07a4-6944-47b1-832e-5dff9fe341c4"}, "e11147b7-2ec9-4185-9f3e-1eb4d17ec8c2": {"doc_hash": "c0b36efd1eaca3979cd2148f7983b8f709a9bb1bba6c542c27ba2efc27233c0a", "ref_doc_id": "bb1a07a4-6944-47b1-832e-5dff9fe341c4"}, "284e461c-18c7-4b67-a865-ce74fc01354e": {"doc_hash": "6c200d7d1f6cc392b40d43189708fd88e8c277aa29d4ae01a363f3a0ffafe148", "ref_doc_id": "bb1a07a4-6944-47b1-832e-5dff9fe341c4"}, "6684648b-0ac7-483d-9989-ddb0dbd51658": {"doc_hash": "7f301f48a11e48d3a82db11a67b735231649466327d1b9cc8c28510987b8206e", "ref_doc_id": "bb1a07a4-6944-47b1-832e-5dff9fe341c4"}, "fea6bc01-b9d3-4ea3-a067-b890b1418966": {"doc_hash": "f2198865f6f499c092e98cf3634adc0bfd592918fbd2285197a11817a5427311", "ref_doc_id": "bb1a07a4-6944-47b1-832e-5dff9fe341c4"}, "01a25409-b87f-4db3-bc38-2a4e4922b9d0": {"doc_hash": "95c03fdedb6d3fe4589c9e04ecf230171932b42d2924cfa52a4596915fd8cedd", "ref_doc_id": "bb1a07a4-6944-47b1-832e-5dff9fe341c4"}, "dfc3fa80-0798-4afc-918a-8d253923e66d": {"doc_hash": "fc2ab6cb4fb49db3642865e17bc5af55cc062266ab2788c7936f7b897edbee35", "ref_doc_id": "bb1a07a4-6944-47b1-832e-5dff9fe341c4"}, "3c2935f8-9dbd-44ed-bf7b-fd7cc811004d": {"doc_hash": "1525d2834c8d67d8406022788858e313b1edb9be0740e7a143d3794713616d8e", "ref_doc_id": "bb1a07a4-6944-47b1-832e-5dff9fe341c4"}, "56c18120-5518-4f31-bab7-50846be95689": {"doc_hash": "4540adde94ccf2f1c2499e8c50c8fdfbbd6f61e4937a5a739bac54dda8fc475d", "ref_doc_id": "bb1a07a4-6944-47b1-832e-5dff9fe341c4"}, "205c3c80-2b85-4754-a117-aa5468378f88": {"doc_hash": "292e62c92c76664f782d6fda496a9adada4f64f4b72e97cd0bf8791d64cb72d5", "ref_doc_id": "9783acf8-1965-4724-a3ef-0ea2dc1d39ea"}, "a66ee751-b150-4ace-80c6-f6287df9df64": {"doc_hash": "668014a187f6842eddaeb3e92f54e3dfac6a0b27f34cabb82765f77d5db65fd2", "ref_doc_id": "3e25f122-2f91-48e3-90a1-3c135ee4e1e3"}, "65e5b3b0-ff03-492a-ab0e-1cc4c07e7900": {"doc_hash": "b503c7b87cf503273c31fbfe5a575e2aeddfad35858cf82bdb0d2eae5c34e1ec", "ref_doc_id": "3e25f122-2f91-48e3-90a1-3c135ee4e1e3"}, "75563db9-8b98-48e8-ad0c-21d6923b93ef": {"doc_hash": "bf6f133c1ae14eee1ef543870fb05b05cd6b1b10a21518160352ba6a8950f691", "ref_doc_id": "8d3cc16e-42ef-4617-baf2-aabf5d33ca06"}, "25a42784-36aa-4ec9-801c-b721ddce1a8e": {"doc_hash": "8375143a580e37733d1616369ccfdd01231c83d8ab91eb05ffc3dc14c69c7ae8", "ref_doc_id": "8d3cc16e-42ef-4617-baf2-aabf5d33ca06"}, "30e4dcd2-e875-4d4d-9e07-80b4a9dd61c6": {"doc_hash": "099e1c0887717fac00e4f2f9e0e8248a1c7cfbda19f5ee809efeddc99f072dba", "ref_doc_id": "8d3cc16e-42ef-4617-baf2-aabf5d33ca06"}, "c45a8a02-5a40-4887-bc3b-2ec373152505": {"doc_hash": "3cdd195c3dbad527d253eb6c9907abe994831e4b2977843fb858a0439ad41bad", "ref_doc_id": "c9b5eba0-8b14-4697-88d6-c8ce7cd182a9"}, "6a9990d1-6649-4b31-ad07-fcbb9bbafb61": {"doc_hash": "4b558b2d6ddd575f04b27a57b7c0c574a06778e4ef7b8f4e71fd12f0f6106731", "ref_doc_id": "c9b5eba0-8b14-4697-88d6-c8ce7cd182a9"}, "c117c0d8-2ae9-49f9-aa53-4f7bfb83c2da": {"doc_hash": "292a94635e268d38ab175e2815ae4daeef27137d536eae7d7a47736ddfc62bcf", "ref_doc_id": "c9b5eba0-8b14-4697-88d6-c8ce7cd182a9"}, "08d057ee-01fa-4d31-acb8-61ffb19c6d2d": {"doc_hash": "439465390fdfb9ceb0d61730cd7703f9404539283c42dd9160132001dfadefc5", "ref_doc_id": "bd5b8f1f-211a-4cc1-810e-fd3efaebd20e"}, "ab93fbb1-be9c-47ad-a1eb-108ae5b3ad37": {"doc_hash": "df81d16e575b27e4f8d9eedb48fa6b5b8028ad6e6d5d65df87228babd8cea3a9", "ref_doc_id": "bd5b8f1f-211a-4cc1-810e-fd3efaebd20e"}, "5bb2cc83-4e15-4131-bb2a-739e188fdfa5": {"doc_hash": "209934439e1efe5c63f9493d05586e754a3406b4b985cc05f8e90323a0a9f6af", "ref_doc_id": "bd5b8f1f-211a-4cc1-810e-fd3efaebd20e"}, "db19e2b2-3432-41ed-9739-c1412172868b": {"doc_hash": "43da0de89baa5c71bbae23873b33e9baa33c36f18def66e9cbdcf35d072ec493", "ref_doc_id": "bd5b8f1f-211a-4cc1-810e-fd3efaebd20e"}, "90893272-7465-476c-9e36-352124bc9a34": {"doc_hash": "2ab2ecf7f94ca88dbb9cecdcf9591d0f6643525e7a290810dca775af74406fad", "ref_doc_id": "bd5b8f1f-211a-4cc1-810e-fd3efaebd20e"}, "4105950d-196c-429a-9876-34a66625b39b": {"doc_hash": "5c533badb3ec36700a6d5d93c80b159762f71755b683b47105fd1964547ffad6", "ref_doc_id": "b1db15f2-4c85-4d62-bcb0-8f7f43a76d83"}, "ed740d9b-30d6-4dcd-9dd4-14ea4281cbc3": {"doc_hash": "6132487c17fb8f8f1a7800e6cc3cfa046cf677e7a386ed9c63938d64ed80c0d8", "ref_doc_id": "b1db15f2-4c85-4d62-bcb0-8f7f43a76d83"}, "3427b9e5-8afc-4626-b3d7-6673a8e88ab4": {"doc_hash": "6d2be959e1efad60b139bf0ddde4d20f0df0bc0fab537a5fb677d250d901ed0b", "ref_doc_id": "b1db15f2-4c85-4d62-bcb0-8f7f43a76d83"}, "1d7347f9-3a89-4981-a560-599d6e99aa72": {"doc_hash": "4c80f9eee76a30289b3614f2434e5d9c5df939f3f2008a4176443be5085eadc6", "ref_doc_id": "b1db15f2-4c85-4d62-bcb0-8f7f43a76d83"}, "05866629-f5a8-4208-b3bf-4c8fd629a0a0": {"doc_hash": "03cd76f518ab9a1a721fbdcfa0f952c463d7962e107e31af50c3a1a70d03471d", "ref_doc_id": "b1db15f2-4c85-4d62-bcb0-8f7f43a76d83"}, "d1a42ced-5e84-455c-a33f-94da9da184e1": {"doc_hash": "9e3d7e87c517fec2d59879826848836feaf481a071b9fa8f88cf9c8611aa9776", "ref_doc_id": "9717ba0e-2ae8-49f8-84fa-cf5372da3f73"}, "ce1c60e1-c7d7-4f0b-8a5d-8362c1f04442": {"doc_hash": "083dd03f0fa64c96bb1c87c3e067f7c10917ed8357b6af194ca8d86c7de05eb7", "ref_doc_id": "9717ba0e-2ae8-49f8-84fa-cf5372da3f73"}, "d30e5654-56e4-4943-8f38-5f0177c293ea": {"doc_hash": "d2bfbe1178bb9e2a987dfbb7ef09b51be08995a73dea6bb21cffd0e7c5e1afaa", "ref_doc_id": "9717ba0e-2ae8-49f8-84fa-cf5372da3f73"}, "3ec763d7-3cbd-45b9-838d-b4a83c516072": {"doc_hash": "6497dc7ebac1e188313eb13dc273d89f5a587d2193237ad7483ee79e92f3c7ae", "ref_doc_id": "9717ba0e-2ae8-49f8-84fa-cf5372da3f73"}, "ea9555e6-6521-4fe4-86a9-3368541bd73f": {"doc_hash": "60a964e1e692c9ccc045de808d737e496f5f62979bcdeac857355565d210e034", "ref_doc_id": "853cf565-79e6-4840-ba1e-7ef621273528"}, "33ce5c06-0491-4537-8529-a4a72d8bcc3c": {"doc_hash": "c212a9f69ebadb1eec4a1babfd64d892b6d1793789a49b0189be402ace4a7309", "ref_doc_id": "853cf565-79e6-4840-ba1e-7ef621273528"}, "b3e6d18d-d36c-4eee-ba5b-12cf20992371": {"doc_hash": "73635f80a09c67f76ec0ed9de8f9b0b7b68c4898264079e24b4a483ac83a5d7a", "ref_doc_id": "853cf565-79e6-4840-ba1e-7ef621273528"}, "94a1193d-71ce-4c7a-a209-090d19a983e2": {"doc_hash": "c3d33c18b756ffafbeaf12d1df5c7f8e3fa38881a331100d0eb6ebaf4664f0ac", "ref_doc_id": "853cf565-79e6-4840-ba1e-7ef621273528"}, "5f78f4f5-8740-4a68-8f15-33f4ce983b74": {"doc_hash": "e83f02724d9ef37eb7023b9a0162b2a17812567e5a702f1965d3c13d6d07ef77", "ref_doc_id": "5b71990f-327a-41b7-82e3-7cd6ba4744ae"}, "9cca0889-6813-42c0-85a0-9bf29be18983": {"doc_hash": "b818470536e0e913483004076f441a3eb5bdac56be9d82f1ba0050aa7ec522a8", "ref_doc_id": "5b71990f-327a-41b7-82e3-7cd6ba4744ae"}, "ccb1bfc1-8657-457d-b5f2-d1cfb7848dfa": {"doc_hash": "bcca0b7ecbe130772deb1e0175386ed75337f5c56855fb982240dfb2fef33874", "ref_doc_id": "5b71990f-327a-41b7-82e3-7cd6ba4744ae"}, "42b578e5-3b92-462e-b820-59d7d5036cef": {"doc_hash": "8be8ba6d1a062bbbf8d7466fb6c082eed52f2586ee8a6a0aef4c295c81d6a8c8", "ref_doc_id": "5b71990f-327a-41b7-82e3-7cd6ba4744ae"}, "f7247636-53fa-40f2-8ac3-f69dd40f826c": {"doc_hash": "b155ffcfb8ada80047187e1bdf222efcd832218c5860e2554c8be532c18a4757", "ref_doc_id": "5b71990f-327a-41b7-82e3-7cd6ba4744ae"}, "d4224a67-d264-48d0-b4d0-c1c499a35b1e": {"doc_hash": "bf3028c83a0b5cb66e807f26c48a53daecfb3c3536ffad856548c0a2e5c8ba0e", "ref_doc_id": "5b71990f-327a-41b7-82e3-7cd6ba4744ae"}, "4d12f1e3-a926-479b-92c1-e32df90f37e8": {"doc_hash": "491daa3cebe21e27acaf978ad3bfd9747247e72657b0308aa1d83ac340e212d2", "ref_doc_id": "5b71990f-327a-41b7-82e3-7cd6ba4744ae"}, "e6db17b8-3a22-4781-9dab-cb85c8aaafc0": {"doc_hash": "3565ce854c56552064faeacb439c5792ad2b9812851dbc971d7d4e4c8e8deb43", "ref_doc_id": "5b71990f-327a-41b7-82e3-7cd6ba4744ae"}, "b6ff1f30-2536-4baf-bb4d-98ae3353bbf5": {"doc_hash": "4b80bdea9a1177d6f0640e9a4e9b43f6334d87b803ad6b7557ffb52c3f3c9754", "ref_doc_id": "5b71990f-327a-41b7-82e3-7cd6ba4744ae"}, "e26e7897-8f47-44ba-ab0d-26b7b956d7bc": {"doc_hash": "b356b602f8409566a765e954b4063709b3800d29d0e1a6521c962b273d19d089", "ref_doc_id": "5b71990f-327a-41b7-82e3-7cd6ba4744ae"}, "0229e2ab-70b0-440c-9a4a-9059405b2e2f": {"doc_hash": "8a8dc1a029e8fe9acd80f1f2ce52d613cb8ce22325314c1613ceab883689ac2c", "ref_doc_id": "5b71990f-327a-41b7-82e3-7cd6ba4744ae"}, "442df979-8215-4159-8bb8-207789a55d65": {"doc_hash": "c882f05511a3dc504f89e21bdb9b354aa1372fd95556a039bed5e50a9d3788d9", "ref_doc_id": "5b71990f-327a-41b7-82e3-7cd6ba4744ae"}, "e7c52954-8f2f-4afc-9c28-46c5771ce138": {"doc_hash": "f30145855bd0906074798fd29c9d79d7b75342f9bbfee23f50cb5c8a3701fe6e", "ref_doc_id": "5b71990f-327a-41b7-82e3-7cd6ba4744ae"}, "cc3fc2ae-4423-42c3-85b1-ee9b1dde6059": {"doc_hash": "ff649d2efd6b1761fb56718a91a40da851dda7390227229b5851b9d6f0a88b78", "ref_doc_id": "5b71990f-327a-41b7-82e3-7cd6ba4744ae"}, "8d0b870c-d28a-4b61-b176-1a93dbab2915": {"doc_hash": "30e34cdc7c0b53e40ec09a7d220d1b80d03271f16df0464b70d6e3c61815a634", "ref_doc_id": "f7ccddad-5194-4d88-a0e7-f754ebc4b955"}, "7eb35797-2fba-4d08-a761-1bdd22824810": {"doc_hash": "d788994a70fc75f90bf760b344932bf4410863143165ecabce761b18fb56047c", "ref_doc_id": "8d094328-4f0c-4ba0-b1c2-de6a937b4266"}, "0059cad4-fbb1-4299-ba9f-5bd0780861ea": {"doc_hash": "89501ae7dc2854d10824173afbc30c91cca7a64a93a1c7c542ddf27fd0fe1944", "ref_doc_id": "38d19395-2e08-4e6f-9cdc-39326b0ef108"}, "bfa0d8c7-7f38-4908-800c-7eee9d2a0d10": {"doc_hash": "93e2aa279b1b3f0636218261280d7bf84f9bf5da7dd84661813b0d8630c1c454", "ref_doc_id": "38d19395-2e08-4e6f-9cdc-39326b0ef108"}, "81377956-b9a0-49c5-9d4f-224e4ffdebc1": {"doc_hash": "ff293e71c577f3deae1c08c5eafa4d43132deec39e2b88c378e3d2944bfa3763", "ref_doc_id": "38d19395-2e08-4e6f-9cdc-39326b0ef108"}, "ace06f35-cb61-4b0d-8c48-df4e94eb0243": {"doc_hash": "121b4ae6c7a837b7413af2108a8412188f0e6936a70f2cda4e7f7ee52df9fc5f", "ref_doc_id": "38d19395-2e08-4e6f-9cdc-39326b0ef108"}, "5700686b-089a-4f58-9661-dd1cb0c976ed": {"doc_hash": "59ba7a96a06ee0acffca3f423ddfab0ee3b6bf918ea18b1da021b2b17016daf8", "ref_doc_id": "38d19395-2e08-4e6f-9cdc-39326b0ef108"}, "9f64d852-8973-480b-b100-06a32c77ff15": {"doc_hash": "6a572950d593beaba9be9383dca2a3188a89659a7fbaee8869b3529f9fc57c75", "ref_doc_id": "2f0786ff-e065-4248-a6ab-472c11b4f948"}, "7b4e8419-e87c-48cb-baa8-d33e17d47c5b": {"doc_hash": "6d9ef5ab90afdc9b6bbffebe02755c2a54d08a23a83942f8cb4285078529baf0", "ref_doc_id": "eaf8775c-846e-4264-b2bc-09d41fc9e62a"}, "731c500d-b0d3-412c-9f42-8f1f60295925": {"doc_hash": "42124e494b7b5bd6067f9f49d0d9801b12e406ae552e2ae5ae518ae8aa10665e", "ref_doc_id": "eaf8775c-846e-4264-b2bc-09d41fc9e62a"}, "94272ef9-035b-4b90-ac77-eab8478e093f": {"doc_hash": "1e640b687e90643fa4315a34515fa236e9032be8299d40b45b0c403ba365c8ba", "ref_doc_id": "eaf8775c-846e-4264-b2bc-09d41fc9e62a"}, "f5b8de4c-93f5-41cf-bb3e-c5ebf22c02e6": {"doc_hash": "f739701ee184d8d36f8083a4f9645985a0731a73c6e6fa8dee2628eaee7a0fa7", "ref_doc_id": "eaf8775c-846e-4264-b2bc-09d41fc9e62a"}, "95dbb194-29c3-4541-b713-eea7f48ebe3e": {"doc_hash": "51d0b92276e818392c85a2cb9667fae23bdaff427832535bcbae0ec80f694c05", "ref_doc_id": "eaf8775c-846e-4264-b2bc-09d41fc9e62a"}, "ccddc612-1574-4107-b38e-e8a610165215": {"doc_hash": "76103120c7c3cffeb58254b485e1a0807d1d6749095c4e2cf218da9c9b62f1dd", "ref_doc_id": "30d6cc4b-0fe1-4dd7-afdd-66923a155732"}, "2480b9dd-1118-4902-b6ac-628c6b96048f": {"doc_hash": "fa2d2253aafc47ff2cd23e7c499839013fded147595b3af5ea450041e5b05610", "ref_doc_id": "30d6cc4b-0fe1-4dd7-afdd-66923a155732"}, "fe104bc7-25d8-4164-9c4b-7d5de1c59a12": {"doc_hash": "48260522ce0139973824904ad85c169fd9b93bdcd7a3b648f1fc4f7f5d41123b", "ref_doc_id": "30d6cc4b-0fe1-4dd7-afdd-66923a155732"}, "fc223f22-48e6-4f9e-8605-cf8f5d56652b": {"doc_hash": "5255ed0bcff8b52fdebbb7b8968eb1f7e9d8f6a2fd00588a20cd910ce2fabf76", "ref_doc_id": "30d6cc4b-0fe1-4dd7-afdd-66923a155732"}, "f60c31a5-6d4a-4f95-a61e-cc26358a8662": {"doc_hash": "a150e6f238f161c74448e46db805229c798738c40e7e7f7fbef7381b0ed64864", "ref_doc_id": "30d6cc4b-0fe1-4dd7-afdd-66923a155732"}, "455352be-d4d9-44d3-b260-cba3062cdd10": {"doc_hash": "b7ee80347cdeffcbaeeead226bdcbe02bf4ff4128beb0147093e3799608eb4fa", "ref_doc_id": "30d6cc4b-0fe1-4dd7-afdd-66923a155732"}, "00944c39-88e6-4f7d-87d4-a7ab32606956": {"doc_hash": "850c4a94851b510440b82b515aceccc69bfbffc64defc4adab55a0c4f335a0c3", "ref_doc_id": "30d6cc4b-0fe1-4dd7-afdd-66923a155732"}, "3f23f710-7643-47b6-84fa-aed98b366026": {"doc_hash": "847b5a0a75259fcb1af574f1c82b58dda29290007bc647c2e6ee14ae3917af47", "ref_doc_id": "30d6cc4b-0fe1-4dd7-afdd-66923a155732"}, "24ee121b-0432-49c9-a326-f06b15dacbf5": {"doc_hash": "66af4f5c3d889644d136756c5f50596b4993d4cadac7e19971f28e3faa37f98b", "ref_doc_id": "30d6cc4b-0fe1-4dd7-afdd-66923a155732"}, "61674ede-9040-4047-b628-71b25973adba": {"doc_hash": "14be5612a5910cf41ec40818dcb3fc513d0f67a89b0d78836461dd93bdab2228", "ref_doc_id": "60aada48-38f9-4299-b5a9-9fcac7886e83"}, "02eafcdc-f5a5-46ef-bdd9-d9b6cb906847": {"doc_hash": "c2aa04f0c443c1504c2e7d19350d707ccec20a78341b947bc424160656f258ca", "ref_doc_id": "60aada48-38f9-4299-b5a9-9fcac7886e83"}, "f1898914-b4cb-48b1-99f9-f39ebd7e5924": {"doc_hash": "ea4bb057f1b5910eb71d40750b900935859d8bce0416add95c054b75b418123d", "ref_doc_id": "0c99ade6-1a9e-484d-8673-341f7e9766eb"}, "d2586316-5fd1-4e58-864d-628420b5ae2f": {"doc_hash": "aeb6d5bfc6561bffa56c00d60a98d2181904062d943efc6329363ace8cb6b5dc", "ref_doc_id": "0c99ade6-1a9e-484d-8673-341f7e9766eb"}, "e846af91-e4bc-468b-8bf7-5670f6d27e99": {"doc_hash": "80e26f11254cbf1cabcd32fab6d816a660ff49daa19cca09a7e9442955fdc722", "ref_doc_id": "0c99ade6-1a9e-484d-8673-341f7e9766eb"}, "0e70b4b1-9f47-410b-86d2-5592ed7a4e35": {"doc_hash": "ab03f0f6aefd98463690ba20e3894fd14abf791da87e83e70edc30977b2d71d3", "ref_doc_id": "0c99ade6-1a9e-484d-8673-341f7e9766eb"}, "b07f6633-44d0-4636-b615-65c3b71474e7": {"doc_hash": "7ce7fa95a4a714c8502e4bcf0089db662def91cdf4c3b42835ca145af010ae90", "ref_doc_id": "0c99ade6-1a9e-484d-8673-341f7e9766eb"}, "ee05de7f-f805-4355-afd2-cbe4812c6263": {"doc_hash": "46514dd53f0007f70426f04989c4a596a466d2126e5d4f35528e5fa679cf4c88", "ref_doc_id": "83d6e205-d5f0-4291-802f-85c79a899db8"}, "66732e29-5597-4743-8462-ff2c51405ddf": {"doc_hash": "4000834f48ae41a05c943e7618221287051116120463a1261779ed414d307ff2", "ref_doc_id": "83d6e205-d5f0-4291-802f-85c79a899db8"}, "4d7ecb51-5ac1-44e2-bedc-2f7ee8f0438b": {"doc_hash": "f76d3b81f41d871c994798ed1c2dbd28b1ad87ae72d17a52834a26757e23b8ca", "ref_doc_id": "83d6e205-d5f0-4291-802f-85c79a899db8"}, "27160782-c7dc-4f06-bb51-4a7843652f5d": {"doc_hash": "fde1d4a4db40bdb372400da3c354c5fbedc8e37ebe8438f2d81d6afb1ebe1de8", "ref_doc_id": "83d6e205-d5f0-4291-802f-85c79a899db8"}, "525f5141-f421-40e9-a2e4-6d17ba4bc958": {"doc_hash": "1d93754b1f467785d090a1874fb97a6519382f610c0381b279db25c2f155ae0c", "ref_doc_id": "83d6e205-d5f0-4291-802f-85c79a899db8"}, "4f25183f-6f0a-4e20-a718-0f7f30225b20": {"doc_hash": "f077bf40477acaf03cc117ec37fbd548ec6d051b7bf89d46c62ef18fa5d72abc", "ref_doc_id": "83d6e205-d5f0-4291-802f-85c79a899db8"}, "6595c7bf-b400-42f1-b382-63b4fb774af6": {"doc_hash": "7342d63282621654c088ae7cd4431f2ad65c239c6842c7271fa57b660ab4a3d5", "ref_doc_id": "83d6e205-d5f0-4291-802f-85c79a899db8"}, "e9c76c9e-8fc2-4769-920a-7ea830fe05e4": {"doc_hash": "1b37b3fd6e913a11d991d4ee50db8ad428026a87da809476d25f11a2df371432", "ref_doc_id": "83d6e205-d5f0-4291-802f-85c79a899db8"}, "3187ef9b-b450-49aa-a908-21ed310a945d": {"doc_hash": "c4a007cf7e2a873cee1bd33d2179345e89f6c1eac50350a36ea74245c8302be1", "ref_doc_id": "83d6e205-d5f0-4291-802f-85c79a899db8"}, "2601e591-7ae0-41f7-b8c6-01ec634fb4a3": {"doc_hash": "666b7a6091eb4b02d4c6ec20721be2ed65050473c58c10c06a0b76939de604b9", "ref_doc_id": "83d6e205-d5f0-4291-802f-85c79a899db8"}, "e11ef6db-88b9-4c78-8a5e-3c6501758d55": {"doc_hash": "8f5bfcc550267b5b12e6c2757f32f2f7d766b0b3a8c93e68e534442f469b8178", "ref_doc_id": "83d6e205-d5f0-4291-802f-85c79a899db8"}, "c2452a17-c720-4357-a07d-f00870f891f8": {"doc_hash": "5a6a5446854ad45d868e4a0ed495d70f46d5665bafd090820b0c047592460586", "ref_doc_id": "83d6e205-d5f0-4291-802f-85c79a899db8"}, "f3746d27-f600-48bd-89e4-7b1cc757f9f6": {"doc_hash": "ef35568299de17bb0e84c8385c9a3f02c89cbe4992afb89fcdac813653fe4c8f", "ref_doc_id": "83d6e205-d5f0-4291-802f-85c79a899db8"}, "ec5e20de-09c1-45a4-a676-71feea16ceef": {"doc_hash": "215d57bbd36291efa4a2b120f426d68033153fe21409c1e2812aa011152f840d", "ref_doc_id": "83d6e205-d5f0-4291-802f-85c79a899db8"}, "bacc47b3-ffef-4437-bddf-6f4b6dfb5681": {"doc_hash": "e29b50755cb98cac8bafd53b99ddb6fc2ab2151e03e2418952241fee857951bf", "ref_doc_id": "83d6e205-d5f0-4291-802f-85c79a899db8"}}, "docstore/ref_doc_info": {"b43c985c-9291-4546-8396-43fef2089134": {"node_ids": ["942b2e0c-e53a-4648-8a9f-c6040b2c6ff8", "cdf8fa84-273e-48fb-ac38-f565003d24c5"], "metadata": {"page number": "4 - 12", "chapter": "Introduction"}}, "0a516dbb-2de4-488a-b681-0e73fd98c9a7": {"node_ids": ["13a903af-57c2-4317-be27-f8f3b31bc6ea", "19023b39-bf25-4b31-88dd-ab8251717e2c"], "metadata": {"page number": "13 - 15", "chapter": "Introduction"}}, "52827240-61a7-40e1-8078-45bacdcb7d9d": {"node_ids": ["45c746f2-1658-4d10-8798-23f5c4d8e162"], "metadata": {"page number": "15 - 16"}}, "717c74f0-f551-4617-bf30-e276298d1582": {"node_ids": ["9156c820-862b-4eda-84c7-7b7632de30b4", "4f3291a2-2015-44bc-80c8-79c563ea570b"], "metadata": {"page number": "16 - 19", "chapter": "Probability and Regression Review"}}, "7c3a6c55-9360-4422-9ac1-87a0145579d6": {"node_ids": ["2f729636-0b2a-4a18-a03b-0fca3c13baac", "a9ce38c9-acc2-4f68-af34-ade27cdb27f8"], "metadata": {"page number": "19 - 22"}}, "314f3329-a54b-4e56-8822-20973e9f563f": {"node_ids": ["3a909d11-e83f-465f-8b8d-3bfd44717207", "1371dc4d-518d-4ff8-a820-1ceea4b8e222"], "metadata": {"page number": "22 - 25"}}, "bb4056ec-7ec0-47f8-97e4-4af53e34c7a3": {"node_ids": ["26a865ee-5356-492d-8e0e-cf9ece65d9a0"], "metadata": {"page number": "25 - 26"}}, "0e501736-f232-4e9b-99cd-511596548668": {"node_ids": ["23448f74-5eef-4900-a80d-b7221740d012", "191c5854-0afe-43e5-b645-c69381dc7791"], "metadata": {"page number": "26 - 30"}}, "3dc624e4-a621-459b-9343-5c9d0651aa5e": {"node_ids": ["b38b4017-cade-407c-be15-4aab4ea8e80e"], "metadata": {"page number": "30 - 31"}}, "86ad906d-8465-40ae-80ce-78a9d964b8ac": {"node_ids": ["6361436a-9595-4826-98a4-0d4a8fe63e6a", "9f8f892c-762f-4cfc-a2c6-0d93ade12309", "5ec549e9-4256-4d62-ab5e-27a400c61620", "ae7fb1c9-03ef-41a0-a8ff-fbb0200e9ab8", "00220262-ff96-47a7-94ce-47181c19cd06", "871d9c76-eb20-42bf-82eb-854ca19267ad", "41de045d-dcd8-4859-beaa-a5b245df59ab", "e6324ae8-415b-4378-92cf-d521c070a73a", "a4ccd7f7-bb4c-41af-9d03-49777119f215", "23f4472d-5730-4497-8f25-f1d3a2b8935b", "b57161ab-f09b-4efe-bdd0-33f8c6b5b3bb", "4e8aa477-bec7-431f-bceb-a78000db563b", "842c2091-a976-448e-8902-e9069d66d7d4", "239192fa-bcca-47cb-8d57-8b18fe4ff79e", "c6495d9b-d117-4e65-87b6-cc8a4ac65f3b", "6c2354b8-13d3-4794-bcc3-fc867663d61a", "b59b7994-6a71-477e-b62a-f32f5d971d9e", "0baf671a-06b2-40fb-9d52-b47aa81b097b", "e476494e-51c6-4d8d-a91d-7a0d245b3b45", "48635a48-89aa-42ca-a979-c9fbc2fc90ef", "0688844b-95cc-4293-acca-fab7610e0ee7", "0cc7cfbf-9128-4a51-bb5c-d2be612c1a91", "d5ae99f1-0b74-4749-a7f2-2a07423f145d", "b55d4b25-7e28-4c65-af3a-bc83aa6e492c", "6ca22f0a-0101-4f58-99b7-03f60a42796f", "cf428dc9-624f-4ad5-8b8c-42bac12cdb94", "809d651a-50e5-4947-80c3-a06e1dcf8121", "9ce80a1a-9b2a-4fa1-915e-93878bf37a06", "080e709e-2c6d-4206-ac48-2aac0cccbbd4", "6d916272-dd5b-4712-93ff-eb8347135643", "1faed875-be71-4f56-9041-1c4934363585", "48922572-24b8-43ce-95d0-f146f47a9671"], "metadata": {"page number": "32 - 111"}}, "b54e1c91-a86f-40ac-afbe-d6b1e34d2c64": {"node_ids": ["36a0c3a4-c944-4edd-9e06-bc216346c954"], "metadata": {"page number": "112 - 113"}}, "955ddc19-f5ce-411b-b542-1a22027b6054": {"node_ids": ["9924e15b-0480-4bdf-adaf-e3481237fcc6", "e4561bb4-b35d-454e-81e6-42023f5607c2", "8d2ee2a9-4e5d-46ec-a55f-7b509822f836", "a3f93cad-494e-4d42-a10a-033da91d5748", "9a42df33-55b6-41a0-936a-ab2f5d38e7ab", "b5b94589-66b6-4b37-b50d-7137fecda024", "1935f891-ec9c-4857-8676-c136c62a55bf", "788f5d9b-7c3f-4118-9b18-299ff807945c", "5b4862c9-e4e7-4859-92a5-2eee96de8b97", "106a1df9-dcf7-43cd-ae1a-386da9e48ac4", "e24931e6-26d5-4850-bfc2-b49bb797df92"], "metadata": {"page number": "113 - 135"}}, "a3e020a5-0f41-4b6c-b676-b0179dc36958": {"node_ids": ["90421679-c6e1-45dc-97a2-4375725556a2", "fe066021-8699-4c1c-a112-8d73aa445280"], "metadata": {"page number": "136 - 140", "chapter": "Potential Outcomes Causal Model"}}, "39a1e882-b5dd-4076-870d-98cfa39f2345": {"node_ids": ["edfaf9aa-0b12-4c71-9aab-8764f8f98138", "56408816-91e0-4bf6-a309-dcbf6ee05392", "165ae155-f6cb-4585-a40c-38f2851203b2", "8c632b4c-15e8-4e1a-a2c8-40bcb3d6f102", "bbfbff1d-d29c-4036-bc98-4670e198a78d", "bfc12990-1ab9-4538-b340-a3935271b8d5", "65264c56-b76e-4031-902c-82487cb4cd35", "816ae97f-aa5a-4c2b-a7c9-083b296c106e", "588b4611-c557-4839-938c-045601ea7e35", "6de4c20e-a4c0-4969-8bc6-6af20801f02e", "62a91a12-8022-4894-bc99-a0863296d34c", "2a264822-5315-4642-be39-1ba8e6061c0a", "0b521db6-b74d-474a-b18b-e4539facdd44"], "metadata": {"page number": "140 - 166", "chapter": "Potential Outcomes Causal Model"}}, "81eef3b9-e34d-4b00-8a19-890225e31bcc": {"node_ids": ["90eaf067-33f6-4902-b983-c897fefb509a", "96ae19bb-0997-4812-9dd3-540861e336a7", "5b543a75-b019-4279-9d72-786a979d53d7", "c4161d9b-ee14-4926-9310-548c93d4db5d", "afb18742-996d-4b30-9677-dfbbd7f1e195", "44c4979b-985c-44f2-b655-e7837bb6ce69", "63f4da4a-0e49-4ec9-a37d-a358a3932413", "6ecbfb9b-4afe-4c7a-915e-57c481930f05", "e434a416-d6d3-4936-a7c4-47bf549262e2", "733067d5-ebd3-4a19-952f-bf27c4b036d2", "40a56963-2e6f-4f30-8808-e31221e3364b", "248147f2-b673-4b06-8a61-65686e1305ab", "760e4741-bcd4-4cfd-84bf-528fa6a66d64", "b86ab68d-9922-4b5b-9843-e558c00322da", "405f61bc-c0a2-456f-96b7-36932ba297d2", "21cb16d8-a7e9-4e0d-8a4f-faa13387ea80", "e460fcaf-a992-4f29-8166-98955e5462a7", "ea07c07b-5454-40cd-b8e4-6d522296be88", "a598177b-2d6f-4508-bf88-f27d648c2172"], "metadata": {"page number": "166 - 211", "chapter": "Potential Outcomes Causal Model"}}, "7a2d50d1-1130-4536-8364-423c157b6059": {"node_ids": ["37c04e80-b693-4a58-8652-8a639ea99a8f", "2bcd3c56-c03c-4451-ad1a-3440c181186b", "59353d60-5952-4f90-997a-e48e7388a3ad"], "metadata": {"page number": "211 - 219"}}, "9fd33664-e6ea-4fe1-ae3b-699b771c4008": {"node_ids": ["d582b2ca-1e32-4551-8b24-06f85b4dbdd9", "b6ec7c3d-3467-4c30-936f-3c0cf47b3100", "cd0e0e8a-1fba-4d5a-ad7f-af50ba2c1e87", "f054d891-0ddf-4d24-984d-0388df46402b", "3d7e0001-caaa-4e4e-9a63-045bfcb861a7", "47ff8b58-7f29-45b3-82a1-5fcf6ec0deda", "85654770-e510-4a32-824b-64794e31ba4b", "76eb42df-45b6-4072-a2f8-049c7ddd3c01", "4b3ed90a-1690-4ae1-ac0e-d8ade0bf9689", "22d8ac8c-fbe5-47e2-bdd0-993692c9aed8", "3a358362-e01f-4432-b134-1863e105b843", "2100053e-99f0-4312-99e3-8641692d42f2", "44f90f56-7618-406b-ac7d-9b60e535bfee", "5e8e915e-65f2-4b90-961c-0ebdb6d58c35", "a14a8db0-fd67-4dda-99c1-c6fb16340558", "dd5016ef-f821-422c-8809-4f03d2f6a652", "afdf4866-84de-44a2-95d3-1ab79cc77502", "4f4c6529-934a-4f73-bd45-2f1b69e92f10", "17ca8703-e1d5-43ef-83e9-453a59278a17"], "metadata": {"page number": "219 - 262"}}, "3f6fab02-0aa2-4ef7-a94c-950957c48764": {"node_ids": ["f295f988-3d16-4de6-b1c5-6b009099f0a9"], "metadata": {"page number": "263 - 263", "chapter": "Regression Discontinuity"}}, "3d8b6c2b-0a22-45a8-bbb1-03011dbcff45": {"node_ids": ["fd244716-6b65-423a-a321-a377abd5f8ac", "920e24fe-4a2e-4074-ac1a-ea33b04ed8f1", "ed91555d-571b-4339-9bd1-97891f68a8a8", "524756d1-9882-496d-b338-067374826496", "305a7aac-41cd-437d-9711-aab926ecee25", "b5bd2545-bd5c-4c98-92d9-7543ca302bb5", "51f7e9f5-47c0-41cf-a990-091928260203"], "metadata": {"page number": "263 - 274", "chapter": "Regression Discontinuity"}}, "3895c9fb-cef4-4c38-bd5f-7c8889fe5b0a": {"node_ids": ["d4e3a577-aac6-4049-b886-2d6f85cc7ff3", "9de6615f-8624-4a84-a47f-cd10b2a4281b", "bb7920af-060a-40f5-8cb5-0c95ea6f0475", "3e644237-3be4-4fdc-90e7-940ce46568d8", "9e52a776-c8be-488d-bdd1-190b802a1ac8", "e32eecd1-7222-4cf8-887b-901e4cbc0038", "b97a5047-0c37-42bc-80ef-75cd77e75c2f", "85f49d42-1483-439c-843b-18182d6d83de", "21e6acb9-5275-4094-afff-afca52fa7d76", "63ac8d68-383b-4152-92df-05c654f60bf9", "adf69572-0d0c-4089-a81c-3fd8fe96f1c1", "085e07b6-61bd-403a-8481-9bab8ade3fb8", "4404019b-30d8-4d16-9da4-c398059583b9", "5e1fa5c4-248d-4abc-9c06-5d66e0a5615c", "c1837d1d-a042-4678-8007-3202df1d5158", "d6371224-5c5b-4442-8a5b-ddcb321b33f4", "94cf0326-16f6-4a44-9abc-93ac34e29e2d"], "metadata": {"page number": "274 - 313", "chapter": "Regression Discontinuity"}}, "4f33b2b1-e0b1-4436-80fb-33767c564527": {"node_ids": ["86c248d8-643f-40a0-9121-4226ec37da1f"], "metadata": {"page number": "313 - 314", "chapter": "Regression Discontinuity"}}, "bb1a07a4-6944-47b1-832e-5dff9fe341c4": {"node_ids": ["8dc5e05a-6302-47ce-a15e-1e1c5e0ac640", "9cb2841a-3c5d-44b8-98bd-5f3b318f2c07", "b9edad27-d956-4d95-9c9f-e2bddd52e1e8", "e11147b7-2ec9-4185-9f3e-1eb4d17ec8c2", "284e461c-18c7-4b67-a865-ce74fc01354e", "6684648b-0ac7-483d-9989-ddb0dbd51658", "fea6bc01-b9d3-4ea3-a067-b890b1418966", "01a25409-b87f-4db3-bc38-2a4e4922b9d0", "dfc3fa80-0798-4afc-918a-8d253923e66d", "3c2935f8-9dbd-44ed-bf7b-fd7cc811004d", "56c18120-5518-4f31-bab7-50846be95689"], "metadata": {"page number": "314 - 341"}}, "9783acf8-1965-4724-a3ef-0ea2dc1d39ea": {"node_ids": ["205c3c80-2b85-4754-a117-aa5468378f88"], "metadata": {"page number": "342 - 342", "chapter": "Instrumental Variables"}}, "3e25f122-2f91-48e3-90a1-3c135ee4e1e3": {"node_ids": ["a66ee751-b150-4ace-80c6-f6287df9df64", "65e5b3b0-ff03-492a-ab0e-1cc4c07e7900"], "metadata": {"page number": "342 - 346", "chapter": "Instrumental Variables"}}, "8d3cc16e-42ef-4617-baf2-aabf5d33ca06": {"node_ids": ["75563db9-8b98-48e8-ad0c-21d6923b93ef", "25a42784-36aa-4ec9-801c-b721ddce1a8e", "30e4dcd2-e875-4d4d-9e07-80b4a9dd61c6"], "metadata": {"page number": "346 - 350", "chapter": "Instrumental Variables"}}, "c9b5eba0-8b14-4697-88d6-c8ce7cd182a9": {"node_ids": ["c45a8a02-5a40-4887-bc3b-2ec373152505", "6a9990d1-6649-4b31-ad07-fcbb9bbafb61", "c117c0d8-2ae9-49f9-aa53-4f7bfb83c2da"], "metadata": {"page number": "350 - 357", "chapter": "Instrumental Variables"}}, "bd5b8f1f-211a-4cc1-810e-fd3efaebd20e": {"node_ids": ["08d057ee-01fa-4d31-acb8-61ffb19c6d2d", "ab93fbb1-be9c-47ad-a1eb-108ae5b3ad37", "5bb2cc83-4e15-4131-bb2a-739e188fdfa5", "db19e2b2-3432-41ed-9739-c1412172868b", "90893272-7465-476c-9e36-352124bc9a34"], "metadata": {"page number": "357 - 366", "chapter": "Instrumental Variables"}}, "b1db15f2-4c85-4d62-bcb0-8f7f43a76d83": {"node_ids": ["4105950d-196c-429a-9876-34a66625b39b", "ed740d9b-30d6-4dcd-9dd4-14ea4281cbc3", "3427b9e5-8afc-4626-b3d7-6673a8e88ab4", "1d7347f9-3a89-4981-a560-599d6e99aa72", "05866629-f5a8-4208-b3bf-4c8fd629a0a0"], "metadata": {"page number": "366 - 376", "chapter": "Instrumental Variables"}}, "9717ba0e-2ae8-49f8-84fa-cf5372da3f73": {"node_ids": ["d1a42ced-5e84-455c-a33f-94da9da184e1", "ce1c60e1-c7d7-4f0b-8a5d-8362c1f04442", "d30e5654-56e4-4943-8f38-5f0177c293ea", "3ec763d7-3cbd-45b9-838d-b4a83c516072"], "metadata": {"page number": "376 - 383", "chapter": "Instrumental Variables"}}, "853cf565-79e6-4840-ba1e-7ef621273528": {"node_ids": ["ea9555e6-6521-4fe4-86a9-3368541bd73f", "33ce5c06-0491-4537-8529-a4a72d8bcc3c", "b3e6d18d-d36c-4eee-ba5b-12cf20992371", "94a1193d-71ce-4c7a-a209-090d19a983e2"], "metadata": {"page number": "383 - 391", "chapter": "Instrumental Variables"}}, "5b71990f-327a-41b7-82e3-7cd6ba4744ae": {"node_ids": ["5f78f4f5-8740-4a68-8f15-33f4ce983b74", "9cca0889-6813-42c0-85a0-9bf29be18983", "ccb1bfc1-8657-457d-b5f2-d1cfb7848dfa", "42b578e5-3b92-462e-b820-59d7d5036cef", "f7247636-53fa-40f2-8ac3-f69dd40f826c", "d4224a67-d264-48d0-b4d0-c1c499a35b1e", "4d12f1e3-a926-479b-92c1-e32df90f37e8", "e6db17b8-3a22-4781-9dab-cb85c8aaafc0", "b6ff1f30-2536-4baf-bb4d-98ae3353bbf5", "e26e7897-8f47-44ba-ab0d-26b7b956d7bc", "0229e2ab-70b0-440c-9a4a-9059405b2e2f", "442df979-8215-4159-8bb8-207789a55d65", "e7c52954-8f2f-4afc-9c28-46c5771ce138", "cc3fc2ae-4423-42c3-85b1-ee9b1dde6059"], "metadata": {"page number": "391 - 419", "chapter": "Panel Data"}}, "f7ccddad-5194-4d88-a0e7-f754ebc4b955": {"node_ids": ["8d0b870c-d28a-4b61-b176-1a93dbab2915"], "metadata": {"page number": "420 - 420", "chapter": "Difference in Differences"}}, "8d094328-4f0c-4ba0-b1c2-de6a937b4266": {"node_ids": ["7eb35797-2fba-4d08-a761-1bdd22824810"], "metadata": {"page number": "420 - 422", "chapter": "Difference in Differences"}}, "38d19395-2e08-4e6f-9cdc-39326b0ef108": {"node_ids": ["0059cad4-fbb1-4299-ba9f-5bd0780861ea", "bfa0d8c7-7f38-4908-800c-7eee9d2a0d10", "81377956-b9a0-49c5-9d4f-224e4ffdebc1", "ace06f35-cb61-4b0d-8c48-df4e94eb0243", "5700686b-089a-4f58-9661-dd1cb0c976ed"], "metadata": {"page number": "422 - 431", "chapter": "Difference in Differences"}}, "2f0786ff-e065-4248-a6ab-472c11b4f948": {"node_ids": ["9f64d852-8973-480b-b100-06a32c77ff15"], "metadata": {"page number": "431 - 432", "chapter": "Difference in Differences"}}, "eaf8775c-846e-4264-b2bc-09d41fc9e62a": {"node_ids": ["7b4e8419-e87c-48cb-baa8-d33e17d47c5b", "731c500d-b0d3-412c-9f42-8f1f60295925", "94272ef9-035b-4b90-ac77-eab8478e093f", "f5b8de4c-93f5-41cf-bb3e-c5ebf22c02e6", "95dbb194-29c3-4541-b713-eea7f48ebe3e"], "metadata": {"page number": "432 - 442", "chapter": "Difference in Differences"}}, "30d6cc4b-0fe1-4dd7-afdd-66923a155732": {"node_ids": ["ccddc612-1574-4107-b38e-e8a610165215", "2480b9dd-1118-4902-b6ac-628c6b96048f", "fe104bc7-25d8-4164-9c4b-7d5de1c59a12", "fc223f22-48e6-4f9e-8605-cf8f5d56652b", "f60c31a5-6d4a-4f95-a61e-cc26358a8662", "455352be-d4d9-44d3-b260-cba3062cdd10", "00944c39-88e6-4f7d-87d4-a7ab32606956", "3f23f710-7643-47b6-84fa-aed98b366026", "24ee121b-0432-49c9-a326-f06b15dacbf5"], "metadata": {"page number": "443 - 461", "chapter": "Difference in Differences"}}, "60aada48-38f9-4299-b5a9-9fcac7886e83": {"node_ids": ["61674ede-9040-4047-b628-71b25973adba", "02eafcdc-f5a5-46ef-bdd9-d9b6cb906847"], "metadata": {"page number": "461 - 463", "chapter": "Difference in Differences"}}, "0c99ade6-1a9e-484d-8673-341f7e9766eb": {"node_ids": ["f1898914-b4cb-48b1-99f9-f39ebd7e5924", "d2586316-5fd1-4e58-864d-628420b5ae2f", "e846af91-e4bc-468b-8bf7-5670f6d27e99", "0e70b4b1-9f47-410b-86d2-5592ed7a4e35", "b07f6633-44d0-4636-b615-65c3b71474e7"], "metadata": {"page number": "464 - 472", "chapter": "Difference in Differences"}}, "83d6e205-d5f0-4291-802f-85c79a899db8": {"node_ids": ["ee05de7f-f805-4355-afd2-cbe4812c6263", "66732e29-5597-4743-8462-ff2c51405ddf", "4d7ecb51-5ac1-44e2-bedc-2f7ee8f0438b", "27160782-c7dc-4f06-bb51-4a7843652f5d", "525f5141-f421-40e9-a2e4-6d17ba4bc958", "4f25183f-6f0a-4e20-a718-0f7f30225b20", "6595c7bf-b400-42f1-b382-63b4fb774af6", "e9c76c9e-8fc2-4769-920a-7ea830fe05e4", "3187ef9b-b450-49aa-a908-21ed310a945d", "2601e591-7ae0-41f7-b8c6-01ec634fb4a3", "e11ef6db-88b9-4c78-8a5e-3c6501758d55", "c2452a17-c720-4357-a07d-f00870f891f8", "f3746d27-f600-48bd-89e4-7b1cc757f9f6", "ec5e20de-09c1-45a4-a676-71feea16ceef", "bacc47b3-ffef-4437-bddf-6f4b6dfb5681"], "metadata": {"page number": "472 - 501", "chapter": "Difference in Differences"}}}}